{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuesday, May 21, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on 5/21/24 I googled around for a dataset and found one thru SAMHSA.  it includes everyone, queer and otherwise, but it looks promising.  it's really, really big though, so it's taking a lot of cleaning before I can even evaluate whether it's what I want or not.  maybe i should just define that it is what I want, and then figure out what that is as I clean it.  I definitely think I could make a project out of it, even if it's not the project of my dreams.  this dataset has nearly 3000 columns and it's running VERY slow in Rstudio.  after finding it, I spend several hours going thru the data dictionary to try and eliminate columns.  I got thru the \"self-administered substance use sections,\" and will pick up tomorrow on \"imputed substance use.\"\n",
    "\n",
    "files created:\n",
    "- everything in the potential datasets folder\n",
    "- 2024-05-21_set-up.py\n",
    "- evaluating-columns-in-samhsa-dataset.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capstone Setup\n",
    "\n",
    "# Module Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, date, time\n",
    "from string import capwords\n",
    "\n",
    "# Working Directory\n",
    "os.getcwd()\n",
    "os.chdir('C:/Users/emily/Git_Stuff/General_Assembly/04_Projects/project-capstone')\n",
    "os.listdir()\n",
    "\n",
    "# Import Data\n",
    "# (This will likely change as my dataset choice changes!)\n",
    "df = pd.read_csv(\n",
    "  './potential_datasets/2024-05-21_download_SAMHSA_NSDUH-2019-DS0001/NSDUH_2019_Tab.txt', \n",
    "  sep = '\\t', low_memory=False)\n",
    "\n",
    "# This thing has 2741 columns!  \n",
    "# I'm going to pair it down by printing out the column names, \n",
    "# copying them to a text file, and then\n",
    "# going through the codebook to see which ones I really need.\n",
    "\n",
    "#print(list(df.columns))\n",
    "# commented out bcz omg.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wednesday, May 22, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today I am continuing to clean that SAMHSA dataset.  I am feeling more and more strongly that I should just make a project out of this, whatever it is, because I am investing so much time into investigating it.  I can always find a more queer-centric dataset later and play with it in my portfolio.\n",
    "\n",
    "~I am going to have multiple scripts today,~ nope, had a power outage instead!\n",
    "\n",
    "files created:\n",
    "- 2024-05-22_continued_column_exploration_of_samhsa_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May 22, 2024\n",
    "\n",
    "# continued column cleaning in SAMHSA dataset\n",
    "# I'm really starting to think that when I get to the bottom of this one, I should just use it\n",
    "\n",
    "# Status: I have gone thru a lot of columns by hand, and noticed some patterns.\n",
    "# Specifically, for rightnow, I have noticed that all variables that only serve to show when another variable has been imputed\n",
    "# start with II.  Therefore, I'm going to use a list comprehension (or similar) to eliminate them all at once.\n",
    "\n",
    "# Here are the remaining column names.  I'm copying them in from evaluating-columns-in-samhsa-dataset.txt, and when I'm done,\n",
    "# I will copy them back.  I am deleting column names in that dataset as I go along.  If I ever need a full list\n",
    "# of the original column names again, I can always pull them again from the original dataset.\n",
    "\n",
    "col_names_1 = ['IRCGRFM', 'IICGRFM', 'II2CGRFM', 'IRSMKLSS30N', 'IISMKLSS30N', 'IRALCFM', 'IIALCFM', 'II2ALCFM', 'IRALCBNG30D', 'IIALCBNG30D', 'IRMJFM', 'IIMJFM', 'II2MJFM', 'IRCOCFM', 'IICOCFM', 'II2COCFM', 'IRCRKFM', 'IICRKFM', 'II2CRKFM', 'IRHERFM', 'IIHERFM', 'II2HERFM', 'IRHALLUC30N', 'IIHALLUC30N', 'IRINHAL30N', 'IIINHAL30N', 'IRMETHAM30N', 'IIMETHAM30N', 'IRPNRNM30FQ', 'IIPNRNM30FQ', 'IRTRQNM30FQ', 'IITRQNM30FQ', 'IRSTMNM30FQ', 'IISTMNM30FQ', 'IRSEDNM30FQ', 'IISEDNM30FQ', 'IRCIGAGE', 'IICIGAGE', 'IRCIGYFU', 'IICIGYFU', 'IRCDUAGE', 'IICDUAGE', 'IRCD2YFU', 'IICD2YFU', 'IRCGRAGE', 'IICGRAGE', 'IRCGRYFU', 'IICGRYFU', 'IRSMKLSSTRY', 'IISMKLSSTRY', 'IRSMKLSSYFU', 'IISMKLSSYFU', 'IRALCAGE', 'IIALCAGE', 'IRALCYFU', 'IIALCYFU', 'IRMJAGE', 'IIMJAGE', 'IRMJYFU', 'IIMJYFU', 'IRCOCAGE', 'IICOCAGE', 'IRCOCYFU', 'IICOCYFU', 'IRCRKAGE', 'IICRKAGE', 'IRCRKYFU', 'IICRKYFU', 'IRHERAGE', 'IIHERAGE', 'IRHERYFU', 'IIHERYFU', 'IRHALLUCAGE', 'IIHALLUCAGE', 'IRHALLUCYFU', 'IIHALLUCYFU', 'IRLSDAGE', 'IILSDAGE', 'IRLSDYFU', 'IILSDYFU', 'IRPCPAGE', 'IIPCPAGE', 'IRPCPYFU', 'IIPCPYFU', 'IRECSTMOAGE', 'IIECSTMOAGE', 'IRECSTMOYFU', 'IIECSTMOYFU', 'IRINHALAGE', 'IIINHALAGE', 'IRINHALYFU', 'IIINHALYFU', 'IRMETHAMAGE', 'IIMETHAMAGE', 'IRMETHAMYFU', 'IIMETHAMYFU', 'IRPNRNMINIT', 'IIPNRNMINIT', 'IRTRQNMINIT', 'IITRQNMINIT', 'IRSTMNMINIT', 'IISTMNMINIT', 'IRSEDNMINIT', 'IISEDNMINIT', 'IRPNRNMYFU', 'IIPNRNMYFU', 'IRPNRNMAGE', 'IIPNRNMAGE', 'IRTRQNMYFU', 'IITRQNMYFU', 'IRTRQNMAGE', 'IITRQNMAGE', 'IRSTMNMYFU', 'IISTMNMYFU', 'IRSTMNMAGE', 'IISTMNMAGE', 'IRSEDNMYFU', 'IISEDNMYFU', 'IRSEDNMAGE', 'IISEDNMAGE', 'CIGFLAG', 'CIGYR', 'CIGMON', 'CGRFLAG', 'CGRYR', 'CGRMON', 'PIPFLAG', 'PIPMON', 'SMKLSSFLAG', 'SMKLSSYR', 'SMKLSSMON', 'TOBFLAG', 'TOBYR', 'TOBMON', 'ALCFLAG', 'ALCYR', 'ALCMON', 'MRJFLAG', 'MRJYR', 'MRJMON', 'COCFLAG', 'COCYR', 'COCMON', 'CRKFLAG', 'CRKYR', 'CRKMON', 'HERFLAG', 'HERYR', 'HERMON', 'HALLUCFLAG', 'HALLUCYR', 'HALLUCMON', 'LSDFLAG', 'LSDYR', 'LSDMON', 'PCPFLAG', 'PCPYR', 'PCPMON', 'ECSTMOFLAG', 'ECSTMOYR', 'ECSTMOMON', 'DAMTFXFLAG', 'DAMTFXYR', 'DAMTFXMON', 'KETMINFLAG', 'KETMINYR', 'KETMINMON', 'SALVIAFLAG', 'SALVIAYR', 'SALVIAMON', 'INHALFLAG', 'INHALYR', 'INHALMON', 'METHAMFLAG', 'METHAMYR', 'METHAMMON', 'PNRANYFLAG', 'PNRANYYR', 'OXYCNANYYR', 'TRQANYFLAG', 'TRQANYYR', 'STMANYFLAG', 'STMANYYR', 'SEDANYFLAG', 'SEDANYYR', 'TQSDANYFLG', 'TQSDANYYR', 'PSYANYFLAG', 'PSYANYYR', 'PNRNMFLAG', 'PNRNMYR', 'PNRNMMON', 'OXYCNNMYR', 'TRQNMFLAG', 'TRQNMYR', 'TRQNMMON', 'STMNMFLAG', 'STMNMYR', 'STMNMMON', 'SEDNMFLAG', 'SEDNMYR', 'SEDNMMON', 'TQSDNMFLAG', 'TQSDNMYR', 'TQSDNMMON', 'PSYCHFLAG', 'PSYCHYR', 'PSYCHMON', 'OPINMYR', 'OPINMMON', 'HERPNRYR', 'ILLFLAG', 'ILLYR', 'ILLMON', 'MJONLYFLAG', 'MJONLYYR', 'MJONLYMON', 'ILLEMFLAG', 'ILLEMYR', 'ILLEMMON', 'CDUFLAG', 'DCIGMON', 'CDCGMO', 'CDNOCGMO', 'BNGDRKMON', 'HVYDRKMON', 'ILTOBALCFG', 'ILTOBALCYR', 'ILTOBALCMN', 'ILLALCMON', 'TOBALCFLG', 'TOBALCYR', 'TOBALCMN', 'ILLANDALC', 'ILLORALC', 'ILLALCFLG', 'PEYOTE2', 'MESC2', 'PSILCY2', 'AMYLNIT2', 'CLEFLU2', 'GAS2', 'GLUE2', 'ETHER2', 'SOLVENT2', 'LGAS2', 'NITOXID2', 'FELTMARKR2', 'SPPAINT2', 'AIRDUSTER2', 'OTHAEROS2', 'HYDCPDAPYU', 'ZOHYANYYR2', 'OXCOPDAPYU', 'TRAMPDAPYU', 'CODEPDAPYU', 'MORPPDAPYU', 'FENTPDAPYU', 'BUPRPDAPYU', 'OXYMPDAPYU', 'DEMEPDAPYU', 'HYDMPDAPYU', 'MTDNPDAPYU', 'PNROTANYR2', 'TRBENZAPYU', 'ALPRPDAPYU', 'LORAPDAPYU', 'CLONPDAPYU', 'DIAZPDAPYU', 'MUSRLXAPYU', 'CYCLPDAPYU', 'SOMAPDAPYU', 'TRQOTANYR2', 'AMMEPDAPYU', 'AMPHETAPYU', 'METHPDAPYU', 'ANOSTMAPYU', 'PROVPDAPYU', 'STMOTANYR2', 'ZOLPPDAPYU', 'ESZOPDAPYU', 'ZALEPDAPYU', 'SVBENZAPYU', 'TRIAPDAPYU', 'TEMAPDAPYU', 'FLURPDAPYU', 'BARBITAPYU', 'SEDOTANYR2', 'BENZOSAPYU', 'HYDCPDPYMU', 'OXCOPDPYMU', 'TRAMPDPYMU', 'CODEPDPYMU', 'MORPPDPYMU', 'FENTPDPYMU', 'BUPRPDPYMU', 'OXYMPDPYMU', 'DEMEPDPYMU', 'HYDMPDPYMU', 'MTDNPDPYMU', 'PNROTHPYMU2', 'TRBENZPYMU', 'ALPRPDPYMU', 'LORAPDPYMU', 'CLONPDPYMU', 'DIAZPDPYMU', 'MUSRLXPYMU', 'CYCLPDPYMU', 'SOMAPDPYMU', 'TRQOTHPYMU2', 'AMMEPDPYMU', 'AMPHETPYMU', 'METHPDPYMU', 'ANOSTMPYMU', 'PROVPDPYMU', 'STMOTHPYMU2', 'ZOLPPDPYMU', 'ESZOPDPYMU', 'SVBENZPYMU', 'SEDOTHPYMU2', 'BENZOSPYMU', 'ALCYDAYS', 'MRJYDAYS', 'COCYDAYS', 'CRKYDAYS', 'HERYDAYS', 'HALLNDAYYR', 'INHNDAYYR', 'METHNDAYYR', 'CIGMDAYS', 'CGRMDAYS', 'SMKLSMDAYS', 'ALCMDAYS', 'MRJMDAYS', 'COCMDAYS', 'CRKMDAYS', 'HERMDAYS', 'HALLNDAYPM', 'INHNDAYPM', 'METHNDAYPM', 'PNRNDAYPM', 'TRQNDAYPM', 'STMNDAYPM', 'SEDNDAYPM', 'BNGDRMDAYS', 'CIGPDAY', 'CIG1PACK', 'CIGAVGD', 'CIGAVGM', 'ALCNUMDKPM', 'FUCIG18', 'FUCIG21', 'FUCD218', 'FUCD221', 'FUCGR18', 'FUCGR21', 'FUSMKLSS18', 'FUSMKLSS21', 'FUALC18', 'FUALC21', 'FUMJ18', 'FUMJ21', 'FUCOC18', 'FUCOC21', 'FUCRK18', 'FUCRK21', 'FUHER18', 'FUHER21', 'FUHALLUC18', 'FUHALLUC21', 'FULSD18', 'FULSD21', 'FUPCP18', 'FUPCP21', 'FUECSTMO18', 'FUECSTMO21', 'FUINHAL18', 'FUINHAL21', 'FUMETHAM18', 'FUMETHAM21', 'FUPNRNM18', 'FUPNRNM21', 'FUTRQNM18', 'FUTRQNM21', 'FUSTMNM18', 'FUSTMNM21', 'FUSEDNM18', 'FUSEDNM21', 'PNRMAINRSN', 'TRQMAINRSN', 'STMMAINRSN', 'SEDMAINRSN', 'SRCPNRNM2', 'SRCTRQNM2', 'SRCSTMNM2', 'SRCSEDNM2', 'SRCFRPNRNM', 'SRCFRTRQNM', 'SRCFRSTMNM', 'SRCFRSEDNM', 'SRCCLFRPNR', 'SRCCLFRTRQ', 'SRCCLFRSTM', 'SRCCLFRSED', 'COLDMEDS', 'COLDREC', 'COLDYR1', 'COLDYR2', 'COLDYR3', 'COLDYR4', 'COLDYR5', 'OTCFLAG', 'GHB', 'GHBREC', 'COCNEEDL', 'CONDLREC', 'HERSMOKE', 'HRSMKREC', 'HERSNIFF', 'HRSNFREC', 'HERNEEDL', 'HEOTSMK', 'HEOTSNF', 'HEOTNDL', 'HEOTOTH', 'HEOTSP', 'HRNDLREC', 'METHNEEDL', 'METHNDLRC', 'OTDGNEDL', 'OTDGNDLA', 'OTDGNDLB', 'OTDGNDLC', 'OTDGNDLD', 'OTDGNDLE', 'OTDGNDLRC', 'GNNDREUS', 'GNNDLSH1', 'GNNDCLEN', 'GNNDLSH2', 'GNNDGET2', 'ANYNDLREC', 'CHMNDLREC', 'ANYNEEDL', 'NEDHER', 'NEDCOC', 'METHNEEDL2', 'HERSMOK2', 'HERSNIF2', 'COLDFLGR', 'COLDYRR', 'COLDMONR', 'GHBFLGR', 'GHBYRR', 'GHBMONR', 'RSKCIGPKD', 'RSKMRJMON', 'RSKMRJWK', 'RSKLSDTRY', 'RSKLSDWK', 'RSKHERTRY', 'RSKHERWK', 'RSKCOCMON', 'RSKCOCWK', 'RSKBNGDLY', 'RSKBNGWK', 'DIFGETMRJ', 'DIFGETLSD', 'DIFGETCOC', 'DIFGETCRK', 'DIFGETHER', 'APPDRGMON', 'RSKYFQDGR', 'RSKYFQTES', 'RKFQPBLT', 'RKFQDBLT', 'GRSKCIGPKD', 'GRSKMRJMON', 'GRSKMRJWK', 'GRSKCOCMON', 'GRSKCOCWK', 'GRSKHERTRY', 'GRSKHERWK', 'GRSKLSDTRY', 'GRSKLSDWK', 'GRSKBNGDLY', 'GRSKBNGWK', 'DIFOBTMRJ', 'DIFOBTCOC', 'DIFOBTCRK', 'DIFOBTHER', 'DIFOBTLSD', 'APPDRGMON2', 'BLNTEVER', 'BLNTAGE', 'BLNTYFU', 'BLNTMFU', 'BLNTREC', 'BLRECFL2', 'BLNT30DY','BLNT30C1', 'BLNT30C2', 'RSNOMRJ', 'RSNMRJMO', 'BLNTNOMJ', 'MEDMJYR', 'MEDMJALL', 'MEDMJPA2', 'CIGIRTBL', 'CIGCRAVE', 'CIGCRAGP', 'CIGINCTL', 'CIGAVOID', 'CIGFNSMK', 'CIGFNLKE', 'CIGPLANE', 'CIGRNOUT', 'CIGREGDY', 'CIGREGWK', 'CIGREGNM', 'CIGNMCHG', 'CIGSVLHR', 'CIGINFLU', 'CIGNOINF', 'CIGINCRS', 'CIGSATIS', 'CIGLOTMR', 'CIGWAKE', 'ALCLOTTM', 'ALCGTOVR', 'ALCLIMIT', 'ALCKPLMT', 'ALCNDMOR', 'ALCLSEFX', 'ALCCUTDN', 'ALCCUTEV', 'ALCCUT1X', 'ALCWD2SX', 'ALCWDSMT', 'ALCEMOPB', 'ALCEMCTD', 'ALCPHLPB', 'ALCPHCTD', 'ALCLSACT', 'ALCSERPB', 'ALCPDANG', 'ALCLAWTR', 'ALCFMFPB', 'ALCFMCTD', 'MRJLOTTM', 'MRJGTOVR', 'MRJLIMIT', 'MRJKPLMT', 'MRJNDMOR', 'MRJLSEFX', 'MRJCUTDN', 'MRJCUTEV', 'MRJEMOPB', 'MRJEMCTD', 'MRJPHLPB', 'MRJPHCTD', 'MRJLSACT', 'MRJSERPB', 'MRJPDANG', 'MRJLAWTR', 'MRJFMFPB', 'MRJFMCTD', 'COCLOTTM', 'COCGTOVR', 'COCLIMIT', 'COCKPLMT', 'COCNDMOR', 'COCLSEFX', 'COCCUTDN', 'COCCUTEV', 'COCCUT1X', 'COCFLBLU', 'COCWD2SX', 'COCWDSMT', 'COCEMOPB', 'COCEMCTD', 'COCPHLPB', 'COCPHCTD', 'COCLSACT', 'COCSERPB', 'COCPDANG', 'COCLAWTR', 'COCFMFPB', 'COCFMCTD', 'HERLOTTM', 'HERGTOVR', 'HERLIMIT', 'HERKPLMT', 'HERNDMOR', 'HERLSEFX', 'HERCUTDN', 'HERCUTEV', 'HERCUT1X', 'HERWD3SX', 'HERWDSMT', 'HEREMOPB', 'HEREMCTD', 'HERPHLPB', 'HERPHCTD', 'HERLSACT', 'HERSERPB', 'HERPDANG', 'HERLAWTR', 'HERFMFPB', 'HERFMCTD', 'HALULOTTM', 'HALUGTOVR', 'HALULIMIT', 'HALUKPLMT', 'HALUNDMOR', 'HALULSEFX', 'HALUCUTDN', 'HALUCUTEV', 'HALUEMOPB', 'HALUEMCTD', 'HALUPHLPB', 'HALUPHCTD', 'HALULSACT', 'HALUSERPB', 'HALUPDANG', 'HALULAWTR', 'HALUFMFPB', 'HALUFMCTD', 'INHLLOTTM', 'INHLGTOVR', 'INHLLIMIT', 'INHLKPLMT', 'INHLNDMOR', 'INHLLSEFX', 'INHLCUTDN', 'INHLCUTEV', 'INHLEMOPB', 'INHLEMCTD', 'INHLPHLPB', 'INHLPHCTD', 'INHLLSACT', 'INHLSERPB', 'INHLPDANG', 'INHLLAWTR', 'INHLFMFPB', 'INHLFMCTD', 'METHLOTTM', 'METHGTOVR']\n",
    "col_names_2 = ['METHLIMIT', 'METHKPLMT', 'METHNDMOR', 'METHLSEFX', 'METHCUTDN', 'METHCUTEV', 'METHCUT1X', 'METHFLBLU', 'METHWD2SX', 'METHWDSMT', 'METHEMOPB', 'METHEMCTD', 'METHPHLPB', 'METHPHCTD', 'METHLSACT', 'METHSERPB', 'METHPDANG', 'METHLAWTR', 'METHFMFPB', 'METHFMCTD', 'PNRLLOTTM', 'PNRLGTOVR', 'PNRLLIMIT', 'PNRLKPLMT', 'PNRLNDMOR', 'PNRLLSEFX', 'PNRLCUTDN', 'PNRLCUTEV', 'PNRLCUT1X', 'PNRLWD3SX', 'PNRLWDSMT', 'PNRLEMOPB', 'PNRLEMCTD', 'PNRLPHLPB', 'PNRLPHCTD', 'PNRLLSACT', 'PNRLSERPB', 'PNRLPDANG', 'PNRLLAWTR', 'PNRLFMFPB', 'PNRLFMCTD', 'TRQLLOTTM', 'TRQLGTOVR', 'TRQLLIMIT', 'TRQLKPLMT', 'TRQLNDMOR', 'TRQLLSEFT', 'TRQLCUTDN', 'TRQLCUTEV', 'TRQLEMOPB', 'TRQLEMCTD', 'TRQLPHLPB', 'TRQLPHCTD', 'TRQLLSACT', 'TRQLSERPB', 'TRQLPDANG', 'TRQLLAWTR', 'TRQLFMFPB', 'TRQLFMCTD', 'STIMLOTTM', 'STIMGTOVR', 'STIMLIMIT', 'STIMKPLMT', 'STIMNDMOR', 'STIMLSEFX', 'STIMCUTDN', 'STIMCUTEV', 'STIMCUT1X', 'STIMFLBLU', 'STIMWD2SX', 'STIMWDSMT', 'STIMEMOPB', 'STIMEMCTD', 'STIMPHLPB', 'STIMPHCTD', 'STIMLSACT', 'STIMSERPB', 'STIMPDANG', 'STIMLAWTR', 'STIMFMFPB', 'STIMFMCTD', 'SEDVLOTTM', 'SEDVGTOVR', 'SEDVLIMIT', 'SEDVKPLMT', 'SEDVNDMOR', 'SEDVLSEFX', 'SEDVCUTDN', 'SEDVCUTEV', 'SEDVCUT1X', 'SEDVWD1SX', 'SEDVWDSMT', 'SEDVEMOPB', 'SEDVEMCTD', 'SEDVPHLPB', 'SEDVPHCTD', 'SEDVLSACT', 'SEDVSERPB', 'SEDVPDANG', 'SEDVLAWTR', 'SEDVFMFPB', 'SEDVFMCTD', 'DEPENDHAL', 'DEPENDINH', 'DEPENDMTH', 'DEPENDPNR', 'DEPENDTRQ', 'DEPENDSTM', 'DEPENDSED', 'ABUPOSHAL', 'ABUPOSINH', 'ABUPOSMTH', 'ABUPOSPNR', 'ABUPOSTRQ', 'ABUPOSSTM', 'ABUPOSSED', 'IRCGIRTB', 'IICGIRTB', 'IRCGCRV', 'IICGCRV', 'IRCGCRGP', 'IICGCRGP', 'IRCGNCTL', 'IICGNCTL', 'IRCGAVD', 'IICGAVD', 'IRCGPLN', 'IICGPLN', 'IRCGROUT', 'IICGROUT', 'IRCGRGDY', 'IICGRGDY', 'IRCGRGWK', 'IICGRGWK', 'IRCGRGNM', 'IICGRGNM', 'IRCGNCG', 'IICGNCG', 'IRCGSLHR', 'IICGSLHR', 'IRCGINFL', 'IICGINFL', 'IRCGNINF', 'IICGNINF', 'IRCGINCR', 'IICGINCR', 'IRCGSAT', 'IICGSAT', 'IRCGLMR', 'IICGLMR', 'IRDEPENDHAL', 'IIDEPENDHAL', 'IRDEPENDINH', 'IIDEPENDINH', 'IRDEPENDMTH', 'IIDEPENDMTH', 'IRDEPENDPNR', 'IIDEPENDPNR', 'IRDEPENDTRQ', 'IIDEPENDTRQ', 'IRDEPENDSTM', 'IIDEPENDSTM', 'IRDEPENDSED', 'IIDEPENDSED', 'IRABUPOSHAL', 'IIABUPOSHAL', 'IRABUPOSINH', 'IIABUPOSINH', 'IRABUPOSMTH', 'IIABUPOSMTH', 'IRABUPOSPNR', 'IIABUPOSPNR', 'IRABUPOSTRQ', 'IIABUPOSTRQ', 'IRABUPOSSTM', 'IIABUPOSSTM', 'IRABUPOSSED', 'IIABUPOSSED', 'NDSSANSP', 'NDSSDNSP', 'FTNDDNSP', 'DNICNSP', 'DEPNDALC', 'DEPNDMRJ', 'DEPNDCOC', 'DEPNDHER', 'DEPNDPYHAL', 'DEPNDPYINH', 'DEPNDPYMTH', 'DEPNDPYPNR', 'DEPNDPYTRQ', 'DEPNDPYSTM', 'DEPNDPYSED', 'DEPNDPYPSY', 'DEPNDPYILL', 'DEPNDPYIEM', 'DPPYILLALC', 'ABUSEALC', 'ABUSEMRJ', 'ABUSECOC', 'ABUSEHER', 'ABUSEPYHAL', 'ABUSEPYINH', 'ABUSEPYMTH', 'ABUSEPYPNR', 'ABUSEPYTRQ', 'ABUSEPYSTM', 'ABUSEPYSED', 'ABUSEPYPSY', 'ABUSEPYILL', 'ABUSEPYIEM', 'ABPYILLALC', 'ABODALC', 'ABODMRJ', 'ABODCOC', 'ABODHER', 'UDPYHAL', 'UDPYINH', 'UDPYMTH', 'UDPYPNR', 'UDPYTRQ', 'UDPYSTM', 'UDPYSED', 'UDPYTRQSED', 'UDPYPSY', 'UDPYOPI', 'UDPYHRPNR', 'UDPYILL', 'UDPYIEM', 'UDPYILAL', 'UDPYILAAL', 'DUDNOAUD', 'AUDNODUD', 'BOOKED', 'NOBOOKY2', 'BKMVTHFT', 'BKLARCNY', 'BKBURGL', 'BKSRVIOL', 'BKSMASLT', 'BKROB', 'BKARSON', 'BKDRVINF', 'BKDRUNK', 'BKPOSTOB', 'BKDRUG', 'BKSEXNR', 'BKFRAUD', 'BKOTH', 'BKOTHOF2', 'PROBATON', 'PAROLREL', 'DRVINALCO', 'DRVINMARJ', 'DRVINCOCN', 'DRVINHERN', 'DRVINHALL', 'DRVININHL', 'DRVINMETH', 'DRVINALON', 'MXMJPNLT', 'DRVINALCO2', 'DRVINMARJ2', 'DRVINDRG', 'DRVINDROTMJ', 'DRVINALDRG', 'PAROL', 'PROB', 'MRJYRBFR', 'MRJAGLST', 'MRJYLU', 'MRJMLU', 'CIGAGLST', 'CIGYLU', 'CIGMLU', 'CIGDLLST', 'CIGDLYLU', 'CIGDLMLU', 'SMKAGLAST', 'SMKYRLAST', 'SMKMOLAST', 'CGRAGLST', 'CIGARYLU', 'CIGARMLU', 'ALCAGLST', 'ALCYLU', 'ALCMLU', 'COCAGLST', 'COCYLU', 'COCMLU', 'CRKAGLST', 'CRKYLU', 'CRKMLU', 'HERAGLST', 'HERYLU', 'HERMLU', 'HALLAGLST', 'HALLYRLST', 'HALLMOLST', 'LSDAGLST', 'LSDYLU', 'LSDMLU', 'PCPAGLST', 'PCPYLU', 'PCPMLU', 'ECSTMOAGL', 'ECSTMOYLU', 'ECSTMOMLU', 'INHLAGLST', 'INHLYRLST', 'INHLMOLST', 'METHAGLST', 'METHYRLST', 'METHMOLST', 'CIGYRBFR', 'ALCYRBFR', 'COCYRBFR', 'TXEVRRCVD', 'TXYRRECVD', 'TXYRALDGB', 'TXYRHOSOV', 'TXYRHOSAD', 'TXYRRESOV', 'TXYRRESAD', 'TXYROUTPT', 'TXYROUTAD', 'TXYRMHCOP', 'TXYRMHCAD', 'TXYREMRGN', 'TXYREMRAD', 'TXYRDRPRV', 'TXYRDRPAD', 'TXYRPRISN', 'TXYRPRIAD', 'TXYRSLFHP', 'TXYRSLFAD', 'TXYROTHER', 'TXYROTHSP2', 'TXYROTHAD', 'TXYRERDRG', 'TXYRERNUM2', 'TXCURRENT', 'NDTXYRADG', 'NDMORTXYR', 'NDMORTALC', 'NDMORTMRJ', 'NDMORTCOC', 'NDMORTHER', 'NDMORTHAL', 'NDMORTINH', 'NDMORTMTH', 'NDMORTPNR', 'NDMORTTRQ', 'NDMORTSTM', 'NDMORTSED', 'NDMORTOTH', 'NDTXYRALC', 'NDTXYRMRJ', 'NDTXYRCOC', 'NDTXYRHER', 'NDTXYRHAL', 'NDTXYRINH', 'NDTXYRMTH', 'NDTXYRPNR', 'NDTXYRTRQ', 'NDTXYRSTM', 'NDTXYRSED', 'NDTXYROTH', 'NDTXYOTH1', 'NDTXYOTH2', 'NDTXYOTH3', 'NDTXYOTH4', 'NDTXYOTH5', 'NDTXEFFRT', 'NDTXNOCOV', 'NDTXNOTPY', 'NDTXTSPHR', 'NDTXWANTD', 'NDTXNSTOP', 'NDTXPFULL', 'NDTXDKWHR', 'NDTXNBRNG', 'NDTXJOBNG', 'NDTXNONED', 'NDTXHANDL', 'NDTXNOHLP', 'NDTXNTIME', 'NDTXFNDOU', 'NDTXOTRSN', 'NDTXMIMPT', 'NDMREFFRT', 'NDMRNOCOV', 'NDMRNOTPY', 'NDMRTSPHR', 'NDMRWANTD', 'NDMRNSTOP', 'NDMRPFULL', 'NDMRDKWHR', 'NDMRNBRNG', 'NDMRJOBNG', 'NDMRNONED', 'NDMRHANDL', 'NDMRNOHLP', 'NDMRNTIME', 'NDMRFNDOU', 'NDMROTRSN', 'NDMRMIMPT', 'TXRCVDREC', 'TXLTYMNPL2', 'TXLTYALCO', 'TXLTYMRJH', 'TXLTYCOCN', 'TXLTYHERN', 'TXLTYHALL', 'TXLTYINHL', 'TXLTYMETH', 'TXLTYPNRL', 'TXLTYTRQL', 'TXLTYSTIM', 'TXLTYSEDV', 'TXLTYOTHR', 'TXLTYMAIN2', 'TXLTYOCOM2', 'TXLTYDAYS2', 'TXPAYHINS', 'TXPAYMCRE', 'TXPAYMCAD', 'TXPAYPUBL', 'TXPAYSVNG', 'TXPAYFAML', 'TXPAYCOUR', 'TXPAYMILT', 'TXPAYBOSS', 'TXPAYOTHR', 'TXPAYOTSP2', 'TXPAYFREE', 'TXENRLOCT', 'TXYRONDTX', 'TXALCONLY', 'TXALCONAG', 'TXDRGONLY', 'TXDRGONAG', 'TXALCDRGU', 'TXALCDAGE', 'TXDRGALCU', 'TXDRGAAGE', 'TXYALONAG', 'TXYALODRG', 'TXYALODAG', 'TXYDRONAG', 'TXYDROALC', 'TXYDROAAG', 'TXYALDAAG', 'TXYALDDAG', 'TXFGALAGE', 'TXFGDGAGE', 'TXFGADAGE', 'TXSHGWENT', 'TXSHGALDB', 'TXSHGFLAG', 'TXEVRRCVD2', 'TXYRALC', 'TXYRILL', 'TXYRALNIL', 'TXYRILNAL', 'TXYRRECVD2', 'TXYRILANAL', 'TXLTYALCO2', 'TXLTYMRJH2', 'TXLTYCOCN2', 'TXLTYHERN2', 'TXLTYHALL2', 'TXLTYINHL2', 'TXLTYMETH2', 'TXLTYPNRL2', 'TXLTYTRQL2', 'TXLTYSTIM2', 'TXLTYSEDV2', 'TXLTYILL', 'TXPAYHINS2', 'TXPAYMCRE2', 'TXPAYMCAD2', 'TXPAYPUBL2', 'TXPAYSVNG2', 'TXPAYFAML2', 'TXPAYCOUR2', 'TXPAYMILT2', 'TXPAYBOSS2', 'TXPDHINSAL', 'TXPDMCREAL', 'TXPDMCADAL', 'TXPDPUBLAL', 'TXPDSVNGAL', 'TXPDFAMLAL', 'TXPDCOURAL', 'TXPDMILTAL', 'TXPDBOSSAL', 'TXPDHINSIL', 'TXPDMCREIL', 'TXPDMCADIL', 'TXPDPUBLIL', 'TXPDSVNGIL', 'TXPDFAMLIL', 'TXPDCOURIL', 'TXPDMILTIL', 'TXPDBOSSIL', 'TXYRSPALC', 'TXYRSPILL', 'TXYSPALNIL', 'TXYSPILNAL', 'TXYRSPILAL', 'TXYSILANAL', 'TXLTCURRSP', 'TXYRHOSAL', 'TXYRRESAL', 'TXYROUTAL', 'TXYRMHCAL', 'TXYREMRAL', 'TXYRDRPAL', 'TXYRPRIAL', 'TXYRSLFAL', 'TXYRHOSIL', 'TXYRRESIL', 'TXYROUTIL', 'TXYRMHCIL', 'TXYREMRIL', 'TXYRDRPIL', 'TXYRPRIIL', 'TXYRSLFIL', 'TXYRHOSOV2', 'TXYRRESOV2', 'TXYROUTPT2', 'TXYRMHCOP2', 'TXYREMRGN2', 'TXYRDRPRV2', 'TXYRPRISN2', 'TXYRSLFHP2', 'TXYRNDALC', 'TXYRNDILL', 'TXYRNDILAL', 'NDFLTXALC', 'NDFLTXILL', 'NDFLTXILAL', 'NDTXEFTALC', 'NDTXEFTILL', 'NDTXEFILAL', 'TXYRNOSPAL', 'TXYRNOSPIL', 'TXYNSPILAL', 'NDTRNNOCOV', 'NDTRNNOTPY', 'NDTRNTSPHR', 'NDTRNWANTD', 'NDTRNNSTOP', 'NDTRNPFULL', 'NDTRNDKWHR', 'NDTRNNBRNG', 'NDTRNJOBNG', 'NDTRNNONED', 'NDTRNHANDL', 'NDTRNNOHLP', 'NDTRNNTIME', 'NDTRNFNDOU', 'NDTRNMIMPT', 'PREGNANT', 'HTANSWER', 'HTINCHE2', 'WTANSWER', 'WTPOUND2', 'NMERTMT2', 'INHOSPYR', 'NMNGTHS2', 'NMVSOPT2', 'NMVSOEST', 'HPUSETOB', 'HPUSEALC', 'HPUSEDRG', 'HPQTTOB', 'HPALCAMT', 'HPALCFRQ', 'HPALCPRB', 'HPALCCUT', 'HPALCTX', 'HPALCNOT', 'HPDRGTALK', 'STDANYYR', 'HRTCONDEV', 'DIABETEVR', 'COPDEVER', 'CIRROSEVR', 'HEPBCEVER', 'KIDNYDSEV', 'ASTHMAEVR', 'HIVAIDSEV', 'CANCEREVR', 'HIGHBPEVR', 'NONABOVEV', 'CABLADDER', 'CABLOLEULYM', 'CAOTHER2', 'CABREAST', 'CACERVIX', 'CACOLNRECT', 'CAESOPSTOM', 'CAGALLIVPAN', 'CAKIDNEY', 'CALARYLUNG', 'CAMELANOM', 'CAMOUTTHRO', 'CAOVARY', 'CAPROSTEST', 'CASKINOTH', 'CASKINDK', 'CATHYROID', 'CAUTERUS', 'CANCERYR', 'HRTCONDAG', 'HRTCONDYR', 'DIABETEAG', 'COPDAGE', 'CIRROSAGE', 'HEPBCAGE', 'KIDNYDSAG', 'ASTHMAAGE', 'ASTHMANOW', 'HIVAIDSAG', 'HIGHBPMED', 'HIGHBPAGE', 'PREG', 'PREG2', 'TRIMEST', 'BMI2']\n",
    "col_names_3 = ['AUINPYR', 'AUINPSYH', 'AUINPGEN', 'AUINMEDU', 'AUINAHSP', 'AUINRESD', 'AUINSFAC', 'AUNMPSY2', 'AUNMPGE2', 'AUNMMED2', 'AUNMAHS2', 'AUNMRES2', 'AUNMSFA2', 'AUPINSLF', 'AUPINOFM', 'AUPINPHI', 'AUPINMCR', 'AUPINMCD', 'AUPINREH', 'AUPINEMP', 'AUPINMIL', 'AUPINPUB', 'AUPINPRV', 'AUPINFRE', 'AUPINFM2', 'AUOPTYR', 'AUOPMENT', 'AUOPTHER', 'AUOPDOC', 'AUOPCLNC', 'AUOPDTMT', 'AUOPOTOP', 'AUOPYRS2', 'AUNMMEN2', 'AUNMTHE2', 'AUNMDOC2', 'AUNMCLN2', 'AUNMDTM2', 'AUNMOTO2', 'AUPOPSLF', 'AUPOPOFM', 'AUPOPPHI', 'AUPOPMCR', 'AUPOPMCD', 'AUPOPREH', 'AUPOPEMP', 'AUPOPMIL', 'AUPOPPUB', 'AUPOPPRV', 'AUPOPFRE', 'AUPOPMOS', 'AUPOPAMT', 'AURXYR', 'AUUNMTYR', 'AUUNCOST', 'AUUNNBR', 'AUUNJOB', 'AUUNNCOV', 'AUUNENUF', 'AUUNWHER', 'AUUNCFID', 'AUUNCMIT', 'AUUNNOND', 'AUUNHNDL', 'AUUNNHLP', 'AUUNBUSY', 'AUUNFOUT', 'AUUNNTSP', 'AUUNSOR', 'AUUNRIM2', 'AUALTYR', 'AUALACUP', 'AUALCHIR', 'AUALHERB', 'AUALSGRP', 'AUALINET', 'AUALRELG', 'AUALHLIN', 'AUALMASG', 'AUALOTH', 'AUALOTS2', 'AUMOTVYR', 'AMHINP2', 'AMHOUTP3', 'AMHRX2', 'AMHTXRC3', 'AMHSVTYP', 'AMHTXND2', 'AMHTXAND', 'MHLMNT3', 'MHLTHER3', 'MHLDOC3', 'MHLCLNC3', 'MHLDTMT3', 'MHLSCHL3', 'MHLOTH3', 'MHPDSLF2', 'MHPDOFM2', 'MHPDPHI2', 'MHPDMCR2', 'MHPDMCD2', 'MHPDREH2', 'MHPDEMP2', 'MHPDMIL2', 'MHPDPUB2', 'MHPDPRV2', 'MHPDFRE2', 'MHRCOST2', 'MHRNBRS2', 'MHRJOBS2', 'MHRNCOV2', 'MHRENUF2', 'MHRWHER2', 'MHRCFID2', 'MHRCMIT2', 'MHRNOND2', 'MHRHAND2', 'MHRNOHP2', 'MHRTIME2', 'MHRFOUT2', 'MHRTRAN2', 'MHRSOTH2', 'RCVMHOSPTX', 'RCVMHNSPTX', 'RCVSPTXNMH', 'RCVMHASPTX', 'SNYSELL', 'SNYSTOLE', 'SNYATTAK', 'SNFAMJEV', 'SNRLGSVC', 'SNRLGIMP', 'SNRLDCSN', 'SNRLFRND', 'YEATNDYR', 'YEHMSLYR', 'YESCHFLT', 'YESCHWRK', 'YESCHIMP', 'YESCHINT', 'YETCGJOB', 'YELSTGRD', 'YESTSCIG', 'YESTSMJ', 'YESTSALC', 'YESTSDNK', 'YEPCHKHW', 'YEPHLPHW', 'YEPCHORE', 'YEPLMTTV', 'YEPLMTSN', 'YEPGDJOB', 'YEPPROUD', 'YEYARGUP', 'YEYFGTSW', 'YEYFGTGP', 'YEYHGUN', 'YEYSELL', 'YEYSTOLE', 'YEYATTAK', 'YEPPKCIG', 'YEPMJEVR', 'YEPMJMO', 'YEPALDLY', 'YEGPKCIG', 'YEGMJEVR', 'YEGMJMO', 'YEGALDLY', 'YEFPKCIG', 'YEFMJEVR', 'YEFMJMO', 'YEFALDLY', 'YETLKNON', 'YETLKPAR', 'YETLKBGF', 'YETLKOTA', 'YETLKSOP', 'YEPRTDNG', 'YEPRBSLV', 'YEVIOPRV', 'YEDGPRGP', 'YESLFHLP', 'YEPRGSTD', 'YESCHACT', 'YECOMACT', 'YEFAIACT', 'YEOTHACT', 'YEDECLAS', 'YEDERGLR', 'YEDESPCL', 'YEPVNTYR', 'YERLGSVC', 'YERLGIMP', 'YERLDCSN', 'YERLFRND', 'SCHFELT', 'TCHGJOB', 'AVGGRADE', 'STNDSCIG', 'STNDSMJ', 'STNDALC', 'STNDDNK', 'PARCHKHW', 'PARHLPHW', 'PRCHORE2', 'PRLMTTV2', 'PARLMTSN', 'PRGDJOB2', 'PRPROUD2', 'ARGUPAR', 'YOFIGHT2', 'YOGRPFT2', 'YOHGUN2', 'YOSELL2', 'YOSTOLE2', 'YOATTAK2', 'PRPKCIG2', 'PRMJEVR2', 'PRMJMO', 'PRALDLY2', 'YFLPKCG2', 'YFLTMRJ2', 'YFLMJMO', 'YFLADLY2', 'FRDPCIG2', 'FRDMEVR2', 'FRDMJMON', 'FRDADLY2', 'TALKPROB', 'PRTALK3', 'PRBSOLV2', 'PREVIOL2', 'PRVDRGO2', 'GRPCNSL2', 'PREGPGM2', 'YTHACT2', 'DRPRVME3', 'ANYEDUC3', 'RLGATTD', 'RLGIMPT', 'RLGDCSN', 'RLGFRND', 'DSTNRV30', 'DSTHOP30', 'DSTRST30', 'DSTCHR30', 'DSTEFF30', 'DSTNGD30', 'DSTWORST', 'DSTNRV12', 'DSTHOP12', 'DSTRST12', 'DSTCHR12', 'DSTEFF12', 'DSTNGD12', 'IMPREMEM', 'IMPCONCN', 'IMPGOUT', 'IMPGOUTM', 'IMPPEOP', 'IMPPEOPM', 'IMPSOC', 'IMPSOCM', 'IMPHHLD', 'IMPHHLDM', 'IMPRESP', 'IMPRESPM', 'IMPWORK', 'IMPWEEKS', 'IMPDYFRQ', 'IMPYDAYS', 'SUICTHNK', 'SUICPLAN', 'SUICTRY', 'K6SCMON', 'SPDMON', 'K6SCYR', 'K6SCMAX', 'SPDYR', 'MHSUITHK', 'MHSUTK_U', 'MHSUIPLN', 'MHSUITRY', 'WSPDSC2', 'WHODASC2', 'WHODASC3', 'SMIPP_U', 'SMIYR_U', 'AMIYR_U', 'SMMIYR_U', 'MMIYR_U', 'LMIYR_U', 'LMMIYRU', 'MI_CAT_U', 'SMISUDPY', 'AMISUDPY', 'LMMISUDPY', 'ADDPREV', 'ADDSCEV', 'ADLOSEV', 'ADDPDISC', 'ADDPLSIN', 'ADDSLSIN', 'ADLSI2WK', 'ADDPR2WK', 'ADWRHRS', 'ADWRDST', 'ADWRCHR', 'ADWRIMP', 'ADDPPROB', 'ADWRPROB', 'ADWRAGE', 'ADWRDEPR', 'ADWRDISC', 'ADWRLSIN', 'ADWRPLSR', 'ADWRELES', 'ADWREMOR', 'ADWRGAIN', 'ADWRGROW', 'ADWRPREG', 'ADWRGNL2', 'ADWRLOSE', 'ADWRDIET', 'ADWRLSL2', 'ADWRSLEP', 'ADWRSMOR', 'ADWRENRG', 'ADWRSLOW', 'ADWRSLNO', 'ADWRJITT', 'ADWRJINO', 'ADWRTHOT', 'ADWRCONC', 'ADWRDCSN', 'ADWRNOGD', 'ADWRWRTH', 'ADWRDLOT', 'ADWRDBTR', 'ADWRSTHK', 'ADWRSPLN', 'ADWRSATP', 'AD_MDEA1', 'AD_MDEA2', 'AD_MDEA3', 'AD_MDEA4', 'AD_MDEA5', 'AD_MDEA6', 'AD_MDEA7', 'AD_MDEA8', 'AD_MDEA9', 'ADSMMDEA', 'ADPBINTF', 'ADPBDLYA', 'ADPBRMBR', 'ADPBAGE', 'ADPBNUM', 'ADPB2WK', 'ADPSHMGT', 'ADPSWORK', 'ADPSRELS', 'ADPSSOC', 'ADPSDAYS', 'ADSEEDOC', 'ADFAMDOC', 'ADOTHDOC', 'ADPSYCH', 'ADPSYMD', 'ADSOCWRK', 'ADCOUNS', 'ADOTHMHP', 'ADNURSE', 'ADRELIG', 'ADHERBAL', 'ADOTHHLP', 'ADTMTNOW', 'ADRX12MO', 'ADRXNOW', 'ADRXHLP', 'ADTMTHLP', 'AMDELT', 'AMDEYR', 'AMDEY2_U', 'ATXMDEYR', 'ARXMDEYR', 'AMDETXRX', 'ADOCMDE', 'AOMDMDE', 'APSY1MDE', 'APSY2MDE', 'ASOCMDE', 'ACOUNMDE', 'AOMHMDE', 'ANURSMDE', 'ARELMDE', 'AHBCHMDE', 'AOTHMDE', 'AHLTMDE', 'AALTMDE', 'ASDSHOM2', 'ASDSWRK2', 'ASDSREL2', 'ASDSSOC2', 'ASDSOVL2', 'AMDEIMP', 'YUHOSPYR', 'YUHOSPN2', 'YUHOSUIC', 'YUHODEPR', 'YUHOFEAR', 'YUHOBKRU', 'YUHOEATP', 'YUHOANGR', 'YUHOFITE', 'YUHOFMLY', 'YUHOFRND', 'YUHOOTPP', 'YUHOSCHL', 'YUHOSOR', 'YURSIDYR', 'YURSIDN2', 'YURSSUIC', 'YURSDEPR', 'YURSFEAR', 'YURSBKRU', 'YURSEATP', 'YURSANGR', 'YURSFITE', 'YURSFMLY', 'YURSFRND', 'YURSOTPP', 'YURSSCHL', 'YURSSOR', 'YUFCARYR', 'YUFCARN2', 'YUFCSUIC', 'YUFCDEPR', 'YUFCFEAR', 'YUFCBKRU', 'YUFCEATP', 'YUFCANGR', 'YUFCFITE', 'YUFCFMLY', 'YUFCFRND', 'YUFCOTPP', 'YUFCSCHL', 'YUFCSOR', 'YUDYTXYR', 'YUDYTXN2', 'YUDYSUIC', 'YUDYDEPR', 'YUDYFEAR', 'YUDYBKRU', 'YUDYEATP', 'YUDYANGR', 'YUDYFITE', 'YUDYFMLY', 'YUDYFRND', 'YUDYOTPP', 'YUDYSCHL', 'YUDYSOR', 'YUMHCRYR', 'YUMHCRN2', 'YUMHSUIC', 'YUMHDEPR', 'YUMHFEAR', 'YUMHBKRU', 'YUMHEATP', 'YUMHANGR', 'YUMHFITE', 'YUMHFMLY', 'YUMHFRND', 'YUMHOTPP', 'YUMHSCHL', 'YUMHSOR', 'YUTPSTYR', 'YUTPSTN2', 'YUTPSUIC', 'YUTPDEPR', 'YUTPFEAR', 'YUTPBKRU', 'YUTPEATP', 'YUTPANGR', 'YUTPFITE', 'YUTPFMLY', 'YUTPFRND', 'YUTPOTPP', 'YUTPSCHL', 'YUTPSOR', 'YUIHTPYR', 'YUIHTPN2', 'YUIHSUIC', 'YUIHDEPR', 'YUIHFEAR', 'YUIHBKRU', 'YUIHEATP', 'YUIHANGR', 'YUIHFITE', 'YUIHFMLY', 'YUIHFRND', 'YUIHOTPP', 'YUIHSCHL', 'YUIHSOR', 'YUFDOCYR', 'YUFDOCN2', 'YUFDSUIC', 'YUFDDEPR', 'YUFDFEAR', 'YUFDBKRU', 'YUFDEATP', 'YUFDANGR', 'YUFDFITE', 'YUFDFMLY', 'YUFDFRND', 'YUFDOTPP', 'YUFDSCHL', 'YUFDSOR', 'YUSWSCYR', 'YUSWSUIC', 'YUSWDEPR', 'YUSWFEAR', 'YUSWBKRU', 'YUSWEATP', 'YUSWANGR', 'YUSWFITE', 'YUSWFMLY', 'YUSWFRND', 'YUSWOTPP', 'YUSWSCHL', 'YUSWSOR', 'YUSCEMYR', 'YUSCPGYR', 'YUJVDTON', 'YUJVDTN2', 'YUJVDTYR', 'YHOSP', 'YRESID', 'YFOST', 'YDAYTRT', 'YCLIN', 'YTHER', 'YHOME', 'YPED', 'YSPEC', 'YSHSW', 'YJAIL', 'ANYMHIN2', 'ANYMHOUT', 'ANYSMH2', 'ANYNSMH', 'ANYMHED2', 'ANYSEDMF', 'HOSPVST', 'RESIDVST', 'FOSTVST', 'DYTXVST', 'CLINVST', 'THERVST', 'HOMEVST', 'SPINVST2', 'SPOUTVST', 'SMHVST2', 'SIMHSUI2', 'SIMHDPR2', 'SIMHFEA2', 'SIMHBRK2', 'SIMHEAT2', 'SIMHANG2', 'SIMHFIT2', 'SIMHFML2', 'SIMHFRD2', 'SIMHOTP2', 'SIMHSCH2', 'SIMHMEN2', 'SIMHOTH3', 'SOMHSUI', 'SOMHDPR', 'SOMHFEA', 'SOMHBRK', 'SOMHEAT', 'SOMHANGR', 'SOMHFITE', 'SOMHFMLY', 'SOMHFRND', 'SOMHOTPP', 'SOMHSCHL', 'SOMHMEND', 'SOMHOTH2', 'SMHSUI2', 'SMHDPR2', 'SMHFEA2', 'SMHBRK2', 'SMHEAT2', 'SMHANGR2', 'SMHFITE2', 'SMHFMLY2', 'SMHFRND2', 'SMHOTPP2', 'SMHSCHL2', 'SMHMEND2', 'SMHOTH3', 'SHSWSUI', 'SHSWDPR', 'SHSWFEA', 'SHSWBRK', 'SHSWEAT', 'SHSWANGR', 'SHSWFITE', 'SHSWFMLY', 'SHSWFRND', 'SHSWOTPP', 'SHSWSCHL', 'SHSWMEND', 'SHSWOTH2', 'FDOCSUI', 'FDOCDPR', 'FDOCFEA', 'FDOCBRK', 'FDOCEAT', 'FDOCANGR', 'FDOCFITE', 'FDOCFMLY', 'FDOCFRND', 'FDOCOTPP', 'FDOCSCHL', 'FDOCMEND', 'FDOCOTH2', 'YMHOSPTX', 'YMHNSPTX', 'YSPTXNMH', 'YMHASPTX', 'YODPREV', 'YODSCEV', 'YOLOSEV', 'YODPDISC', 'YODPLSIN', 'YODSLSIN', 'YOLSI2WK', 'YODPR2WK', 'YOWRHRS', 'YOWRDST', 'YOWRCHR', 'YOWRIMP', 'YODPPROB', 'YOWRPROB', 'YOWRAGE', 'YOWRDEPR', 'YOWRDISC', 'YOWRLSIN', 'YOWRPLSR', 'YOWRELES', 'YOWREMOR', 'YOWRGAIN', 'YOWRGROW', 'YOWRPREG', 'YOWRGNL2', 'YOWRLOSE', 'YOWRDIET', 'YOWRLSL2', 'YOWRSLEP', 'YOWRSMOR', 'YOWRENRG', 'YOWRSLOW', 'YOWRSLNO', 'YOWRJITT', 'YOWRJINO', 'YOWRTHOT', 'YOWRCONC', 'YOWRDCSN', 'YOWRNOGD', 'YOWRWRTH', 'YOWRDLOT', 'YOWRDBTR', 'YOWRSTHK', 'YOWRSPLN', 'YOWRSATP', 'YO_MDEA1', 'YO_MDEA2', 'YO_MDEA3', 'YO_MDEA4', 'YO_MDEA5', 'YO_MDEA6', 'YO_MDEA7', 'YO_MDEA8', 'YO_MDEA9', 'YODSMMDE', 'YOPBINTF', 'YOPBDLYA', 'YOPBRMBR', 'YOPBAGE', 'YOPBNUM', 'YOPB2WK']\n",
    "col_names_4 = ['YOPSHMGT', 'YOPSWORK', 'YOPSRELS', 'YOPSSOC', 'YOPSDAYS', 'YOSEEDOC', 'YOFAMDOC', 'YOOTHDOC', 'YOPSYCH', 'YOPSYMD', 'YOSOCWRK', 'YOCOUNS', 'YOOTHMHP', 'YONURSE', 'YORELIG', 'YOHERBAL', 'YOOTHHLP', 'YOTMTNOW', 'YORX12MO', 'YORXNOW', 'YORXHLP', 'YOTMTHLP', 'YMDELT', 'YMDEYR', 'YMDEAUDPY', 'YMDEIUDPY', 'YMDEUDPY', 'YTXMDEYR', 'YRXMDEYR', 'YMDETXRX', 'YDOCMDE', 'YOMDMDE', 'YPSY1MDE', 'YPSY2MDE', 'YSOCMDE', 'YCOUNMDE', 'YOMHMDE', 'YNURSMDE', 'YRELMDE', 'YHBCHMDE', 'YOTHMDE', 'YHLTMDE', 'YALTMDE', 'YMDEHPRX', 'YMDEHPO', 'YMDERXO2', 'YMDEHARX', 'YSDSHOME', 'YSDSWRK', 'YSDSREL', 'YSDSSOC', 'YSDSOVRL', 'MDEIMPY', 'YMDEIMAUD', 'YMDEIMIUD', 'YMDEIMUDPY', 'CADRLAST', 'CADRPEOP', 'CADRCAR', 'CADRHOME', 'CADROTHM', 'CADRPUBL', 'CADRBAR', 'CADREVNT', 'CADRSCHL', 'CADROTH', 'CADROTS2', 'CABUYFRE', 'CAGVMONY', 'CABUYWHO', 'CABPLACE', 'CABUNDAG', 'CAGVWHO', 'CAFREWHO', 'CAFRESP2', 'CADRKDRUG', 'CADRKMARJ', 'CADRKCOCN', 'CADRKHERN', 'CADRKHALL', 'CADRKINHL', 'CADRKMETH', 'CABINGFLG', 'CABINGEVR', 'CABINGAGE', 'CABINGYFU', 'CABINGMFU', 'EIBINGAGE', 'EIBINGYFU', 'EIBINGMFU', 'CASUPROB', 'CASURCVR', 'CAMHPROB', 'CAMHRCVR', 'KRATEVER', 'KRATREC', 'UADPEOP', 'UADCAR', 'UADHOME', 'UADOTHM', 'UADPUBL', 'UADBAR', 'UADEVNT', 'UADSCHL', 'UADROTH', 'UADOTSP', 'UADPAID', 'UADMONY', 'UADBWHO', 'UADPLACE', 'UADBUND', 'UADCAG', 'UADFWHO', 'UADFRD', 'CADRKMARJ2', 'CADRKCOCN2', 'CADRKHERN2', 'CADRKHALL2', 'CADRKINHL2', 'CADRKMETH2', 'CASUPROB2', 'RCVYSUBPRB', 'CAMHPROB2', 'RCVYMHPRB', 'ALMEDYR2', 'OPMEDYR2', 'ALOPMEDYR', 'KRATFLG', 'KRATYR', 'KRATMON', 'MMGETMJ', 'MMBTREC1', 'MMBTPYR', 'MMBTREC2', 'MMBT30DY', 'MMJNTLOO', 'MMJNTNUM', 'MMJNPCTB', 'MMJNPCAT', 'MMLSUNIT', 'MMLSGMS', 'MMLS10GM', 'MMLSOZS', 'MMLSLBS', 'MMLSPCTB', 'MMLSPCAT', 'MMBUYWHO', 'MMBPLACE', 'MMBPLOS2', 'MMBATJOB', 'MMBCLOSE', 'MMBSELL', 'MMBGIVE', 'MMTRADE', 'MMTRD30D', 'MMTRDREC', 'MMT30FRQ', 'MMTJNTLS', 'MMTJNTNM', 'MMTJWRCB', 'MMTJWRTH', 'MMTLUNIT', 'MMTLGMS', 'MMTLOZS', 'MMTLWRCB', 'MMTLWRTH', 'MMTRDWHO', 'MMTPLACE', 'MMTPLOS2', 'MMTCLOSE', 'MMTKEEP', 'MMTSELL', 'MMTGIVE', 'MMGKEEP', 'MMGSELL', 'MMGGIVE', 'MMFREWHO', 'MMFPLACE', 'MMFPLOS2', 'MMFCLOSE', 'MMFKEEP', 'MMFSELL', 'MMFGIVE', 'LANGVER', 'QUARTER', 'GQTYPE2', 'AGE2', 'NOMARR2', 'SERVICE', 'MILSTAT', 'ACTDEVER', 'ACTD2001', 'ACTD9001', 'ACTD7590', 'ACTDVIET', 'ACTDPRIV', 'COMBATPY', 'HEALTH', 'MOVSINPYR2', 'SEXATRACT', 'SEXIDENT', 'SPEAKENGL', 'DIFFHEAR', 'DIFFSEE', 'DIFFTHINK', 'DIFFWALK', 'DIFFDRESS', 'DIFFERAND', 'IRSEX', 'IRMARIT', 'IIMARIT', 'IREDUHIGHST2', 'IIEDUHIGHST2', 'CATAGE', 'CATAG2', 'CATAG3', 'CATAG6', 'CATAG7', 'PREGAGE2', 'DRVINAGE', 'DRVINDETAG', 'SEXAGE', 'NEWRACE2', 'SEXRACE', 'EDUHIGHCAT', 'HEALTH2', 'EDUSCHLGO', 'EDUSCHGRD2', 'EDUFULPAR', 'EDUSCKMON', 'EDUSCKEST', 'EDUSCKCOM', 'EDUSKPMON', 'EDUSKPEST', 'EDUSKPCOM', 'MILTFAMLY', 'MILTSPPAR', 'MILTPARNT', 'MILTCHLDR', 'MILTSIBLN', 'COLLENRLFT', 'COLLENRLST', 'WRKSTATWK2', 'WRKDPSTWK', 'WRKHADJOB', 'WRKDHRSWK2', 'WRK35WKUS', 'WRKRSNNOT', 'WRKRSNJOB', 'WRKEFFORT', 'WRKDPSTYR', 'WRKSELFEM', 'WRKNUMJOB2', 'WRKNJBPYR', 'WRKNJBWKS', 'WRKLASTYR2', 'WRKSICKMO', 'WRKSKIPMO', 'WRKDRGPOL', 'WRKDRGALB', 'WRKDRGEDU', 'WRKDRGHLP', 'WRKTSTALC', 'WRKTSTDRG', 'WRKTSTHIR', 'WRKTSTRDM', 'WRKTST1ST', 'WRKOKPREH', 'WRKOKRAND', 'IRWRKSTAT', 'IIWRKSTAT', 'II2WRKSTAT', 'IRWRKSTAT18', 'IIWRKSTAT18', 'II2WRKST18', 'EDFAM18', 'IMOTHER', 'IFATHER', 'NRCH17_2', 'IRHHSIZ2', 'IIHHSIZ2', 'IRKI17_2', 'IIKI17_2', 'IRHH65_2', 'IIHH65_2', 'PRXRETRY', 'PRXYDATA', 'MEDICARE', 'CAIDCHIP', 'CHAMPUS', 'PRVHLTIN', 'GRPHLTIN', 'HLTINALC', 'HLTINDRG', 'HLTINMNT', 'HLTINNOS', 'HLCNOTYR', 'HLCNOTMO', 'HLCLAST', 'HLLOSRSN', 'HLNVCOST', 'HLNVOFFR', 'HLNVREF', 'HLNVNEED', 'HLNVSOR', 'IRMEDICR', 'IIMEDICR', 'IRMCDCHP', 'IIMCDCHP', 'IRCHMPUS', 'IICHMPUS', 'IRPRVHLT', 'IIPRVHLT', 'IROTHHLT', 'IIOTHHLT', 'HLCALLFG', 'HLCALL99', 'ANYHLTI2', 'IRINSUR4', 'IIINSUR4', 'OTHINS', 'CELLWRKNG', 'CELLNOTCL', 'IRFAMSOC', 'IIFAMSOC', 'IRFAMSSI', 'IIFAMSSI', 'IRFSTAMP', 'IIFSTAMP', 'IRFAMPMT', 'IIFAMPMT', 'IRFAMSVC', 'IIFAMSVC', 'IRWELMOS', 'IIWELMOS', 'IRPINC3', 'IIPINC3', 'IRFAMIN3', 'IIFAMIN3', 'GOVTPROG', 'INCOME', 'POVERTY3', 'TOOLONG', 'TROUBUND', 'PDEN10', 'COUTYP4', 'MAIIN102', 'AIIND102', 'ANALWT_C', 'VESTR', 'VEREP']\n",
    "\n",
    "starting_length = len(col_names_1) + len(col_names_2) + len(col_names_3) + len(col_names_4)\n",
    "\n",
    "c1 = [c for c in col_names_1 if c[:2].lower()!='ii']\n",
    "c2 = [c for c in col_names_2 if c[:2].lower()!='ii']\n",
    "c3 = [c for c in col_names_3 if c[:2].lower()!='ii']\n",
    "c4 = [c for c in col_names_4 if c[:2].lower()!='ii']\n",
    "\n",
    "# _1 because I'm optimistic about finding more patterns like this\n",
    "reduced_length_1 = len(c1) + len(c2) + len(c3) + len(c4)\n",
    "\n",
    "# Now I'm going to print them out and go through them one section at a time. \n",
    "# no point in copying alllllll of that back in at once.\n",
    "\n",
    "print(c1)\n",
    "\n",
    "# me just figuring stuff out\n",
    "# it's not important\n",
    "# but I'm preserving it because I'm proud of it\n",
    "# I'm a genius, I'm a genius, I'm the smartest person in the world\n",
    "# >>> col_names_1[0]\n",
    "# 'IRCGRFM'\n",
    "# >>> col_names_1[0][:1]=='II'\n",
    "# False\n",
    "# >>> col_names_1[0][:1]=='IR'\n",
    "# False\n",
    "# >>> col_names_1[0][:1]\n",
    "# 'I'\n",
    "# >>> col_names_1[0][:2]\n",
    "# 'IR'\n",
    "# >>> col_names_1[0][:2]=='II'\n",
    "# False\n",
    "# >>> col_names_1[0][:2]=='IR'\n",
    "# True\n",
    "# >>> col_names_1[0][:2].lower()\n",
    "# 'ir'\n",
    "# >>> col_names_1[0][:2].lower()=='ir'\n",
    "# True\n",
    "# >>> col_names_1[0][:2].lower()=='ii'\n",
    "# False\n",
    "# >>> col_names_4[-4]\n",
    "# 'AIIND102'\n",
    "# >>> col_names_4[-4][:2].lower()=='ii'\n",
    "# False\n",
    "# >>> c1 = [c for c in col_names_1]\n",
    "# >>> c1 = c1[:5]\n",
    "# >>> c1\n",
    "# ['IRCGRFM', 'IICGRFM', 'II2CGRFM', 'IRSMKLSS30N', 'IISMKLSS30N']\n",
    "# >>> c1 = [c for c in c1 if c[:2].lower()!='ii']\n",
    "# >>> c1\n",
    "# ['IRCGRFM', 'IRSMKLSS30N']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thursday, May 23, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "today I went searching for alternative datasets because the one I'd been working on was taking so long and I thought I probably wouldn't end up being happy with it.  I'm sure I could have done something with it, but I was so excited to be back in queerland and I didn't want to give up on that.  so I went looking and was SHOCKED at how little was available (et tu, taskforce!?), but then I found a dataset from Ilan H. Meyer!!!!  It's a lot smaller than the samhsa one, but it's more specific to queer stuff, and it's Dr. Meyer!!!!  and there are citations from Fingerhut and Frost and Herek and Lisa Diamond and the whole gang!!  I've been so happy playing with it all day; I'm almost getting misty eyed!\n",
    "\n",
    "anyway, work notes.\n",
    "\n",
    "I created my first ever \"project\" in R, which seems to just be a folder that's tacked to one of the side panels.  You can click to open files from there; it's cool!\n",
    "\n",
    "it's a study of queer people across several age groups and across several years. so there's time series stuff in there too, but tbh that scares me a bit.  it'd be cool to do later and i definitely don't want to write it off as \"I can't do that,\" but knowing myself and my proclivity towards taking forever on things and that I do NOT have a resub accomodation on this project AND that it is the super important last project, I want to KISS.  so what I decided to do is look only at the first wave (before the attrition effects kicked in, which were sizable).  it's only n=1518, but it's Ilan Meyer!  I bet they're a good 1518.  Hank seemed somewhat worried about the sample size, but not to the point of it being a dealbreaker.  he said to plow on and we still have time to pivot if my stuff doesn't work.\n",
    "\n",
    "so, I dropped all the features that were exclusive to wave 2 and/or 3, leaving me with 505 features.  then I went thru the codebook and found all the scales they used.  I tracked down the columns where they had calculated the scores, and dropped all the component parts.  I then investigated their imputation process a little bit.  I have some concerns about it (seems weird that more than half of the people who didn't answer the question on IPV would be imputed as NO), but it is very sound in theory and, more importantly, I don't have enough data to drop NAs nor do I have any better ideas on how to impute them.  They *did* the imputation I would have done - regress the missing value on related, populated values, then randomly select a value from the nearest 5 extant values based on something that sounds a lot like KNN.  it seems very robust.  after looking into that, I dropped the non-imputed scale score features, and only kept the imputed ones.\n",
    "\n",
    "after that, I had a little under 400 columns left (quite an achievement, down from 1300+, in just a few hours of work!).  having finally figured out how the scales work, and that they did in fact calculate a lot of these things, I went back to the demographics section of the codebook and looked to see if they did similar stuff there.  it seems like they may have, but I'm not sure.  so far it seems more like they just duplicated their w1q00 variables into columns wiht more semantic names, but I'm just getting started.  that's where to pick up tomorrow.\n",
    "\n",
    "**I think the way forward here is to *NOT* edit scripts from previous days; rather, run them again and pick up where they left off in a new one.**\n",
    "\n",
    "files created: \n",
    "- 2024-05-23_exploring_Meyer_2023_dataset.py\n",
    "- 2024-05-23_set_up.py **NEW SET UP SCRIPT FOR NEW DF!!!!**\n",
    "- 2024-05-23_cols_to_check.txt\n",
    "- 2 folders called 2024-05-23_dowload... in the potential_datasets folder, plus all files within, plus corresponding zip folders.\n",
    "- - the one ending in attempt_2 is the one I ended up using.  they're the same, but the data is formatted nicer.\n",
    "\n",
    "GOALS FOR ~TOMORROW~  NEXT TIME\\*:\n",
    "- finish column cleaning\n",
    "- save a cleaned up (reduced) CSV\n",
    "- start setting up github\n",
    "- make an initial commit\n",
    "\n",
    "\\*may need to use Friday night to work on NN stuff so that you can talk to Hank in flex time if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script 1 in full:\n",
    "# 2024-05-23_set_up.py\n",
    "\n",
    "# Capstone Setup\n",
    "\n",
    "# Module Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, date, time\n",
    "from string import capwords\n",
    "\n",
    "# Working Directory\n",
    "os.getcwd()\n",
    "#os.chdir('C:/Users/emily/Git_Stuff/General_Assembly/04_Projects/project-capstone')\n",
    "os.listdir()\n",
    "\n",
    "# Import Data\n",
    "meyer = pd.read_csv('./potential_datasets/2024_05_23_download_ICPSR_Meyer_2023_generations_data_attempt_2/ICPSR_37166/DS0007/37166-0007-Data.tsv', sep = '\\t', low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script 2 in full\n",
    "# 2024-05-23_exploring_Meyer_2023_dataset.py\n",
    "\n",
    "# 5/23/24\n",
    "\n",
    "# check out this dataset from Ilan Meyer, my beloved\n",
    "\n",
    "# Import Data\n",
    "meyer = pd.read_csv('./potential_datasets/2024_05_23_download_ICPSR_Meyer_2023_generations_data_attempt_2/ICPSR_37166/DS0007/37166-0007-Data.tsv', sep = '\\t', low_memory=False)\n",
    "meyer.shape #(1518, 1329), but we can cut that down.\n",
    "og_columns = list(meyer.columns)\n",
    "\n",
    "# for RIGHT NOW, I think I'm only going to look at wave 1\n",
    "# the wave stuff is REALLY COOL, but i don't think I understand\n",
    "# time series well enough to get into it for this project\n",
    "# if I get a model working based on wave 1, and write it up\n",
    "# and all that \"good enough,\" then I'll come back and add \n",
    "# some time series stuff\n",
    "# oh it can even just be another phase of the project!\n",
    "# no need to replace anything!  just section B.\n",
    "\n",
    "meyer['WAVEPARTICIPATED'].value_counts(dropna =False)\n",
    "# matches what's in the documentation\n",
    "\n",
    "# ok, so, for now, I am going to drop all the variables marked W2 and W3.  \n",
    "cols = list(meyer.columns)\n",
    "w1_cols = [c for c in cols if c[:2]!='W2']\n",
    "w1_cols = [c for c in w1_cols if c[:2]!='W3']\n",
    "len(w1_cols)\n",
    "\n",
    "# drop columns\n",
    "meyer = meyer[w1_cols]\n",
    "meyer.shape\n",
    "\n",
    "# NAs?\n",
    "meyer.isna().sum().sum()\n",
    "# it says 0, but it's not 0\n",
    "# they must have imputed it with an empty string or something\n",
    "# when I do .value_counts(), a lot of columns have a blank row, e.g.,\n",
    "#                      14\n",
    "# where there absolutely should not be a blank.\n",
    "# so that's fun!  gonna have to clean that up!\n",
    "\n",
    "# i originally thought they only included the base variables\n",
    "# and I would have to compute the scale scores myself, but\n",
    "# OMG THEY DID INCLUDE THE CALC'D SCALES!!!!!!\n",
    "# I JUST COULDN'T FIND THEM BECAUSE ALL CAPS!!\n",
    "\n",
    "meyer.columns = [c.lower() for c in list(meyer.columns)]\n",
    "\n",
    "# GOAL: drop the columns that contributed to combination scores\n",
    "\n",
    "# Make lists of the items that comprise scales\n",
    "soc_supp_items = ['w1q164a', 'w1q164b', 'w1q164c', \n",
    "  'w1q164d', 'w1q164e', 'w1q164f', 'w1q164g', 'w1q164h', \n",
    "  'w1q164i', 'w1q164j', 'w1q164k', 'w1q164l']\n",
    "\n",
    "ace_items = ['w1q151', 'w1q152', 'w1q153', \n",
    "  'w1q154', 'w1q155', 'w1q156', 'w1q157', \n",
    "  'w1q158', 'w1q159', 'w1q160', 'w1q161']\n",
    "\n",
    "childhd_gnc_items = [\n",
    "  'w1q147', 'w1q148', 'w1q149', 'w1q150']\n",
    "\n",
    "############# NOT CONDENSED (YET) #############\n",
    "# strain_items = ['w1q146a', 'w1q146b', \n",
    "#   'w1q146c', 'w1q146d', 'w1q146e', 'w1q146f', \n",
    "#   'w1q146g', 'w1q146h', 'w1q146i', 'w1q146j', \n",
    "#   'w1q146k', 'w1q146l']\n",
    "############# NOT CONDENSED (YET) #############\n",
    "\n",
    "daily_discr_items = ['w1q144a', 'w1q144b', \n",
    "  'w1q144c','w1q144d', 'w1q144e', 'w1q144f', \n",
    "  'w1q144g', 'w1q144h', 'w1q144i']\n",
    "\n",
    "# bi_stigma_items = [  # only in wave 2\n",
    "#   'w2q117', 'w2q118', 'w2q119', 'w2q120']\n",
    "\n",
    "int_homo_items = ['w1q128', 'w1q129', \n",
    "  'w1q130', 'w1q131', 'w1q132']\n",
    "\n",
    "felt_stigma_items = ['w1q125', 'w1q126', 'w1q127']\n",
    "\n",
    "drug_items = ['w1q90', 'w1q91', 'w1q92', \n",
    "  'w1q93', 'w1q94', 'w1q95', 'w1q96', \n",
    "  'w1q97', 'w1q98', 'w1q99', 'w1q100']\n",
    "\n",
    "alc_items = ['w1q85', 'w1q86', 'w1q87']\n",
    "\n",
    "ment_dis_items = ['w1q77a', 'w1q77b', \n",
    "  'w1q77c', 'w1q77d', 'w1q77e', 'w1q77f']\n",
    "\n",
    "hc_ster_threat_items = [\n",
    "  'w1q60', 'w1q61', 'w1q62', 'w1q63']\n",
    "\n",
    "comm_conn_items = ['w1q53', 'w1q54', \n",
    "  'w1q55', 'w1q56', 'w1q57', 'w1q58', 'w1q59']\n",
    "\n",
    "lgbis_items = [\n",
    "  'w1q40', 'w1q41', 'w1q42', 'w1q43', 'w1q44']\n",
    "\n",
    "meim_items = [\n",
    "  'w1q21', 'w1q22', 'w1q23', 'w1q24', 'w1q25', 'w1q26']\n",
    "\n",
    "swl_items = [\n",
    "  'w1q186', 'w1q187', 'w1q188', 'w1q189', 'w1q190']\n",
    "\n",
    "swb_items = ['w1q04', 'w1q05', 'w1q06', 'w1q07', \n",
    "  'w1q08', 'w1q09', 'w1q10', 'w1q11', 'w1q12', \n",
    "  'w1q13', 'w1q14', 'w1q15', 'w1q16', 'w1q17', 'w1q18']\n",
    "\n",
    "drop_cols = (soc_supp_items + ace_items + childhd_gnc_items + \n",
    "  daily_discr_items + int_homo_items + felt_stigma_items + \n",
    "  drug_items + alc_items + ment_dis_items + hc_ster_threat_items + \n",
    "  comm_conn_items + lgbis_items + meim_items + swl_items + swb_items)\n",
    "len(drop_cols)\n",
    "\n",
    "meyer.drop(columns = drop_cols, inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "# these are the scales\n",
    "combo_features = ['w1socialwb', 'w1socialwb_i', \n",
    "  'w1lifesat', 'w1lifesat_i', 'w1meim', 'w1meim_i', \n",
    "  'w1idcentral', 'w1idcentral_i', 'w1connectedness',\n",
    "  'w1connectedness_i', 'w1hcthreat', 'w1hcthreat_i', \n",
    "  'w1kessler6', 'w1kessler6_i', 'w1auditc', \n",
    "  'w1auditc_i', 'w1dudit', 'w1dudit_i', 'w1feltstigma', \n",
    "  'w1feltstigma_i', 'w1internalized', 'w1internalized_i', \n",
    "  'w1everyday', 'w1everyday_i', 'w1childgnc', 'w1childgnc_i',\n",
    "  'w1ace', 'w1ace_i', 'w1ace_emo', 'w1ace_emo_i', \n",
    "  'w1ace_inc', 'w1ace_inc_i', 'w1ace_ipv', 'w1ace_ipv_i', \n",
    "  'w1ace_men', 'w1ace_men_i', 'w1ace_phy', 'w1ace_phy_i', \n",
    "  'w1ace_sep', 'w1ace_sep_i', 'w1ace_sex', 'w1ace_sex_i', \n",
    "  'w1ace_sub', 'w1ace_sub_i', 'w1socsupport', \n",
    "  'w1socsupport_fam', 'w1socsupport_fam_i', \n",
    "  'w1socsupport_fr', 'w1socsupport_fr_i', \n",
    "  'w1socsupport_i', 'w1socsupport_so', 'w1socsupport_so_i']\n",
    "\n",
    "# split them up so I can check whether \n",
    "# the _i columns really are free of missing values\n",
    "combo_imputed = [c for c in combo_features if c[-2:]=='_i']\n",
    "combo_not_imputed = [c for c in combo_features if c[-2:]!='_i']\n",
    "\n",
    "# check my splits\n",
    "a = [c for c in combo_imputed if c in combo_not_imputed]\n",
    "b = [c for c in combo_not_imputed if c in combo_imputed]\n",
    "if len(a)==len(b)==0:\n",
    "  print('no overlap')\n",
    "  del (a, b)\n",
    "if (len(combo_imputed)+len(combo_not_imputed))==len(combo_features):\n",
    "  print('all accounted for')\n",
    "  \n",
    "# check for \"missing\" values\n",
    "for i in combo_imputed:\n",
    "  print('')\n",
    "  print(f'{i}, {meyer[i].isna().sum()} NAs')\n",
    "  print(meyer[i].value_counts(dropna = False).sort_values())\n",
    "\n",
    "# see how bad it was in the original\n",
    "for i in combo_not_imputed:\n",
    "  print('')\n",
    "  print(f'{i}, {meyer[i].isna().sum()} NAs')\n",
    "  print(meyer[i].value_counts(dropna = False).sort_values())\n",
    "missing_values = [('w1ace_sex', 75), ('w1ace_phy', 61), \n",
    "  ('w1ace_ipv', 139), ('w1ace_emo', 91), ('w1ace', 277), \n",
    "  ('w1everyday', 40), ('w1dudit', 66), ('w1connectedness', 51), \n",
    "  ('w1socialwb', 59), ('most others', '15-35')]\n",
    "\n",
    "# very grim and not at all surprising that column with the most missings is IPV\n",
    "# (from the ACE survey - so IPV among parents, not participants themselves)\n",
    "# >>> meyer['w1ace_ipv'].value_counts(dropna = False)\n",
    "# w1ace_ipv\n",
    "# 0    950\n",
    "# 1    429\n",
    "#      139\n",
    "# Name: count, dtype: int64\n",
    "# >>> meyer['w1ace_ipv_i'].value_counts(dropna = False)\n",
    "# w1ace_ipv_i\n",
    "# 0    1024\n",
    "# 1     494\n",
    "# Name: count, dtype: int64\n",
    "# >>> 1024-950\n",
    "# 74\n",
    "# >>> 139-74\n",
    "# 65\n",
    "\n",
    "# ok.  their imputation method seems really solid, but that is sus af.\n",
    "# of the people who declined to answer about IPV, you think the majority \n",
    "# of them would have said NO!?!??!??????\n",
    "# I'd be tempted to put every one of those guys down as a yes!!!!\n",
    "# let me see the overlap with other stigmatized stuff\n",
    "len(meyer[(meyer['w1ace_ipv']==' ') & (meyer['w1ace_sex']==' ')])\n",
    "# 30 huh damn I would have thought like all of them\n",
    "# wait.\n",
    "len(meyer[(meyer['w1ace_ipv']=='1') & (meyer['w1ace_sex']==' ')])  # 19\n",
    "len(meyer[(meyer['w1ace_ipv']==' ') & (meyer['w1ace_sex']=='1')])  # 51\n",
    "# ok! that actually lends credibility to their imputation though\n",
    "# if people were too ashamed to admit IPV, they'd probably decline to answer\n",
    "# about CSA too.  these people, at least, really could go either way on IPV.\n",
    "\n",
    "# This is all actually moot \n",
    "# because I don't have a better idea for imputation than what they did.\n",
    "# I think the thing to do, if I can, is to model it on the imputed data\n",
    "# then model it again on the subsample where I've dropped all these NAs\n",
    "# at least the ACE NAs, where there's really a lot.\n",
    "# could also try modeling it just without those columns\n",
    "# a random forest would probably be good, \n",
    "# make sure the whole thing isn't hinging on imputed data\n",
    "# Meyer also probably ran some tests in his paper to make sure this is ok\n",
    "\n",
    "# OK SO.\n",
    "\n",
    "# I've gotta reduce my dimensions\n",
    "# the imputation methods they used seem very reasonable\n",
    "# (see p. 37 of 37166-Documentation-methodology.pdf)\n",
    "# so I'm going to drop the non-imputed ones\n",
    "meyer.drop(columns = combo_not_imputed, inplace = True)\n",
    "meyer.shape  #(1518, 373)\n",
    "\n",
    "# ok!  love that!  let's see what else I can chuck.\n",
    "cols_to_check = [c for c in list(meyer.columns) if c not in combo_imputed]\n",
    "print(cols_to_check)\n",
    "\n",
    "# I am looking thru the documentation and making 2 lists\n",
    "# the variables should ROUGHLY correspond to each other\n",
    "# maybe I'll do some line breaks or whatever.\n",
    "drop_cols_2 = [\n",
    "  'w1q165', 'w1q27', 'w1q28', \n",
    "  'w1q20_1', 'w1q20_2', 'w1q20_3', 'w1q20_4', 'w1q20_5', 'w1q20_6', 'w1q20_7', \n",
    "]\n",
    "\n",
    "keep_cols = [\n",
    "  'w1age', 'cohort', 'w1sex', 'w1gender', 'w1sex_gender', \n",
    "  'screen_race', 'w1race',\n",
    "]\n",
    "\n",
    "check_cols = [\n",
    "  # no age, sex stuff\n",
    "  'w1q20_t_verb', # I suspect this is part of q20 and got rolled into w1race, but I'm not sure off the top of my head\n",
    "]\n",
    "\n",
    "# KEEP GOING ON THESE ON FRIDAY\n",
    "# I JUST FINISHED SEX AND GENDER\n",
    "# PICK UP WITH SEXUAL IDENTITY, PAGE 14 (16) OF THE DOCUMENTATION\n",
    "# 37166-Documentation-methodology.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# not-script 3 in full \n",
    "\\# 2024-05-23_cols_to_check.txt\n",
    "\n",
    "cols_to_check\n",
    "\n",
    "These are the columns that remained after I dropped all the scale items and non-imputed scale features.  What I'm working on now is going through them to try and reduce dimensionality.\n",
    "\n",
    "['studyid', 'waveparticipated', 'w1weight_full', 'w1weight_orig', 'w1cumulative_wt_nr1', 'w1cumulative_wt_nr2', 'w1cumulative_wt_nr3', 'w1cumulative_wt_sampling', 'w1weighting_cell_nr1', 'w1weighting_cell_nr2and3', 'w1frame_wt', 'w1survey_yr', 'geduc1', 'geduc2', 'geducation', 'gemployment2010', 'gmethod_type', 'gmsaname', 'gp1', 'gruca', 'gruca_i', 'gurban', 'gurban_i', 'gzipcode', 'gzipstate', 'gcendiv', 'gcenreg', 'gmilesaway', 'gmilesaway2', 'w1q01', 'w1q02', 'w1q03', 'w1q19a', 'w1q19b', 'w1q19c', 'w1q19d', 'w1q29', 'w1q29_t_verb', 'w1q30_1', 'w1q30_2', 'w1q30_3', 'w1q30_4', 'w1q30_5', 'w1q31a', 'w1q31b', 'w1q31c', 'w1q31d', 'w1q32', 'w1q33', 'w1q34', 'w1q35', 'w1q36', 'w1q37', 'w1q38', 'w1q39_1', 'w1q39_2', 'w1q39_3', 'w1q39_4', 'w1q39_5', 'w1q39_6', 'w1q39_7', 'w1q39_8', 'w1q39_9', 'w1q39_10', 'w1q39_11', 'w1q39_12', 'w1q39_t_verb', 'w1q45', 'w1q46', 'w1q47', 'w1q48', 'w1q49', 'w1q50', 'w1q51', 'w1q52', 'w1q64_1', 'w1q64_2', 'w1q64_3', 'w1q64_4', 'w1q64_5', 'w1q64_6', 'w1q64_7', 'w1q64_8', 'w1q64_9', 'w1q64_10', 'w1q64_11', 'w1q64_12', 'w1q64_13', 'w1q64_t_verb', 'w1q65', 'w1q66_1', 'w1q66_2', 'w1q66_3', 'w1q66_4', 'w1q66_5', 'w1q66_t_verb', 'w1q67', 'w1q68_1', 'w1q68_2', 'w1q68_3', 'w1q69', 'w1q70', 'w1q71', 'w1q72', 'w1q73', 'w1q74_1', 'w1q74_2', 'w1q74_3', 'w1q74_4', 'w1q74_5', 'w1q74_6', 'w1q74_7', 'w1q74_8', 'w1q74_9', 'w1q74_10', 'w1q74_11', 'w1q74_12', 'w1q74_13', 'w1q74_14', 'w1q74_15', 'w1q74_16', 'w1q74_17', 'w1q74_18', 'w1q74_19', 'w1q74_20', 'w1q74_21', 'w1q74_22', 'w1q74_23', 'w1q75', 'w1q76', 'w1q78', 'w1q79', 'w1q80', 'w1q81', 'w1q82', 'w1q83', 'w1q84', 'w1q88', 'w1q89', 'w1q101', 'w1q102', 'w1q103', 'w1q104', 'w1q105', 'w1q106', 'w1q107', 'w1q108', 'w1q109', 'w1q110', 'w1q111', 'w1q112', 'w1q113', 'w1q114', 'w1q115', 'w1q116', 'w1q117', 'w1q118', 'w1q119', 'w1q120', 'w1q121', 'w1q122', 'w1q123a', 'w1q123b', 'w1q123c', 'w1q123d', 'w1q124', 'w1q133', 'w1q133_1', 'w1q133_2', 'w1q133_3', 'w1q134', 'w1q135a', 'w1q135b', 'w1q135c', 'w1q135d', 'w1q135e', 'w1q135f', 'w1q136_1', 'w1q136_2', 'w1q136_3', 'w1q136_4', 'w1q136_5', 'w1q136_6', 'w1q136_7', 'w1q136_8', 'w1q136_9', 'w1q136_10', 'w1q137', 'w1q138', 'w1q139_1', 'w1q139_2', 'w1q139_3', 'w1q139_4', 'w1q139_5', 'w1q139_6', 'w1q139_7', 'w1q139_8', 'w1q139_9', 'w1q139_10', 'w1q140', 'w1q141_1', 'w1q141_2', 'w1q141_3', 'w1q141_4', 'w1q141_5', 'w1q141_6', 'w1q141_7', 'w1q141_8', 'w1q141_9', 'w1q141_10', 'w1q142a', 'w1q142b', 'w1q142c', 'w1q142d', 'w1q142e', 'w1q142f', 'w1q142g', 'w1q142h', 'w1q142i', 'w1q142j', 'w1q142k', 'w1q143_1', 'w1q143_2', 'w1q143_3', 'w1q143_4', 'w1q143_5', 'w1q143_6', 'w1q143_7', 'w1q143_8', 'w1q143_9', 'w1q143_10', 'w1q145_1', 'w1q145_2', 'w1q145_3', 'w1q145_4', 'w1q145_5', 'w1q145_6', 'w1q145_7', 'w1q145_8', 'w1q145_9', 'w1q145_10', 'w1q146a', 'w1q146b', 'w1q146c', 'w1q146d', 'w1q146e', 'w1q146f', 'w1q146g', 'w1q146h', 'w1q146i', 'w1q146j', 'w1q146k', 'w1q146l', 'w1q162', 'w1q163_1', 'w1q163_2', 'w1q163_3', 'w1q163_4', 'w1q163_5', 'w1q163_6', 'w1q163_7', 'w1q163_8', 'w1q163_9', 'w1q163_10', 'w1q166', 'w1q167', 'w1q168', 'w1q169', 'w1q170_1', 'w1q170_2', 'w1q170_3', 'w1q170_4', 'w1q171_1', 'w1q171_2', 'w1q171_3', 'w1q171_4', 'w1q171_5', 'w1q171_6', 'w1q171_7', 'w1q171_8', 'w1q171_9', 'w1q172', 'w1q173', 'w1q174', 'w1q175', 'w1q176', 'w1q177_1', 'w1q177_2', 'w1q177_3', 'w1q177_4', 'w1q177_5', 'w1q177_6', 'w1q177_7', 'w1q177_8', 'w1q177_9', 'w1q177_10', 'w1q177_11', 'w1q177_12', 'w1q178', 'w1q179', 'w1q180', 'w1q181', 'w1q182', 'w1q183', 'w1q184', 'w1q185',  'w1sample',  'w1sexualid', 'w1sexminid', 'w1pinc', 'w1pinc_i', 'w1hinc', 'w1hinc_i', 'w1poverty', 'w1poverty_i', 'w1povertycat', 'w1povertycat_i', 'w1conversion', 'w1conversionhc', 'w1conversionrel', 'gmethod_type_w2', 'grespondent_date_w2', 'gsurvey', 'gmethod_type_w3', 'gp2', 'grace', 'grespondent_date_w3', 'wave3', 'nopolicecontact']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wednesday, May 29, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "today I continued cleaning the Meyer dataset.  I finished all the scales that were defined in the documentation, and dropped columns accordingly.  at that point i was down to 337 columns.  I then started going through the 831 page documenation to piece together the rest of what's what.  i started writing a dictionary of how I want to combine these columns, after which I plan to drop most of them.  there are a few I want to keep to see if they perform better alone than they do as a composite, but I want to be careful of introducing too much colinearity.  because I'd really like to be able to do interpretation, I might have to sacrifice some features I think are interesting in order to be able to make sense of the others.\n",
    "\n",
    "I also found a TON of really interesting features that I had to drop just to make sure I have a straightforward enough project to pull off in 2 weeks (left).  I'd love to go back and tinker with them for my portfolio though!\n",
    "\n",
    "files created: \n",
    "- 2024-05-23_AND_2024-05-29_exploring_Meyer_2023_dataset.py\n",
    "- 2024-05-29_cols_to_check.txt\n",
    "- 2024-05-29_notes_about_non-scale_columns.txt\n",
    "both of the first two files are actually just longer versions of the older ones, so most of their contents are redundant.  I used the same setup file as on 5-23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script 1 in full: 2024-05-23_AND_2024-05-29_exploring_Meyer_2023_dataset.py\n",
    "\n",
    "# 5/23/24\n",
    "\n",
    "# check out this dataset from Ilan Meyer, my beloved\n",
    "\n",
    "# Import Data\n",
    "meyer = pd.read_csv('./potential_datasets/2024_05_23_download_ICPSR_Meyer_2023_generations_data_attempt_2/ICPSR_37166/DS0007/37166-0007-Data.tsv', sep = '\\t', low_memory=False)\n",
    "meyer.shape #(1518, 1329), but we can cut that down.\n",
    "og_columns = list(meyer.columns)\n",
    "\n",
    "# for RIGHT NOW, I think I'm only going to look at wave 1\n",
    "# the wave stuff is REALLY COOL, but i don't think I understand\n",
    "# time series well enough to get into it for this project\n",
    "# if I get a model working based on wave 1, and write it up\n",
    "# and all that \"good enough,\" then I'll come back and add \n",
    "# some time series stuff\n",
    "# oh it can even just be another phase of the project!\n",
    "# no need to replace anything!  just section B.\n",
    "\n",
    "meyer['WAVEPARTICIPATED'].value_counts(dropna =False)\n",
    "# matches what's in the documentation\n",
    "\n",
    "# ok, so, for now, I am going to drop all the variables marked W2 and W3.  \n",
    "cols = list(meyer.columns)\n",
    "w1_cols = [c for c in cols if c[:2]!='W2']\n",
    "w1_cols = [c for c in w1_cols if c[:2]!='W3']\n",
    "len(w1_cols)\n",
    "\n",
    "# drop columns\n",
    "meyer = meyer[w1_cols]\n",
    "meyer.shape\n",
    "\n",
    "# NAs?\n",
    "meyer.isna().sum().sum()\n",
    "# it says 0, but it's not 0\n",
    "# they must have imputed it with an empty string or something\n",
    "# when I do .value_counts(), a lot of columns have a blank row, e.g.,\n",
    "#                      14\n",
    "# where there absolutely should not be a blank.\n",
    "# so that's fun!  gonna have to clean that up!\n",
    "\n",
    "# i originally thought they only included the base variables\n",
    "# and I would have to compute the scale scores myself, but\n",
    "# OMG THEY DID INCLUDE THE CALC'D SCALES!!!!!!\n",
    "# I JUST COULDN'T FIND THEM BECAUSE ALL CAPS!!\n",
    "\n",
    "meyer.columns = [c.lower() for c in list(meyer.columns)]\n",
    "\n",
    "# GOAL: drop the columns that contributed to combination scores\n",
    "\n",
    "# Make lists of the items that comprise scales\n",
    "soc_supp_items = ['w1q164a', 'w1q164b', 'w1q164c', \n",
    "  'w1q164d', 'w1q164e', 'w1q164f', 'w1q164g', 'w1q164h', \n",
    "  'w1q164i', 'w1q164j', 'w1q164k', 'w1q164l']\n",
    "\n",
    "ace_items = ['w1q151', 'w1q152', 'w1q153', \n",
    "  'w1q154', 'w1q155', 'w1q156', 'w1q157', \n",
    "  'w1q158', 'w1q159', 'w1q160', 'w1q161']\n",
    "\n",
    "childhd_gnc_items = [\n",
    "  'w1q147', 'w1q148', 'w1q149', 'w1q150']\n",
    "\n",
    "############# NOT CONDENSED (YET) #############\n",
    "# strain_items = ['w1q146a', 'w1q146b', \n",
    "#   'w1q146c', 'w1q146d', 'w1q146e', 'w1q146f', \n",
    "#   'w1q146g', 'w1q146h', 'w1q146i', 'w1q146j', \n",
    "#   'w1q146k', 'w1q146l']\n",
    "############# NOT CONDENSED (YET) #############\n",
    "\n",
    "daily_discr_items = ['w1q144a', 'w1q144b', \n",
    "  'w1q144c','w1q144d', 'w1q144e', 'w1q144f', \n",
    "  'w1q144g', 'w1q144h', 'w1q144i']\n",
    "\n",
    "# bi_stigma_items = [  # only in wave 2\n",
    "#   'w2q117', 'w2q118', 'w2q119', 'w2q120']\n",
    "\n",
    "int_homo_items = ['w1q128', 'w1q129', \n",
    "  'w1q130', 'w1q131', 'w1q132']\n",
    "\n",
    "felt_stigma_items = ['w1q125', 'w1q126', 'w1q127']\n",
    "\n",
    "drug_items = ['w1q90', 'w1q91', 'w1q92', \n",
    "  'w1q93', 'w1q94', 'w1q95', 'w1q96', \n",
    "  'w1q97', 'w1q98', 'w1q99', 'w1q100']\n",
    "\n",
    "alc_items = ['w1q85', 'w1q86', 'w1q87']\n",
    "\n",
    "ment_dis_items = ['w1q77a', 'w1q77b', \n",
    "  'w1q77c', 'w1q77d', 'w1q77e', 'w1q77f']\n",
    "\n",
    "hc_ster_threat_items = [\n",
    "  'w1q60', 'w1q61', 'w1q62', 'w1q63']\n",
    "\n",
    "comm_conn_items = ['w1q53', 'w1q54', \n",
    "  'w1q55', 'w1q56', 'w1q57', 'w1q58', 'w1q59']\n",
    "\n",
    "lgbis_items = [\n",
    "  'w1q40', 'w1q41', 'w1q42', 'w1q43', 'w1q44']\n",
    "\n",
    "meim_items = [\n",
    "  'w1q21', 'w1q22', 'w1q23', 'w1q24', 'w1q25', 'w1q26']\n",
    "\n",
    "swl_items = [\n",
    "  'w1q186', 'w1q187', 'w1q188', 'w1q189', 'w1q190']\n",
    "\n",
    "swb_items = ['w1q04', 'w1q05', 'w1q06', 'w1q07', \n",
    "  'w1q08', 'w1q09', 'w1q10', 'w1q11', 'w1q12', \n",
    "  'w1q13', 'w1q14', 'w1q15', 'w1q16', 'w1q17', 'w1q18']\n",
    "\n",
    "drop_cols = (soc_supp_items + ace_items + childhd_gnc_items + \n",
    "  daily_discr_items + int_homo_items + felt_stigma_items + \n",
    "  drug_items + alc_items + ment_dis_items + hc_ster_threat_items + \n",
    "  comm_conn_items + lgbis_items + meim_items + swl_items + swb_items)\n",
    "len(drop_cols)\n",
    "\n",
    "meyer.drop(columns = drop_cols, inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "# these are the scales\n",
    "combo_features = ['w1socialwb', 'w1socialwb_i', \n",
    "  'w1lifesat', 'w1lifesat_i', 'w1meim', 'w1meim_i', \n",
    "  'w1idcentral', 'w1idcentral_i', 'w1connectedness',\n",
    "  'w1connectedness_i', 'w1hcthreat', 'w1hcthreat_i', \n",
    "  'w1kessler6', 'w1kessler6_i', 'w1auditc', \n",
    "  'w1auditc_i', 'w1dudit', 'w1dudit_i', 'w1feltstigma', \n",
    "  'w1feltstigma_i', 'w1internalized', 'w1internalized_i', \n",
    "  'w1everyday', 'w1everyday_i', 'w1childgnc', 'w1childgnc_i',\n",
    "  'w1ace', 'w1ace_i', 'w1ace_emo', 'w1ace_emo_i', \n",
    "  'w1ace_inc', 'w1ace_inc_i', 'w1ace_ipv', 'w1ace_ipv_i', \n",
    "  'w1ace_men', 'w1ace_men_i', 'w1ace_phy', 'w1ace_phy_i', \n",
    "  'w1ace_sep', 'w1ace_sep_i', 'w1ace_sex', 'w1ace_sex_i', \n",
    "  'w1ace_sub', 'w1ace_sub_i', 'w1socsupport', \n",
    "  'w1socsupport_fam', 'w1socsupport_fam_i', \n",
    "  'w1socsupport_fr', 'w1socsupport_fr_i', \n",
    "  'w1socsupport_i', 'w1socsupport_so', 'w1socsupport_so_i']\n",
    "\n",
    "# split them up so I can check whether \n",
    "# the _i columns really are free of missing values\n",
    "combo_imputed = [c for c in combo_features if c[-2:]=='_i']\n",
    "combo_not_imputed = [c for c in combo_features if c[-2:]!='_i']\n",
    "\n",
    "# check my splits\n",
    "a = [c for c in combo_imputed if c in combo_not_imputed]\n",
    "b = [c for c in combo_not_imputed if c in combo_imputed]\n",
    "if len(a)==len(b)==0:\n",
    "  print('no overlap')\n",
    "  del (a, b)\n",
    "if (len(combo_imputed)+len(combo_not_imputed))==len(combo_features):\n",
    "  print('all accounted for')\n",
    "  \n",
    "# check for \"missing\" values\n",
    "for i in combo_imputed:\n",
    "  print('')\n",
    "  print(f'{i}, {meyer[i].isna().sum()} NAs')\n",
    "  print(meyer[i].value_counts(dropna = False).sort_values())\n",
    "\n",
    "# see how bad it was in the original\n",
    "for i in combo_not_imputed:\n",
    "  print('')\n",
    "  print(f'{i}, {meyer[i].isna().sum()} NAs')\n",
    "  print(meyer[i].value_counts(dropna = False).sort_values())\n",
    "missing_values = [('w1ace_sex', 75), ('w1ace_phy', 61), \n",
    "  ('w1ace_ipv', 139), ('w1ace_emo', 91), ('w1ace', 277), \n",
    "  ('w1everyday', 40), ('w1dudit', 66), ('w1connectedness', 51), \n",
    "  ('w1socialwb', 59), ('most others', '15-35')]\n",
    "\n",
    "# very grim and not at all surprising that column with the most missings is IPV\n",
    "# (from the ACE survey - so IPV among parents, not participants themselves)\n",
    "# >>> meyer['w1ace_ipv'].value_counts(dropna = False)\n",
    "# w1ace_ipv\n",
    "# 0    950\n",
    "# 1    429\n",
    "#      139\n",
    "# Name: count, dtype: int64\n",
    "# >>> meyer['w1ace_ipv_i'].value_counts(dropna = False)\n",
    "# w1ace_ipv_i\n",
    "# 0    1024\n",
    "# 1     494\n",
    "# Name: count, dtype: int64\n",
    "# >>> 1024-950\n",
    "# 74\n",
    "# >>> 139-74\n",
    "# 65\n",
    "\n",
    "# ok.  their imputation method seems really solid, but that is sus af.\n",
    "# of the people who declined to answer about IPV, you think the majority \n",
    "# of them would have said NO!?!??!??????\n",
    "# I'd be tempted to put every one of those guys down as a yes!!!!\n",
    "# let me see the overlap with other stigmatized stuff\n",
    "len(meyer[(meyer['w1ace_ipv']==' ') & (meyer['w1ace_sex']==' ')])\n",
    "# 30 huh damn I would have thought like all of them\n",
    "# wait.\n",
    "len(meyer[(meyer['w1ace_ipv']=='1') & (meyer['w1ace_sex']==' ')])  # 19\n",
    "len(meyer[(meyer['w1ace_ipv']==' ') & (meyer['w1ace_sex']=='1')])  # 51\n",
    "# ok! that actually lends credibility to their imputation though\n",
    "# if people were too ashamed to admit IPV, they'd probably decline to answer\n",
    "# about CSA too.  these people, at least, really could go either way on IPV.\n",
    "\n",
    "# This is all actually moot \n",
    "# because I don't have a better idea for imputation than what they did.\n",
    "# I think the thing to do, if I can, is to model it on the imputed data\n",
    "# then model it again on the subsample where I've dropped all these NAs\n",
    "# at least the ACE NAs, where there's really a lot.\n",
    "# could also try modeling it just without those columns\n",
    "# a random forest would probably be good, \n",
    "# make sure the whole thing isn't hinging on imputed data\n",
    "# Meyer also probably ran some tests in his paper to make sure this is ok\n",
    "\n",
    "# OK SO.\n",
    "\n",
    "# I've gotta reduce my dimensions\n",
    "# the imputation methods they used seem very reasonable\n",
    "# (see p. 37 of 37166-Documentation-methodology.pdf)\n",
    "# so I'm going to drop the non-imputed ones\n",
    "meyer.drop(columns = combo_not_imputed, inplace = True)\n",
    "meyer.shape  #(1518, 373)\n",
    "\n",
    "# ok!  love that!  let's see what else I can chuck.\n",
    "cols_to_check = [c for c in list(meyer.columns) if c not in combo_imputed]\n",
    "print(cols_to_check)\n",
    "\n",
    "# I am looking thru the documentation and making 2 lists\n",
    "# the variables should ROUGHLY correspond to each other\n",
    "# maybe I'll do some line breaks or whatever.\n",
    "drop_cols_2 = [\n",
    "  'w1q165', 'w1q27', 'w1q28', \n",
    "  'w1q20_1', 'w1q20_2', 'w1q20_3', 'w1q20_4', 'w1q20_5', 'w1q20_6', 'w1q20_7', \n",
    "  'w1q29', 'w1q29_t_verb', \n",
    "  # kept all ed columns\n",
    "  'gruca', 'gruca_i', 'gurban', 'gzipstate',  'gzipcode', \n",
    "  'w1hinc', 'w1poverty', 'w1povertycat', \n",
    "  'w1q133', 'w1q133_1', 'w1q133_2', 'w1q133_3', \n",
    "  # from here I'm just going thru the 831 page documentation and picking stuff out\n",
    "  'w1weight_orig', \n",
    "  'gmsaname', 'gmethod_type', 'gmethod_type_w2', 'gmethod_type_w3', \n",
    "  'w1cumulative_wt_nr1', 'w1cumulative_wt_nr2', 'w1cumulative_wt_nr3', \n",
    "  'w1cumulative_wt_sampling', 'w1weighting_cell_nr1', 'w1weighting_cell_nr2and3', \n",
    "  'w1frame_wt', \n",
    "]\n",
    "\n",
    "keep_cols = ['studyid', 'waveparticipated', 'w1survey_yr', \n",
    "  'w1age', 'cohort', 'w1sex', 'w1gender', 'w1sex_gender', \n",
    "  'screen_race', 'w1race',\n",
    "  'w1sexualid', 'w1sexminid', \n",
    "  'geduc1', 'geduc2', 'geducation', \n",
    "  'gurban_i', 'gcendiv', 'gcenreg', 'gmilesaway', 'gmilesaway2', \n",
    "  'w1hinc_i', 'w1poverty_i', 'w1povertycat_i', \n",
    "  'w1conversion', 'w1conversionhc', 'w1conversionrel', \n",
    "  'w1weight_full', 'gemployment2010', \n",
    "]\n",
    "\n",
    "check_cols = [\n",
    "  # no age, sex stuff\n",
    "  'w1q20_t_verb', # I suspect this is part of q20 and got rolled into w1race, but I'm not sure off the top of my head\n",
    "]\n",
    "\n",
    "# can I weed out the redacted ones quickly?\n",
    "\n",
    "x = list(meyer.columns)\n",
    "x_redacted = [c for c in x if meyer[c].nunique()==1]\n",
    "# it works!  but I already caught these guys\n",
    "\n",
    "\n",
    "# I finished all the variables that were easy to parse out and assigned\n",
    "# them to a list above.  I am now going to drop the ones that I indicated,\n",
    "# and then I will go through the 831 page documentation to make sense \n",
    "# of what's left and try to drop more.\n",
    "\n",
    "meyer.drop(columns = drop_cols_2, inplace = True) # (1518, 337)\n",
    "\n",
    "suicidal_idea_beh = [\n",
    "  'w1q101', 'w1q102', 'w1q103', 'w1q104', 'w1q105', 'w1q106', 'w1q107', \n",
    "  'w1q108', 'w1q109', 'w1q110', 'w1q111', 'w1q112', 'w1q113', 'w1q114', \n",
    "  'w1q115', 'w1q116', 'w1q117', 'w1q118', 'w1q119', 'w1q120', 'w1q121', \n",
    "]\n",
    "\n",
    "drop_cols_2_again = [\n",
    "  'w1q31a', 'w1q31b', 'w1q31c', 'w1q31d', # redundant with sexual identity\n",
    "  'w1q66_1', 'w1q66_2', 'w1q66_3', 'w1q66_4', 'w1q66_5', 'w1q66_t_verb', # interesting to compare \n",
    "  #ER to Dr ofc, but not for this study.  too many columns already ^\n",
    "  'w1q67', 'w1q68_1', 'w1q68_2', 'w1q68_3', 'w1q70', 'w1q71', 'w1q73', # <- same ^ and V\n",
    "  'w1q74_1', 'w1q74_2', 'w1q74_3', 'w1q74_4', 'w1q74_7', 'w1q74_8', 'w1q74_9', \n",
    "  'w1q74_12', 'w1q74_13', 'w1q74_15', 'w1q74_16', 'w1q74_19', 'w1q80', 'w1q81',\n",
    "  'w1q82', 'w1q83', 'w1q84', 'w1q88', 'w1q118', \n",
    "]\n",
    "\n",
    "keep_cols_good_on_own = [\n",
    "  'w1q01', 'w1q02', 'w1q03', 'w1q32', 'w1q33', 'w1q34', 'w1q35', 'w1q36', \n",
    "  'w1q37', 'w1q38', 'w1q52', 'w1q65', 'w1q69', 'w1q72', 'w1q74_21', \n",
    "  'w1q74_22', 'w1q74_23', 'w1q78', 'w1q79', 'w1q89', 'w1q119', 'w1q134', \n",
    "  'w1q136_7', 'w1q139_7', 'w1q140', 'w1q141_7', \n",
    "]\n",
    "\n",
    "keep_cols_ohe_done = [\n",
    "  'w1q30_1', 'w1q30_2', 'w1q30_3', 'w1q30_4', 'w1q30_5', \n",
    "  'w1q39_1', 'w1q39_2', 'w1q39_3', 'w1q39_4', 'w1q39_5', 'w1q39_6', 'w1q39_7', \n",
    "  'w1q39_8', 'w1q39_9', 'w1q39_10', 'w1q39_11', 'w1q39_12', 'w1q39_t_verb',\n",
    "]\n",
    "\n",
    "# manually combine columns in ways that *I* think make sense\n",
    "# here I am creating a dictionary where the keys are the new column names I want\n",
    "# and the values are lists, wherein the first value on the list is the method \n",
    "# of combination, and the remainder are the columns to be combined\n",
    "# if the method is 'recode', then I probably need to give it more direct attn\n",
    "# but all the others I am hoping to be able to automate\n",
    "# the end goal is to use this dictionary to either generate or directly run\n",
    "# the code to create the new columns and drop the ones no longer needed\n",
    "# before dropping the columns, I should check them against keep_cols\n",
    "# to make sure I'm not hoping to keep them individually in addition to combo'ing\n",
    "\n",
    "feat_eng_dict = {\n",
    "  'pers_well_being': ['sum', 'w1q01', 'w1q02', ],\n",
    "  'neighb_welcoming': ['mean', 'w1q19a', 'w1q19b', 'w1q19c', 'w1q19d', ],\n",
    "  'age_awakening': ['min','w1q45', 'w1q46', 'w1q47', 'w1q48', ],\n",
    "  'age_out': ['min', 'w1q49', 'w1q50', 'w1q51', ]\n",
    "  'health_insurance': ['binarize', 'w1q64_1', 'w1q64_2', 'w1q64_3', 'w1q64_4', \n",
    "    'w1q64_5', 'w1q64_6', 'w1q64_7', 'w1q64_8', 'w1q64_9', 'w1q64_10', \n",
    "    'w1q64_11', 'w1q64_12', 'w1q64_13', 'w1q64_t_verb', ], \n",
    "  'serious_health_cond': ['binarize', 'w1q74_5', 'w1q74_6', 'w1q74_10', \n",
    "    'w1q74_11', 'w1q74_14', 'w1q74_17', 'w1q74_18', 'w1q74_20', ], \n",
    "  'disabled': ['binarize', 'w1q75', 'w1q76', ],\n",
    "  'suicidal_ideation': ['sum', 'w1q101', 'w1q105', 'w1q109', ], \n",
    "  'suicide_attempts': ['recode', 'w1q113', 'w1q114', ],\n",
    "  'sui_idea_age_first': ['min', 'w1q102', 'w1q103', 'w1q104', \n",
    "    'w1q106', 'w1q107', 'w1q108', 'w1q110', 'w1q111', 'w1q112', ],\n",
    "  'sui_idea_age_recent': ['max', 'w1q102', 'w1q103', 'w1q104', \n",
    "    'w1q106', 'w1q107', 'w1q108', 'w1q110', 'w1q111', 'w1q112', ],\n",
    "  'sui_attem_age_first': ['min', 'w1q115', 'w1q116', 'w1q117', ], \n",
    "  'sui_attem_age_recent': ['max', 'w1q115', 'w1q116', 'w1q117', ], \n",
    "  'nssh_age_first': ['min', 'w1q120', 'w1q121', 'w1q122', ],\n",
    "  'nssh_age_recent': ['max', 'w1q120', 'w1q121', 'w1q122', ],\n",
    "  'outness': ['sum', 'w1q123a', 'w1q123b', 'w1q123c', 'w1q123d', 'w1q124', ],\n",
    "  \n",
    "  'abusive_treatment': ['sum', 'w1q135a', 'w1q135b', \n",
    "    'w1q135c', 'w1q135d', 'w1q135e', 'w1q135f', ],\n",
    "  'work_neg_outcomes': ['recode', 'w1q137', 'w1q138', ], # account for age\n",
    "  \n",
    "  'abus_treat_non_queer': ['binarize', 'w1q136_1', 'w1q136_5', 'w1q136_6', \n",
    "    'w1q136_8', 'w1q136_9', 'w1q136_10', ],\n",
    "  'work_disc_non_queer': ['binarize', 'w1q139_1', 'w1q139_5', 'w1q139_6', \n",
    "    'w1q139_8', 'w1q139_9', 'w1q139_10', ],\n",
    "  'housing_disc_non_queer': ['binarize', 'w1q141_1', 'w1q141_5', 'w1q141_6', \n",
    "    'w1q141_8', 'w1q141_9', 'w1q141_10', ]\n",
    "  \n",
    "  'abus_treat_sex_gender': ['binarize', 'w1q136_2', 'w1q136_3', 'w1q136_4', ],\n",
    "  'work_disc_sex_gender': ['binarize', 'w1q139_2', 'w1q139_3', 'w1q139_4', ],\n",
    "  'housing_disc_sex_gender': ['binarize', \n",
    "    'w1q141_2', 'w1q141_3', 'w1q141_4', ]\n",
    "  \n",
    "  \n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# script 2 in full: 2024-05-29_cols_to_check.txt\n",
    "\n",
    "cols_to_check\n",
    "\n",
    "These are the columns that remained after I dropped all the scale items and non-imputed scale features.  What I'm working on now is going through them to try and reduce dimensionality.\n",
    "\n",
    "THIS IS A CONTINUATION OF THE PREVIOUS DOC BY THE SAME NAME\n",
    "\n",
    "['gp1',  'w1q142a', 'w1q142b', 'w1q142c', 'w1q142d', 'w1q142e', 'w1q142f', 'w1q142g', 'w1q142h', 'w1q142i', 'w1q142j', 'w1q142k', 'w1q143_1', 'w1q143_2', 'w1q143_3', 'w1q143_4', 'w1q143_5', 'w1q143_6', 'w1q143_7', 'w1q143_8', 'w1q143_9', 'w1q143_10', 'w1q145_1', 'w1q145_2', 'w1q145_3', 'w1q145_4', 'w1q145_5', 'w1q145_6', 'w1q145_7', 'w1q145_8', 'w1q145_9', 'w1q145_10', 'w1q146a', 'w1q146b', 'w1q146c', 'w1q146d', 'w1q146e', 'w1q146f', 'w1q146g', 'w1q146h', 'w1q146i', 'w1q146j', 'w1q146k', 'w1q146l', 'w1q162', 'w1q163_1', 'w1q163_2', 'w1q163_3', 'w1q163_4', 'w1q163_5', 'w1q163_6', 'w1q163_7', 'w1q163_8', 'w1q163_9', 'w1q163_10', 'w1q166', 'w1q167', 'w1q168', 'w1q169', 'w1q170_1', 'w1q170_2', 'w1q170_3', 'w1q170_4', 'w1q171_1', 'w1q171_2', 'w1q171_3', 'w1q171_4', 'w1q171_5', 'w1q171_6', 'w1q171_7', 'w1q171_8', 'w1q171_9', 'w1q172', 'w1q173', 'w1q174', 'w1q175', 'w1q176', 'w1q177_1', 'w1q177_2', 'w1q177_3', 'w1q177_4', 'w1q177_5', 'w1q177_6', 'w1q177_7', 'w1q177_8', 'w1q177_9', 'w1q177_10', 'w1q177_11', 'w1q177_12', 'w1q178', 'w1q179', 'w1q180', 'w1q181', 'w1q182', 'w1q183', 'w1q184', 'w1q185',  'w1sample',  'w1pinc', 'w1pinc_i', 'grespondent_date_w2', 'gsurvey', 'gp2', 'grace', 'grespondent_date_w3', 'wave3', 'nopolicecontact']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# script 3 in full: 2024-05-29_notes_about_non-scale_columns.txt\n",
    "\n",
    "notes about non-scale columns\n",
    "\n",
    "- for right now I've kept the current well being and the future predicted well being columns, because I think it could be interesting to see if predicting higher or lower than current correlates with anything.  I've also made instructions to combine them (higher is better, regardless fo hether it's current WB or optimism for future WB), which may end up being more parsimonious.\n",
    "- I've also kept Q3, which is about happiness, but I suspect it's very corr'd with WB.  check that, and then either combine or drop it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thursday, May 30, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "today I finished goin through all the columns!  I was less productive than usual because I went outside to protect the goslings from a hawk.  I used the 2024-05-23 setup file, and continued to add on to the exploring file.\n",
    "\n",
    "files created: \n",
    "- 2024-05-23_thru_2024-05-30_exploring_Meyer_2023_dataset.py\n",
    "- 2024-05-30_cols_to_check.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script 1 in full:\n",
    "# 2024-05-23_thru_2024-05-30_exploring_Meyer_2023_dataset.py\n",
    "\n",
    "# 5/23/24 AND 5/29/24\n",
    "\n",
    "# check out this dataset from Ilan Meyer, my beloved\n",
    "\n",
    "# Import Data\n",
    "meyer = pd.read_csv('./potential_datasets/2024_05_23_download_ICPSR_Meyer_2023_generations_data_attempt_2/ICPSR_37166/DS0007/37166-0007-Data.tsv', sep = '\\t', low_memory=False)\n",
    "meyer.shape #(1518, 1329), but we can cut that down.\n",
    "og_columns = list(meyer.columns)\n",
    "\n",
    "# for RIGHT NOW, I think I'm only going to look at wave 1\n",
    "# the wave stuff is REALLY COOL, but i don't think I understand\n",
    "# time series well enough to get into it for this project\n",
    "# if I get a model working based on wave 1, and write it up\n",
    "# and all that \"good enough,\" then I'll come back and add \n",
    "# some time series stuff\n",
    "# oh it can even just be another phase of the project!\n",
    "# no need to replace anything!  just section B.\n",
    "\n",
    "meyer['WAVEPARTICIPATED'].value_counts(dropna =False)\n",
    "# matches what's in the documentation\n",
    "\n",
    "# ok, so, for now, I am going to drop all the variables marked W2 and W3.  \n",
    "cols = list(meyer.columns)\n",
    "w1_cols = [c for c in cols if c[:2]!='W2']\n",
    "w1_cols = [c for c in w1_cols if c[:2]!='W3']\n",
    "len(w1_cols)\n",
    "\n",
    "# drop columns\n",
    "meyer = meyer[w1_cols]\n",
    "meyer.shape\n",
    "\n",
    "# NAs?\n",
    "meyer.isna().sum().sum()\n",
    "# it says 0, but it's not 0\n",
    "# they must have imputed it with an empty string or something\n",
    "# when I do .value_counts(), a lot of columns have a blank row, e.g.,\n",
    "#                      14\n",
    "# where there absolutely should not be a blank.\n",
    "# so that's fun!  gonna have to clean that up!\n",
    "\n",
    "# i originally thought they only included the base variables\n",
    "# and I would have to compute the scale scores myself, but\n",
    "# OMG THEY DID INCLUDE THE CALC'D SCALES!!!!!!\n",
    "# I JUST COULDN'T FIND THEM BECAUSE ALL CAPS!!\n",
    "\n",
    "meyer.columns = [c.lower() for c in list(meyer.columns)]\n",
    "\n",
    "# GOAL: drop the columns that contributed to combination scores\n",
    "\n",
    "# Make lists of the items that comprise scales\n",
    "soc_supp_items = ['w1q164a', 'w1q164b', 'w1q164c', \n",
    "  'w1q164d', 'w1q164e', 'w1q164f', 'w1q164g', 'w1q164h', \n",
    "  'w1q164i', 'w1q164j', 'w1q164k', 'w1q164l']\n",
    "\n",
    "ace_items = ['w1q151', 'w1q152', 'w1q153', \n",
    "  'w1q154', 'w1q155', 'w1q156', 'w1q157', \n",
    "  'w1q158', 'w1q159', 'w1q160', 'w1q161']\n",
    "\n",
    "childhd_gnc_items = [\n",
    "  'w1q147', 'w1q148', 'w1q149', 'w1q150']\n",
    "\n",
    "############# NOT CONDENSED (YET) #############\n",
    "# strain_items = ['w1q146a', 'w1q146b', \n",
    "#   'w1q146c', 'w1q146d', 'w1q146e', 'w1q146f', \n",
    "#   'w1q146g', 'w1q146h', 'w1q146i', 'w1q146j', \n",
    "#   'w1q146k', 'w1q146l']\n",
    "############# NOT CONDENSED (YET) #############\n",
    "\n",
    "daily_discr_items = ['w1q144a', 'w1q144b', \n",
    "  'w1q144c','w1q144d', 'w1q144e', 'w1q144f', \n",
    "  'w1q144g', 'w1q144h', 'w1q144i']\n",
    "\n",
    "# bi_stigma_items = [  # only in wave 2\n",
    "#   'w2q117', 'w2q118', 'w2q119', 'w2q120']\n",
    "\n",
    "int_homo_items = ['w1q128', 'w1q129', \n",
    "  'w1q130', 'w1q131', 'w1q132']\n",
    "\n",
    "felt_stigma_items = ['w1q125', 'w1q126', 'w1q127']\n",
    "\n",
    "drug_items = ['w1q90', 'w1q91', 'w1q92', \n",
    "  'w1q93', 'w1q94', 'w1q95', 'w1q96', \n",
    "  'w1q97', 'w1q98', 'w1q99', 'w1q100']\n",
    "\n",
    "alc_items = ['w1q85', 'w1q86', 'w1q87']\n",
    "\n",
    "ment_dis_items = ['w1q77a', 'w1q77b', \n",
    "  'w1q77c', 'w1q77d', 'w1q77e', 'w1q77f']\n",
    "\n",
    "hc_ster_threat_items = [\n",
    "  'w1q60', 'w1q61', 'w1q62', 'w1q63']\n",
    "\n",
    "comm_conn_items = ['w1q53', 'w1q54', \n",
    "  'w1q55', 'w1q56', 'w1q57', 'w1q58', 'w1q59']\n",
    "\n",
    "lgbis_items = [\n",
    "  'w1q40', 'w1q41', 'w1q42', 'w1q43', 'w1q44']\n",
    "\n",
    "meim_items = [\n",
    "  'w1q21', 'w1q22', 'w1q23', 'w1q24', 'w1q25', 'w1q26']\n",
    "\n",
    "swl_items = [\n",
    "  'w1q186', 'w1q187', 'w1q188', 'w1q189', 'w1q190']\n",
    "\n",
    "swb_items = ['w1q04', 'w1q05', 'w1q06', 'w1q07', \n",
    "  'w1q08', 'w1q09', 'w1q10', 'w1q11', 'w1q12', \n",
    "  'w1q13', 'w1q14', 'w1q15', 'w1q16', 'w1q17', 'w1q18']\n",
    "\n",
    "drop_cols = (soc_supp_items + ace_items + childhd_gnc_items + \n",
    "  daily_discr_items + int_homo_items + felt_stigma_items + \n",
    "  drug_items + alc_items + ment_dis_items + hc_ster_threat_items + \n",
    "  comm_conn_items + lgbis_items + meim_items + swl_items + swb_items)\n",
    "len(drop_cols)\n",
    "\n",
    "meyer.drop(columns = drop_cols, inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "# these are the scales\n",
    "combo_features = ['w1socialwb', 'w1socialwb_i', \n",
    "  'w1lifesat', 'w1lifesat_i', 'w1meim', 'w1meim_i', \n",
    "  'w1idcentral', 'w1idcentral_i', 'w1connectedness',\n",
    "  'w1connectedness_i', 'w1hcthreat', 'w1hcthreat_i', \n",
    "  'w1kessler6', 'w1kessler6_i', 'w1auditc', \n",
    "  'w1auditc_i', 'w1dudit', 'w1dudit_i', 'w1feltstigma', \n",
    "  'w1feltstigma_i', 'w1internalized', 'w1internalized_i', \n",
    "  'w1everyday', 'w1everyday_i', 'w1childgnc', 'w1childgnc_i',\n",
    "  'w1ace', 'w1ace_i', 'w1ace_emo', 'w1ace_emo_i', \n",
    "  'w1ace_inc', 'w1ace_inc_i', 'w1ace_ipv', 'w1ace_ipv_i', \n",
    "  'w1ace_men', 'w1ace_men_i', 'w1ace_phy', 'w1ace_phy_i', \n",
    "  'w1ace_sep', 'w1ace_sep_i', 'w1ace_sex', 'w1ace_sex_i', \n",
    "  'w1ace_sub', 'w1ace_sub_i', 'w1socsupport', \n",
    "  'w1socsupport_fam', 'w1socsupport_fam_i', \n",
    "  'w1socsupport_fr', 'w1socsupport_fr_i', \n",
    "  'w1socsupport_i', 'w1socsupport_so', 'w1socsupport_so_i']\n",
    "\n",
    "# split them up so I can check whether \n",
    "# the _i columns really are free of missing values\n",
    "combo_imputed = [c for c in combo_features if c[-2:]=='_i']\n",
    "combo_not_imputed = [c for c in combo_features if c[-2:]!='_i']\n",
    "\n",
    "# check my splits\n",
    "a = [c for c in combo_imputed if c in combo_not_imputed]\n",
    "b = [c for c in combo_not_imputed if c in combo_imputed]\n",
    "if len(a)==len(b)==0:\n",
    "  print('no overlap')\n",
    "  del (a, b)\n",
    "if (len(combo_imputed)+len(combo_not_imputed))==len(combo_features):\n",
    "  print('all accounted for')\n",
    "  \n",
    "# check for \"missing\" values\n",
    "for i in combo_imputed:\n",
    "  print('')\n",
    "  print(f'{i}, {meyer[i].isna().sum()} NAs')\n",
    "  print(meyer[i].value_counts(dropna = False).sort_values())\n",
    "\n",
    "# see how bad it was in the original\n",
    "for i in combo_not_imputed:\n",
    "  print('')\n",
    "  print(f'{i}, {meyer[i].isna().sum()} NAs')\n",
    "  print(meyer[i].value_counts(dropna = False).sort_values())\n",
    "missing_values = [('w1ace_sex', 75), ('w1ace_phy', 61), \n",
    "  ('w1ace_ipv', 139), ('w1ace_emo', 91), ('w1ace', 277), \n",
    "  ('w1everyday', 40), ('w1dudit', 66), ('w1connectedness', 51), \n",
    "  ('w1socialwb', 59), ('most others', '15-35')]\n",
    "\n",
    "# very grim and not at all surprising that column with the most missings is IPV\n",
    "# (from the ACE survey - so IPV among parents, not participants themselves)\n",
    "# >>> meyer['w1ace_ipv'].value_counts(dropna = False)\n",
    "# w1ace_ipv\n",
    "# 0    950\n",
    "# 1    429\n",
    "#      139\n",
    "# Name: count, dtype: int64\n",
    "# >>> meyer['w1ace_ipv_i'].value_counts(dropna = False)\n",
    "# w1ace_ipv_i\n",
    "# 0    1024\n",
    "# 1     494\n",
    "# Name: count, dtype: int64\n",
    "# >>> 1024-950\n",
    "# 74\n",
    "# >>> 139-74\n",
    "# 65\n",
    "\n",
    "# ok.  their imputation method seems really solid, but that is sus af.\n",
    "# of the people who declined to answer about IPV, you think the majority \n",
    "# of them would have said NO!?!??!??????\n",
    "# I'd be tempted to put every one of those guys down as a yes!!!!\n",
    "# let me see the overlap with other stigmatized stuff\n",
    "len(meyer[(meyer['w1ace_ipv']==' ') & (meyer['w1ace_sex']==' ')])\n",
    "# 30 huh damn I would have thought like all of them\n",
    "# wait.\n",
    "len(meyer[(meyer['w1ace_ipv']=='1') & (meyer['w1ace_sex']==' ')])  # 19\n",
    "len(meyer[(meyer['w1ace_ipv']==' ') & (meyer['w1ace_sex']=='1')])  # 51\n",
    "# ok! that actually lends credibility to their imputation though\n",
    "# if people were too ashamed to admit IPV, they'd probably decline to answer\n",
    "# about CSA too.  these people, at least, really could go either way on IPV.\n",
    "\n",
    "# This is all actually moot \n",
    "# because I don't have a better idea for imputation than what they did.\n",
    "# I think the thing to do, if I can, is to model it on the imputed data\n",
    "# then model it again on the subsample where I've dropped all these NAs\n",
    "# at least the ACE NAs, where there's really a lot.\n",
    "# could also try modeling it just without those columns\n",
    "# a random forest would probably be good, \n",
    "# make sure the whole thing isn't hinging on imputed data\n",
    "# Meyer also probably ran some tests in his paper to make sure this is ok\n",
    "\n",
    "# OK SO.\n",
    "\n",
    "# I've gotta reduce my dimensions\n",
    "# the imputation methods they used seem very reasonable\n",
    "# (see p. 37 of 37166-Documentation-methodology.pdf)\n",
    "# so I'm going to drop the non-imputed ones\n",
    "meyer.drop(columns = combo_not_imputed, inplace = True)\n",
    "meyer.shape  #(1518, 373)\n",
    "\n",
    "# ok!  love that!  let's see what else I can chuck.\n",
    "cols_to_check = [c for c in list(meyer.columns) if c not in combo_imputed]\n",
    "print(cols_to_check)\n",
    "\n",
    "# I am looking thru the documentation and making 2 lists\n",
    "# the variables should ROUGHLY correspond to each other\n",
    "# maybe I'll do some line breaks or whatever.\n",
    "drop_cols_2 = [\n",
    "  'w1q165', 'w1q27', 'w1q28', \n",
    "  'w1q20_1', 'w1q20_2', 'w1q20_3', 'w1q20_4', 'w1q20_5', 'w1q20_6', 'w1q20_7', \n",
    "  'w1q29', 'w1q29_t_verb', \n",
    "  # kept all ed columns\n",
    "  'gruca', 'gruca_i', 'gurban', 'gzipstate',  'gzipcode', \n",
    "  'w1hinc', 'w1poverty', 'w1povertycat', \n",
    "  'w1q133', 'w1q133_1', 'w1q133_2', 'w1q133_3', \n",
    "  # from here I'm just going thru the 831 page documentation and picking stuff out\n",
    "  'w1weight_orig', \n",
    "  'gmsaname', 'gmethod_type', 'gmethod_type_w2', 'gmethod_type_w3', \n",
    "  'w1cumulative_wt_nr1', 'w1cumulative_wt_nr2', 'w1cumulative_wt_nr3', \n",
    "  'w1cumulative_wt_sampling', 'w1weighting_cell_nr1', 'w1weighting_cell_nr2and3', \n",
    "  'w1frame_wt', \n",
    "]\n",
    "\n",
    "keep_cols = ['studyid', 'waveparticipated', 'w1survey_yr', \n",
    "  'w1age', 'cohort', 'w1sex', 'w1gender', 'w1sex_gender', \n",
    "  'screen_race', 'w1race',\n",
    "  'w1sexualid', 'w1sexminid', \n",
    "  'geduc1', 'geduc2', 'geducation', \n",
    "  'gurban_i', 'gcendiv', 'gcenreg', 'gmilesaway', 'gmilesaway2', \n",
    "  'w1hinc_i', 'w1poverty_i', 'w1povertycat_i', \n",
    "  'w1conversion', 'w1conversionhc', 'w1conversionrel', \n",
    "  'w1weight_full', 'gemployment2010', \n",
    "]\n",
    "\n",
    "check_cols = [\n",
    "  # no age, sex stuff\n",
    "  'w1q20_t_verb', # I suspect this is part of q20 and got rolled into w1race, but I'm not sure off the top of my head\n",
    "]\n",
    "\n",
    "# can I weed out the redacted ones quickly?\n",
    "\n",
    "x = list(meyer.columns)\n",
    "x_redacted = [c for c in x if meyer[c].nunique()==1]\n",
    "# it works!  but I already caught these guys\n",
    "\n",
    "\n",
    "# I finished all the variables that were easy to parse out and assigned\n",
    "# them to a list above.  I am now going to drop the ones that I indicated,\n",
    "# and then I will go through the 831 page documentation to make sense \n",
    "# of what's left and try to drop more.\n",
    "\n",
    "meyer.drop(columns = drop_cols_2, inplace = True) # (1518, 337)\n",
    "\n",
    "suicidal_idea_beh = [\n",
    "  'w1q101', 'w1q102', 'w1q103', 'w1q104', 'w1q105', 'w1q106', 'w1q107', \n",
    "  'w1q108', 'w1q109', 'w1q110', 'w1q111', 'w1q112', 'w1q113', 'w1q114', \n",
    "  'w1q115', 'w1q116', 'w1q117', 'w1q118', 'w1q119', 'w1q120', 'w1q121',\n",
    "  'w1q122']\n",
    "\n",
    "drop_cols_2_again = [\n",
    "  'w1q31a', 'w1q31b', 'w1q31c', 'w1q31d', # redundant with sexual identity\n",
    "  'w1q66_1', 'w1q66_2', 'w1q66_3', 'w1q66_4', 'w1q66_5', 'w1q66_t_verb', # interesting to compare \n",
    "  #ER to Dr ofc, but not for this study.  too many columns already ^\n",
    "  'w1q67', 'w1q68_1', 'w1q68_2', 'w1q68_3', 'w1q70', 'w1q71', 'w1q73', # <- same ^ and V\n",
    "  'w1q74_1', 'w1q74_2', 'w1q74_3', 'w1q74_4', 'w1q74_7', 'w1q74_8', 'w1q74_9', \n",
    "  'w1q74_12', 'w1q74_13', 'w1q74_15', 'w1q74_16', 'w1q74_19', 'w1q80', 'w1q81',\n",
    "  'w1q82', 'w1q83', 'w1q84', 'w1q88', 'w1q118', 'w1q170_1', 'w1q170_2', \n",
    "  'w1q170_3', 'w1q170_4', 'w1q172', 'w1q173', 'w1q174', 'w1q176', 'w1q177_1', \n",
    "  'w1q177_2', 'w1q177_3', 'w1q177_4', 'w1q177_5', 'w1q177_6', 'w1q177_7', \n",
    "  'w1q177_8', 'w1q177_9', 'w1q177_10', 'w1q177_11', 'w1q177_12', 'w1q178', \n",
    "  'w1q182', 'w1q183', 'w1q184', 'w1q185',  'w1sample',  'w1pinc', \n",
    "  'grespondent_date_w2', 'gsurvey', 'gp2', 'grace', 'grespondent_date_w3', \n",
    "  'wave3', 'nopolicecontact']\n",
    "\n",
    "keep_cols_good_on_own = ['w1q01', 'w1q02', 'w1q03', 'w1q32', \n",
    "  'w1q33', 'w1q34', 'w1q35', 'w1q36', 'w1q37', 'w1q38', 'w1q52', \n",
    "  'w1q65',  'w1q69', 'w1q72', 'w1q74_21', 'w1q74_22', 'w1q74_23', \n",
    "  'w1q78', 'w1q79', 'w1q89', 'w1q119', 'w1q134', 'w1q136_7', 'w1q139_7', \n",
    "  'w1q140', 'w1q141_7', 'w1q143_7', 'w1q145_7', 'w1q162',  'w1q163_7', \n",
    "  'w1q166', 'w1q167', 'w1q168', 'w1q169', 'w1q175', 'gp1', 'w1pinc_i']\n",
    "\n",
    "keep_cols_ohe_done = ['w1q30_1', 'w1q30_2', 'w1q30_3', 'w1q30_4', 'w1q30_5', \n",
    "  'w1q39_1', 'w1q39_2', 'w1q39_3', 'w1q39_4', 'w1q39_5', 'w1q39_6', 'w1q39_7', \n",
    "  'w1q39_8', 'w1q39_9', 'w1q39_10', 'w1q39_11', 'w1q39_12', 'w1q39_t_verb',\n",
    "  'w1q171_1', 'w1q171_2', 'w1q171_3', 'w1q171_4', 'w1q171_5', 'w1q171_6', \n",
    "  'w1q171_7', 'w1q171_8', 'w1q171_9']\n",
    "\n",
    "# manually combine columns in ways that *I* think make sense\n",
    "# here I am creating a dictionary where the keys are the new column names I want\n",
    "# and the values are lists, wherein the first value on the list is the method \n",
    "# of combination, and the remainder are the columns to be combined\n",
    "# if the method is 'recode', then I probably need to give it more direct attn\n",
    "# but all the others I am hoping to be able to automate\n",
    "# the end goal is to use this dictionary to either generate or directly run\n",
    "# the code to create the new columns and drop the ones no longer needed\n",
    "# before dropping the columns, I should check them against keep_cols\n",
    "# to make sure I'm not hoping to keep them individually in addition to combo'ing\n",
    "\n",
    "feat_eng_dict = {'pers_well_being': ['sum', 'w1q01', 'w1q02'],\n",
    "  'neighb_welcoming': ['mean', 'w1q19a', 'w1q19b', 'w1q19c', 'w1q19d'],\n",
    "  'age_awakening': ['min','w1q45', 'w1q46', 'w1q47', 'w1q48'],\n",
    "  'age_out': ['min', 'w1q49', 'w1q50', 'w1q51'],\n",
    "  'health_insurance': ['binarize', 'w1q64_1', 'w1q64_2', 'w1q64_3', 'w1q64_4', \n",
    "    'w1q64_5', 'w1q64_6', 'w1q64_7', 'w1q64_8', 'w1q64_9', 'w1q64_10', \n",
    "    'w1q64_11', 'w1q64_12', 'w1q64_13', 'w1q64_t_verb'], \n",
    "  'serious_health_cond': ['binarize', 'w1q74_5', 'w1q74_6', 'w1q74_10', \n",
    "    'w1q74_11', 'w1q74_14', 'w1q74_17', 'w1q74_18', 'w1q74_20'], \n",
    "  'disabled': ['binarize', 'w1q75', 'w1q76'],\n",
    "  'suicidal_ideation': ['sum', 'w1q101', 'w1q105', 'w1q109'], \n",
    "  'suicide_attempts': ['recode', 'w1q113', 'w1q114'],\n",
    "  'sui_idea_age_first': ['min', 'w1q102', 'w1q103', 'w1q104', \n",
    "    'w1q106', 'w1q107', 'w1q108', 'w1q110', 'w1q111', 'w1q112'],\n",
    "  'sui_idea_age_recent': ['max', 'w1q102', 'w1q103', 'w1q104', \n",
    "    'w1q106', 'w1q107', 'w1q108', 'w1q110', 'w1q111', 'w1q112'],\n",
    "  'sui_attem_age_first': ['min', 'w1q115', 'w1q116', 'w1q117'], \n",
    "  'sui_attem_age_recent': ['max', 'w1q115', 'w1q116', 'w1q117'], \n",
    "  'nssh_age_first': ['min', 'w1q120', 'w1q121', 'w1q122'],\n",
    "  'nssh_age_recent': ['max', 'w1q120', 'w1q121', 'w1q122'],\n",
    "  'outness': ['sum', 'w1q123a', 'w1q123b', 'w1q123c', 'w1q123d', 'w1q124'],\n",
    "  'abusive_treatment': ['sum', 'w1q135a', 'w1q135b', \n",
    "    'w1q135c', 'w1q135d', 'w1q135e', 'w1q135f'],\n",
    "  'work_neg_outcomes': ['recode', 'w1q137', 'w1q138'], # account for age\n",
    "  'abus_treat_non_queer': ['binarize', 'w1q136_1', 'w1q136_5', 'w1q136_6', \n",
    "    'w1q136_8', 'w1q136_9', 'w1q136_10'],\n",
    "  'stress_past_year_gen': ['recode', 'w1q142a', 'w1q142h', 'w1q142i'],\n",
    "  'stress_past_year_work': ['recode', 'w1q142b', 'w1q142c', 'w1q142e'],\n",
    "  'stress_past_year_interpersonal': ['recode', 'w1q142d', 'w1q142f', 'w1q142g'],\n",
    "  'stress_past_year_crime': ['recode', 'w1q142j', 'w1q142k'],\n",
    "  'work_disc_non_queer': ['binarize', 'w1q139_1', 'w1q139_5', 'w1q139_6', \n",
    "    'w1q139_8', 'w1q139_9', 'w1q139_10'],\n",
    "  'housing_disc_non_queer': ['binarize', 'w1q141_1', 'w1q141_5', 'w1q141_6', \n",
    "    'w1q141_8', 'w1q141_9', 'w1q141_10'],\n",
    "  'stress_past_year_non_queer': ['binarize', 'w1q143_1', 'w1q143_5', 'w1q143_6', \n",
    "    'w1q143_8', 'w1q143_9', 'w1q143_10'],\n",
    "  'daily_discr_non_queer': ['binarize', 'w1q145_1', 'w1q145_5', 'w1q145_6'],\n",
    "  'childhd_bullying_non_queer': ['binarize', 'w1q163_1', 'w1q163_5', 'w1q163_6', \n",
    "    'w1q163_8', 'w1q163_9', 'w1q163_10'],\n",
    "  'abus_treat_sex_gender': ['binarize', 'w1q136_2', 'w1q136_3', 'w1q136_4'],\n",
    "  'work_disc_sex_gender': ['binarize', 'w1q139_2', 'w1q139_3', 'w1q139_4'],\n",
    "  'housing_disc_sex_gender': ['binarize', 'w1q141_2', 'w1q141_3', 'w1q141_4'],\n",
    "  'stress_past_year_sex_gender': ['binarize', 'w1q143_2', 'w1q143_3', 'w1q143_4'],\n",
    "  'daily_discr_sex_gender': ['binarize', 'w1q145_2', 'w1q145_3', 'w1q145_4', \n",
    "    'w1q145_8', 'w1q145_9', 'w1q145_10'],\n",
    "  'childhd_bullying_sex_gender': ['binarize', 'w1q163_2', 'w1q163_3', 'w1q163_4'],\n",
    "  'religiosity': ['recode', 'w1q179', 'w1q180', 'w1q181'], \n",
    "  'chronic_strain': ['sum', 'w1q146a', 'w1q146b', 'w1q146c', 'w1q146d', 'w1q146e', \n",
    "    'w1q146f', 'w1q146g', 'w1q146h', 'w1q146i', 'w1q146j', 'w1q146k', 'w1q146l']}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# script 2 in full:  \n",
    "\\# 2024-05-30_cols_to_check.txt\n",
    "\n",
    "cols_to_check\n",
    "\n",
    "These are the columns that remained after I dropped all the scale items and non-imputed scale features.  What I'm working on now is going through them to try and reduce dimensionality.\n",
    "\n",
    "THIS IS A CONTINUATION OF THE PREVIOUS DOC BY THE SAME NAME\n",
    "\n",
    "[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Friday, May 31, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "\n",
    "\n",
    "files created: \n",
    "- x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script 1 in full:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monday, June 3, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "today I continued working on a plan for null values and imputations.  I played around with a few different approaches, but ultimately I just made a list of every column with null values and made a plan for them one by one.  I am still debating whether ... something.   I forgot what the end of that sentence was.  I really want to do fancy, high quality imputation, but I think mode and median are the way to go, just for expediency.  Once all the null values are gone, it will be time to do the combinations as defined in the dictionary, do some more column dropping, and write up a data cleaning notebook.\n",
    "\n",
    "I dropped 11 rows that had straight people in them.\n",
    "\n",
    "**I'm going to use _ei in my column names to indicate \"Emily imputed.\"  The original authors used _i for their imputations.**\n",
    "\n",
    "I used the 5-31 setup script again, and the giant data cleaning script.  I then created a few documents as I worked.\n",
    "\n",
    "files created: \n",
    "- imputation_plan.xlsx -- a list of all columns with null values, how many, and what to do with them.\n",
    "- can-i-impute-0s.md was actually created on 5/31, but it's a good one.  i looked through all the columns with naturally occuring 0s, and decided whether or not I could impute 0s for the NAs without borking everything up\n",
    "- I created 04_scratch_work/ in the repo, and 01_notes/ and 02_scripts/ within it.  notes/ has copies of all my notes, and scripts/ has copies of the scripts\n",
    "- I updated my gitignore to allow .py files, for now at least\n",
    "- 2024-05-31_continuing_messing_with_meyer_columns.py was created on Friday with just a little bit of bridge work between the column stuff in the giant cleaning script and the imputation planning I did today.\n",
    "- 2024-06-03_imputation_groundwork.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script 1 in full:\n",
    "# 2024-05-31_continuing_messing_with_meyer_columns.py\n",
    "\n",
    "meyer.shape\n",
    "meyer.columns\n",
    "\n",
    "\n",
    "# Before we do this next bit, let's preserve these guys\n",
    "keep_cols = combo_imputed + keep_cols + keep_cols_good_on_own + keep_cols_ohe_done\n",
    "\n",
    "# Now I need to handle the columns in the dictionary\n",
    "\n",
    "# did I duplicate any names?\n",
    "for i in feat_eng_dict.keys():\n",
    "  if i in keep_cols:\n",
    "    print(i)\n",
    "\n",
    "# nope!\n",
    "\n",
    "# meyer_test = pd.read_csv('./potential_datasets/2024_05_23_download_ICPSR_Meyer_2023_generations_data_attempt_2/ICPSR_37166/DS0007/37166-0007-Data.tsv', sep = '\\t', low_memory=False, na_values = ' ')\n",
    "# meyer_test_nas = pd.DataFrame(data = meyer_test.isna().sum())\n",
    "\n",
    "# check again because I figured out how to handle the \"NAs\" as strings\n",
    "meyer.isna().sum().sum()\n",
    "for i in list(zip(list(meyer.dtypes), list(meyer.columns))):\n",
    "  print(i)\n",
    "\n",
    "# drop these guys\n",
    "meyer.drop(columns = drop_cols_2_again, inplace = True)\n",
    "\n",
    "# I want to fill all my NAs with 0s, so I can do math on things.\n",
    "# Can I do that?  Do any of these columns have natural 0s?\n",
    "\n",
    "for i in list(meyer.columns):\n",
    "  a = list(meyer[i].unique())\n",
    "  if 0 in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script 2 in full: \n",
    "# 2024-06-03_imputation_groundwork.py\n",
    "\n",
    "# June 3, 2024\n",
    "# Imputation\n",
    "\n",
    "# This is continuing some work I started last week.\n",
    "# I have defined a dictionary full of columns that I\n",
    "# want to combine one way or another.  I can't do that\n",
    "# if they're full of NAs.  Imputing 0s would be the \n",
    "# easiest way to do it, but I don't want to introduce\n",
    "# unreasonable amounts of error into my data.\n",
    "\n",
    "# I'm going to use _ei to indicate \"Emily imputed.\"\n",
    "# The original authors used _i for their imputations.\n",
    "\n",
    "# Last week I made a list of all the columns with \n",
    "# naturally occurring 0s in them. Today I want a \n",
    "# list of all remaining NAs.\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "meyer.isna().sum().sort_values(ascending = False)\n",
    "\n",
    "# Re-Import Data for clean copy\n",
    "mey = pd.read_csv(\n",
    "  './potential_datasets/2024_05_23_download_ICPSR_Meyer_2023_generations_data_attempt_2/ICPSR_37166/DS0007/37166-0007-Data.tsv', \n",
    "  sep = '\\t', low_memory=False, na_values = ' ') # Thanks to ibrahim rupawala for highlighting the na_values argument\n",
    "  # https://stackoverflow.com/questions/13445241/replacing-blank-values-white-space-with-nan-in-pandas/47105408#47105408\n",
    "\n",
    "mey.shape\n",
    "mey.columns = [c.lower() for c in list(mey.columns)]\n",
    "\n",
    "# I am going to try to find related columns for logical \n",
    "# imputation if possible, or at least to justify 0s.\n",
    "# (I'm going to try very hard not to get too derailed.)\n",
    "\n",
    "# Friends of gmilesaway2\n",
    "gmilesaway2 = mey[mey['gmilesaway2'].isna()]\n",
    "gmilesaway2['w1q67'].value_counts(dropna = False)\n",
    "mey['gmilesaway2'].groupby(mey['w1q67']).value_counts(dropna = False, normalize = True)\n",
    "# w1q67  gmilesaway2\n",
    "# 1.0    0.0            0.882353\n",
    "#        1.0            0.094118\n",
    "#        NaN            0.023529\n",
    "# 2.0    0.0            0.838710\n",
    "#        1.0            0.153226\n",
    "#        NaN            0.008065\n",
    "# 3.0    0.0            0.714844\n",
    "#        1.0            0.273438\n",
    "#        NaN            0.011719\n",
    "\n",
    "mey['gmilesaway2'].groupby(mey['gurban']).value_counts(dropna = False, normalize = True)\n",
    "# gurban  gmilesaway2\n",
    "# 0.0     1.0            0.675824\n",
    "#         0.0            0.313187\n",
    "#         NaN            0.010989\n",
    "# 1.0     0.0            0.801370\n",
    "#         1.0            0.197108\n",
    "#         NaN            0.001522\n",
    "\n",
    "# It's actually not a very good assumption that they live\n",
    "# more than 60 miles away. 73.5% of people live w/i 60m!\n",
    "# And yet, most people have never gone.  \n",
    "# I think the thing to impute here is a 0 under the original \n",
    "# meaning, that is, within 60 miles.\n",
    "\n",
    "\n",
    "# OK ACTUALLY\n",
    "\n",
    "# I've made a list of the NAs and I'm looking at THOSE\n",
    "# for how to impute.  THAT'S what this next section is.\n",
    "\n",
    "# w1q101\n",
    "# subset investigation\n",
    "s = mey.loc[cond, suicidal_idea_beh]\n",
    "s = s[['w1q101',  'w1q105', 'w1q109', 'w1q118', 'w1q119', 'w1q121']]\n",
    "\n",
    "# row condition\n",
    "cond = mey['w1q101'].isna()\n",
    "\n",
    "# column conditions\n",
    "y1 = meyer[((meyer['w1q105'].notna()) & (meyer['w1q105']!=1))]\n",
    "y2 = meyer[((meyer['w1q109'].notna()) & (meyer['w1q109']!=1))]\n",
    "y3 = meyer[((meyer['w1q113'].notna()) & (meyer['w1q113']!=1))]\n",
    "\n",
    "# In another pass I'd love to match the numbers\n",
    "# but for now we're just going to impute \"once.\"\n",
    "# It is at least not overcounting.\n",
    "meyer.loc[[(y1 | y2 | y3) & cond], 'w1q101'] = 2\n",
    "len(meyer[y1])\n",
    "\n",
    "# check these age columns\n",
    "cond = mey['w1q102'].isna() # none!\n",
    "cond = mey['w1q103'].isna()\n",
    "# ok this would take forever. n<10 -> impute median\n",
    "cond = mey['w1q112'].isna()\n",
    "s = mey.loc[cond, suicidal_idea_beh]\n",
    "\n",
    "# nssh\n",
    "cond = mey['w1q119'].isna()\n",
    "s = mey.loc[cond, suicidal_idea_beh]\n",
    "\n",
    "# hey, is it just a few rows with a ton of missing data?\n",
    "# I hate to lose data but if they truly didn't fill out anything...\n",
    "meyer.iloc[10,:].isna().sum()\n",
    "# oh right, there are all those OHE columns with planned missing\n",
    "# so I need to start imputing first, then recheck these\n",
    "# then decide whether to drop anything\n",
    "\n",
    "# why are there straight people in here?\n",
    "cond = mey['w1sexminid'].isna()\n",
    "s = mey[cond]\n",
    "s['w1sexualid']\n",
    "\n",
    "# begone, straights!\n",
    "meyer.shape # (1518, 267)\n",
    "meyer = meyer[meyer['w1sexualid']!=1]\n",
    "meyer.shape # (1507, 267)\n",
    "\n",
    "# ugh I really wish I had done that before!!\n",
    "# let's look at this again\n",
    "pd.set_option('display.max_rows', None)\n",
    "meyer.isna().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuesday, June 4, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "I did a lot of imputation today!  I worked really hard and did most of my imputation.  I left a handful of the complicated ones for tomorrow, and I need to tinker with the simple imputer ones again tomorrow too.\n",
    "\n",
    "I followed the guidelines I laid out in the spreadsheet yesterday.  It took several tries to get an `np.where()` statement working (I tried a whole function first, and then the humble `np.where()` is what actually got the job done!), but was then successful.  I checked and rechecked and rechecked very carefully to make sure I wasn't introducing error into my data.\n",
    "\n",
    "### This paragraph can probably go in as is!\n",
    "\n",
    "I chose to complete imputation on the entire dataset, prior to the train test split, because my primary interest lies in an inferential model.  This means that it is less crucial that my workflow and model be generalizable to new data gathered in the future.  Furthermore, because this data concerns sensitive personal and political issues, the median or mode responses would likely change over time if the survey were to be readministered, certainly over a long enough timeframe.  (Note: The obvious implications of this temporal drift for a 2016 dataset will be further discussed in the limitations and recommendations sections.)  It would be entirely illogical to fit the imputer on a subset of data from 2016 and then continue projecting those values forward into new data indefinitely.  Therefore, I prioritized fidelity to the current data over adaptability to hypothetical future data, and fit the imputers on my entire dataset.\n",
    "\n",
    "BUT the stupid 97s-99s threw off all the summary stats so the si fits I did today don't work.  I need to clean those columns up first and then re-fit them.  I coped the si syntax into its own script to make this easier, and so that I could continue the imputation I was doing today without making that script a mobius strip of time shenanigans.\n",
    "\n",
    "ALSO, I finally emailed UMich for permission to post their stuff.  Hopefully they get back to me quicker than I got to them.\n",
    "\n",
    "files created: \n",
    "\n",
    "- 2024-06-04_imputation_ATTEMPT-1_dealing-with-7s-and-97s-indiy-ohe-columns.py\n",
    "- - first attempt at this nonsense, tried a ton of things, abandoned this script once one of them worked\n",
    "\n",
    "- 2024-06-04_imputation_dealing-with-7s-and-97s-indiy-ohe-columns.py\n",
    "- - copied just the thing that worked from the last script and continued from there\n",
    "\n",
    "- 2024-06-04_imputation_misc.py\n",
    "- - once I got that first batch of stuff worked out, I jumped over here to get a clean start on the rest of it\n",
    "\n",
    "- 2024-06-04_simple_imputer_stuff.py\n",
    "- - this is a COPY of the si stuff from 2024-06-04_imputation_misc.py, because I discovered more 97s-99s that I need to clear out before it actually makes sense to do si\n",
    "\n",
    "- meyer_backup_diy_ohe_no_97s_7s.csv\n",
    "- - safety first\n",
    "\n",
    "- meyer_much_imputation_done_but_not_si_yet.csv\n",
    "- - safety first\n",
    "\n",
    "- MODIFIED imputation_plan.xlsx\n",
    "- - notes and color coding as I go along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 1 in full:\n",
    "2024-06-04_imputation_ATTEMPT-1_dealing-with-7s-and-97s-indiy-ohe-columns.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuesday, June 4, 2024\n",
    "# Imputation, Take 2\n",
    "\n",
    "# Ok, so yesterday I made an excel spreadsheet list of all of NAs\n",
    "# and decided what to do with them.  There are a bunch of different \n",
    "# methods (I chose what I thought was best for each type of data), \n",
    "# and I think the neatest way to handle all of them without making\n",
    "# a giant mess is to break up the dataframe (include the ID in \n",
    "# each subset so that they can be cleanly re-merged), use a \n",
    "# real, official imputer, and then put it back together again.\n",
    "\n",
    "# Oooh boy this could be dangerous.  I'd have to drop them from\n",
    "# the original to avoid overwriting or otherwise losing stuff.\n",
    "# Let me see what the shape is now.\n",
    "\n",
    "# REMEMBER TO DROP THE STRAIGHTS! IT'S THE ONLY THING I NEED \n",
    "# FROM THE 2024-06-03 IMPUTATION SCRIPT!\n",
    "\n",
    "meyer.shape # (1507, 267)\n",
    "# So that it is the shape I want to get back at the end,\n",
    "# minus any that I consciously, deliberately drop.\n",
    "\n",
    "\n",
    "# lmao ok let's start by dropping\n",
    "\n",
    "s = '''\n",
    "w1q20_t_verb\n",
    "w1q39_7\n",
    "w1q39_12\n",
    "w1q39_t_verb\n",
    "w1q39_6\n",
    "w1q39_5\n",
    "w1q39_2\n",
    "w1q39_3\n",
    "w1q39_4\n",
    "w1q39_1\n",
    "w1q39_9\n",
    "w1q39_8\n",
    "w1q39_10\n",
    "w1q39_11\n",
    "gp1\n",
    "gemployment2010\n",
    "gmilesaway'''\n",
    "\n",
    "print(s.replace('\\n', '\", \"'))\n",
    "\n",
    "drop_cols_3 = [\"w1q20_t_verb\", \"w1q39_7\", \"w1q39_12\", \n",
    "  \"w1q39_t_verb\", \"w1q39_6\", \"w1q39_5\", \"w1q39_2\", \"w1q39_3\", \n",
    "  \"w1q39_4\", \"w1q39_1\", \"w1q39_9\", \"w1q39_8\", \"w1q39_10\", \n",
    "  \"w1q39_11\", \"gp1\", \"gemployment2010\", \"gmilesaway\"]\n",
    "\n",
    "len(drop_cols_3)\n",
    "\n",
    "# Ok I started with 267 and dropped 17. Should now be 250.\n",
    "meyer.drop(columns = drop_cols_3).shape # Yep, (1507, 250)!\n",
    "meyer.drop(columns = drop_cols_3, inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "\n",
    "# Alrighty! Now we're just imputing 0s!\n",
    "# Then changing everything that's not 1 to 0.\n",
    "\n",
    "s = '''\n",
    "w1q145_3\n",
    "w1q163_3\n",
    "w1q136_3\n",
    "w1q145_9\n",
    "w1q143_3\n",
    "w1q136_10\n",
    "w1q145_10\n",
    "w1q136_9\n",
    "w1q163_10\n",
    "w1q163_9\n",
    "w1q143_9\n",
    "w1q143_4\n",
    "w1q136_6\n",
    "w1q143_10\n",
    "w1q143_5\n",
    "w1q145_4\n",
    "w1q136_5\n",
    "w1q143_8\n",
    "w1q163_5\n",
    "w1q136_4\n",
    "w1q163_6\n",
    "w1q145_6\n",
    "w1q136_1\n",
    "w1q143_7\n",
    "w1q163_1\n",
    "w1q143_2\n",
    "w1q143_1\n",
    "w1q145_5\n",
    "w1q143_6\n",
    "w1q163_2\n",
    "w1q163_4\n",
    "w1q136_8\n",
    "w1q145_8\n",
    "w1q145_1\n",
    "w1q145_7\n",
    "w1q163_7\n",
    "w1q136_2\n",
    "w1q145_2\n",
    "w1q139_3\n",
    "w1q136_7\n",
    "w1q139_9\n",
    "w1q139_10\n",
    "w1q139_5\n",
    "w1q139_4\n",
    "w1q139_8\n",
    "w1q139_6\n",
    "w1q139_2\n",
    "w1q139_1\n",
    "w1q139_7\n",
    "w1q163_8\n",
    "w1q141_3\n",
    "w1q141_9\n",
    "w1q141_10\n",
    "w1q141_8\n",
    "w1q141_4\n",
    "w1q141_1\n",
    "w1q141_5\n",
    "w1q141_2\n",
    "w1q141_7\n",
    "w1q141_6'''\n",
    "\n",
    "\n",
    "print(s.replace('\\n', '\", \"'))\n",
    "\n",
    "diy_ohe_part_1 = [\"w1q145_3\", \"w1q163_3\", \"w1q136_3\", \"w1q145_9\", \n",
    "  \"w1q143_3\", \"w1q136_10\", \"w1q145_10\", \"w1q136_9\", \"w1q163_10\", \n",
    "  \"w1q163_9\", \"w1q143_9\", \"w1q143_4\", \"w1q136_6\", \"w1q143_10\", \n",
    "  \"w1q143_5\", \"w1q145_4\", \"w1q136_5\", \"w1q143_8\", \"w1q163_5\", \n",
    "  \"w1q136_4\", \"w1q163_6\", \"w1q145_6\", \"w1q136_1\", \"w1q143_7\", \n",
    "  \"w1q163_1\", \"w1q143_2\", \"w1q143_1\", \"w1q145_5\", \"w1q143_6\", \n",
    "  \"w1q163_2\", \"w1q163_4\", \"w1q136_8\", \"w1q145_8\", \"w1q145_1\", \n",
    "  \"w1q145_7\", \"w1q163_7\", \"w1q136_2\", \"w1q145_2\", \"w1q139_3\", \n",
    "  \"w1q136_7\", \"w1q139_9\", \"w1q139_10\", \"w1q139_5\", \"w1q139_4\", \n",
    "  \"w1q139_8\", \"w1q139_6\", \"w1q139_2\", \"w1q139_1\", \"w1q139_7\", \n",
    "  \"w1q163_8\", \"w1q141_3\", \"w1q141_9\", \"w1q141_10\", \"w1q141_8\", \n",
    "  \"w1q141_4\", \"w1q141_1\", \"w1q141_5\", \"w1q141_2\", \"w1q141_7\", \n",
    "  \"w1q141_6\"]\n",
    "\n",
    "# Let me just check that these really are all 1s.\n",
    "\n",
    "for i in diy_ohe_part_1:\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "\n",
    "# Nope!  They're not! Ok let's combine them, screw it.\n",
    "del diy_ohe_part_1\n",
    "\n",
    "s = '''\n",
    "w1q64_12\n",
    "w1q64_5\n",
    "w1q74_5\n",
    "w1q74_6\n",
    "w1q74_20\n",
    "w1q64_11\n",
    "w1q64_10\n",
    "w1q74_18\n",
    "w1q74_14\n",
    "w1q74_17\n",
    "w1q171_8\n",
    "w1q30_3\n",
    "w1q64_13\n",
    "w1q64_7\n",
    "w1q30_4\n",
    "w1q171_6\n",
    "w1q171_4\n",
    "w1q64_8\n",
    "w1q74_10\n",
    "w1q74_21\n",
    "w1q64_6\n",
    "w1q74_11\n",
    "w1q171_5\n",
    "w1q64_3\n",
    "w1q64_1\n",
    "w1q171_9\n",
    "w1q30_5\n",
    "w1q171_3\n",
    "w1q74_22\n",
    "w1q64_9\n",
    "w1q171_2\n",
    "w1q171_7\n",
    "w1q74_23\n",
    "w1q64_4\n",
    "w1q64_2\n",
    "w1q30_1\n",
    "w1q171_1\n",
    "w1q30_2'''\n",
    "\n",
    "print(s.replace('\\n', '\", \"'))\n",
    "\n",
    "diy_ohe = [\"w1q145_3\", \"w1q163_3\", \"w1q136_3\", \"w1q145_9\", \n",
    "  \"w1q143_3\", \"w1q136_10\", \"w1q145_10\", \"w1q136_9\", \"w1q163_10\", \n",
    "  \"w1q163_9\", \"w1q143_9\", \"w1q143_4\", \"w1q136_6\", \"w1q143_10\", \n",
    "  \"w1q143_5\", \"w1q145_4\", \"w1q136_5\", \"w1q143_8\", \"w1q163_5\", \n",
    "  \"w1q136_4\", \"w1q163_6\", \"w1q145_6\", \"w1q136_1\", \"w1q143_7\", \n",
    "  \"w1q163_1\", \"w1q143_2\", \"w1q143_1\", \"w1q145_5\", \"w1q143_6\", \n",
    "  \"w1q163_2\", \"w1q163_4\", \"w1q136_8\", \"w1q145_8\", \"w1q145_1\", \n",
    "  \"w1q145_7\", \"w1q163_7\", \"w1q136_2\", \"w1q145_2\", \"w1q139_3\", \n",
    "  \"w1q136_7\", \"w1q139_9\", \"w1q139_10\", \"w1q139_5\", \"w1q139_4\", \n",
    "  \"w1q139_8\", \"w1q139_6\", \"w1q139_2\", \"w1q139_1\", \"w1q139_7\", \n",
    "  \"w1q163_8\", \"w1q141_3\", \"w1q141_9\", \"w1q141_10\", \"w1q141_8\", \n",
    "  \"w1q141_4\", \"w1q141_1\", \"w1q141_5\", \"w1q141_2\", \"w1q141_7\", \n",
    "  \"w1q141_6\", \"w1q64_12\", \"w1q64_5\", \"w1q74_5\", \"w1q74_6\", \n",
    "  \"w1q74_20\", \"w1q64_11\", \"w1q64_10\", \"w1q74_18\", \"w1q74_14\", \n",
    "  \"w1q74_17\", \"w1q171_8\", \"w1q30_3\", \"w1q64_13\", \"w1q64_7\", \n",
    "  \"w1q30_4\", \"w1q171_6\", \"w1q171_4\", \"w1q64_8\", \"w1q74_10\", \n",
    "  \"w1q74_21\", \"w1q64_6\", \"w1q74_11\", \"w1q171_5\", \"w1q64_3\", \n",
    "  \"w1q64_1\", \"w1q171_9\", \"w1q30_5\", \"w1q171_3\", \"w1q74_22\", \n",
    "  \"w1q64_9\", \"w1q171_2\", \"w1q171_7\", \"w1q74_23\", \"w1q64_4\", \n",
    "  \"w1q64_2\", \"w1q30_1\", \"w1q171_1\", \"w1q30_2\"]\n",
    "\n",
    "value_list = []\n",
    "for i in diy_ohe:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list += a\n",
    "value_list = list(set(value_list))\n",
    "sorted(value_list)\n",
    "\n",
    "sorted(value_list)[-1]-97 # = nan!\n",
    "\n",
    "test = pd.DataFrame(sorted(value_list))-97\n",
    "test[0].unique()\n",
    "\n",
    "# This works!\n",
    "# OK, here's the breakdown\n",
    "# Subtract 97 from everything, because that's the designated expected-NA value\n",
    "# 97-97=0, which is what I'm going to set the NAs to, so it's basically NA=NA\n",
    "# NA-97 = NA, then I just need to fill the NAs\n",
    "# some_number - 97 = some_other_number\n",
    "# So now all I have to do is convert the remaining NAs to 0, \n",
    "# and then convert everything that's not 0 to 1.\n",
    "\n",
    "# Let me check one more thing before proceeding.\n",
    "i=diy_ohe[12]\n",
    "for i in diy_ohe:\n",
    "  if len(list(meyer[i].unique()))!=3:\n",
    "    print(i)\n",
    "    print(meyer[i].value_counts(dropna = False))\n",
    "    print('='*20)\n",
    "# The only ones printing out are ones where there are only *2* values\n",
    "# the target one, and NaN.  We're good to proceed.\n",
    "\n",
    "# I think the most efficient way to do this is probably with a .map()\n",
    "\n",
    "\n",
    "diy_ohe_new = [''.join([x, '_ei']) for x in diy_ohe]\n",
    "\n",
    "meyer.shape # (1507, 250)\n",
    "len(diy_ohe) # 98\n",
    "len(diy_ohe_new) # 98\n",
    "meyer.loc[:, diy_ohe_new] = meyer.loc[:, diy_ohe].apply(lambda x: float(x)-97)\n",
    "meyer.shape # (1507, 348) huzzah!\n",
    "\n",
    "# check this again, should be no more 97s, but nothing else weird\n",
    "value_list = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list += a\n",
    "value_list = list(set(value_list))\n",
    "sorted(value_list)\n",
    "\n",
    "# Nope, it's all nans now\n",
    "\n",
    "def nan_97_fixer(x):\n",
    "  if float(x) == 97.0:\n",
    "    x = np.nan\n",
    "  else:\n",
    "    x = x\n",
    "  return x\n",
    "\n",
    "meyer.loc[:, diy_ohe_new] = meyer.loc[:, diy_ohe].map(nan_97_fixer)\n",
    "\n",
    "# check this again, should be no more 97s, but nothing else weird\n",
    "value_list = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list += a\n",
    "value_list = list(set(value_list))\n",
    "sorted(value_list)\n",
    "\n",
    "# Still all nans???????\n",
    "\n",
    "# ok BASIC test\n",
    "meyer.loc[:, diy_ohe_new] = meyer.loc[:, diy_ohe]\n",
    "\n",
    "value_list = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list += a\n",
    "value_list = list(set(value_list))\n",
    "sorted(value_list)\n",
    "\n",
    "# I just checked the original columns and they're fine.\n",
    "\n",
    "# let me test this\n",
    "pd.notna(98)\n",
    "\n",
    "# try again\n",
    "\n",
    "def nan_97_fixer(x):\n",
    "  if pd.isna(x)==True:\n",
    "    return 0\n",
    "  else: # meaning it's NOT NA\n",
    "    if float(x)==97.:\n",
    "      return 0\n",
    "    elif ((float(x)>0) & (float(x)<97)):\n",
    "      return 1\n",
    "    else:\n",
    "      return 1_000_000_000\n",
    "\n",
    "nan_97_fixer(5.) # it works.\n",
    "\n",
    "meyer.loc[:, diy_ohe_new] = meyer.loc[:, diy_ohe].map(nan_97_fixer)\n",
    "\n",
    "# check this again, should be no more 97s, but nothing else weird\n",
    "value_list = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list += a\n",
    "value_list = list(set(value_list))\n",
    "sorted(value_list)\n",
    "\n",
    "# oh ffs\n",
    "meyer.loc[:, diy_ohe_new] = meyer.loc[:, diy_ohe].replace(97, np.nan)\n",
    "\n",
    "# check this again, should be no more 97s, but nothing else weird\n",
    "value_list = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list += a\n",
    "value_list = list(set(value_list))\n",
    "sorted(value_list)\n",
    "\n",
    "\n",
    "# oh ffs\n",
    "meyer.loc[:, diy_ohe_new] = np.where(meyer.loc[:, diy_ohe]==97, 0, meyer.loc[:, diy_ohe])\n",
    "\n",
    "# check this again, should be no more 97s, but nothing else weird\n",
    "value_list_2 = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list_2 += a\n",
    "value_list_2 = list(set(value_list_2))\n",
    "sorted(value_list_2)\n",
    "\n",
    "[x for x in value_list if x not in value_list_2]  # 97\n",
    "[x for x in value_list_2 if x not in value_list]  # 0\n",
    "\n",
    "# OH THANK GOODNESS.  FINALLY!\n",
    "\n",
    "# check this again because I'm losing my mind\n",
    "meyer.shape\n",
    "\n",
    "# ok!  on we go!\n",
    "meyer[diy_ohe_new] = meyer[diy_ohe_new].fillna(0)\n",
    "\n",
    "# should be short now!\n",
    "value_list_3 = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list_3 += a\n",
    "value_list_3 = list(set(value_list_3))\n",
    "sorted(value_list_3)\n",
    "\n",
    "# AHAAHAHAAAAA I'M SO EXCITED I'M HYPERVENTILATING!!!!!!!\n",
    "meyer.loc[:, diy_ohe_new] = np.where(meyer.loc[:, diy_ohe_new]!=1, 0, meyer.loc[:, diy_ohe_new])\n",
    "\n",
    "# one more time???\n",
    "value_list_4 = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list_4 += a\n",
    "value_list_4 = list(set(value_list_4))\n",
    "sorted(value_list_4)\n",
    "\n",
    "# YAAAAAAAAAAASSSSSSSSSSSS!!!!!\n",
    "\n",
    "meyer.drop(columns = diy_ohe, inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "# ok, I actually probably should have done this before dropping those columns\n",
    "# but whatever, I can check it against the documentation\n",
    "for i in diy_ohe_new:\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('='*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 2 in full:\n",
    "2024-06-04_imputation_dealing-with-7s-and-97s-indiy-ohe-columns.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuesday, June 4, 2024\n",
    "# Imputation, Take 3\n",
    "\n",
    "# This script is a re-write of the other, earlier \n",
    "# script from today.  I tried a bunch of different\n",
    "# ways to deal with the 97s, and it took a while\n",
    "# to get one to work.\n",
    "\n",
    "meyer.shape # (1507, 267)\n",
    "\n",
    "# drop some columns\n",
    "drop_cols_3 = [\"w1q20_t_verb\", \"w1q39_7\", \"w1q39_12\", \n",
    "  \"w1q39_t_verb\", \"w1q39_6\", \"w1q39_5\", \"w1q39_2\", \"w1q39_3\", \n",
    "  \"w1q39_4\", \"w1q39_1\", \"w1q39_9\", \"w1q39_8\", \"w1q39_10\", \n",
    "  \"w1q39_11\", \"gp1\", \"gemployment2010\", \"gmilesaway\"]\n",
    "\n",
    "len(drop_cols_3)\n",
    "\n",
    "# Ok I started with 267 and dropped 17. Should now be 250.\n",
    "meyer.drop(columns = drop_cols_3).shape # Yep, (1507, 250)!\n",
    "meyer.drop(columns = drop_cols_3, inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "diy_ohe = [\"w1q145_3\", \"w1q163_3\", \"w1q136_3\", \"w1q145_9\", \n",
    "  \"w1q143_3\", \"w1q136_10\", \"w1q145_10\", \"w1q136_9\", \"w1q163_10\", \n",
    "  \"w1q163_9\", \"w1q143_9\", \"w1q143_4\", \"w1q136_6\", \"w1q143_10\", \n",
    "  \"w1q143_5\", \"w1q145_4\", \"w1q136_5\", \"w1q143_8\", \"w1q163_5\", \n",
    "  \"w1q136_4\", \"w1q163_6\", \"w1q145_6\", \"w1q136_1\", \"w1q143_7\", \n",
    "  \"w1q163_1\", \"w1q143_2\", \"w1q143_1\", \"w1q145_5\", \"w1q143_6\", \n",
    "  \"w1q163_2\", \"w1q163_4\", \"w1q136_8\", \"w1q145_8\", \"w1q145_1\", \n",
    "  \"w1q145_7\", \"w1q163_7\", \"w1q136_2\", \"w1q145_2\", \"w1q139_3\", \n",
    "  \"w1q136_7\", \"w1q139_9\", \"w1q139_10\", \"w1q139_5\", \"w1q139_4\", \n",
    "  \"w1q139_8\", \"w1q139_6\", \"w1q139_2\", \"w1q139_1\", \"w1q139_7\", \n",
    "  \"w1q163_8\", \"w1q141_3\", \"w1q141_9\", \"w1q141_10\", \"w1q141_8\", \n",
    "  \"w1q141_4\", \"w1q141_1\", \"w1q141_5\", \"w1q141_2\", \"w1q141_7\", \n",
    "  \"w1q141_6\", \"w1q64_12\", \"w1q64_5\", \"w1q74_5\", \"w1q74_6\", \n",
    "  \"w1q74_20\", \"w1q64_11\", \"w1q64_10\", \"w1q74_18\", \"w1q74_14\", \n",
    "  \"w1q74_17\", \"w1q171_8\", \"w1q30_3\", \"w1q64_13\", \"w1q64_7\", \n",
    "  \"w1q30_4\", \"w1q171_6\", \"w1q171_4\", \"w1q64_8\", \"w1q74_10\", \n",
    "  \"w1q74_21\", \"w1q64_6\", \"w1q74_11\", \"w1q171_5\", \"w1q64_3\", \n",
    "  \"w1q64_1\", \"w1q171_9\", \"w1q30_5\", \"w1q171_3\", \"w1q74_22\", \n",
    "  \"w1q64_9\", \"w1q171_2\", \"w1q171_7\", \"w1q74_23\", \"w1q64_4\", \n",
    "  \"w1q64_2\", \"w1q30_1\", \"w1q171_1\", \"w1q30_2\"]\n",
    "\n",
    "value_list = []\n",
    "for i in diy_ohe:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list += a\n",
    "value_list = list(set(value_list))\n",
    "sorted(value_list)\n",
    "\n",
    "# create some new column names\n",
    "diy_ohe_new = [''.join([x, '_ei']) for x in diy_ohe]\n",
    "\n",
    "# let's see if this replaces the 97s right\n",
    "meyer.loc[:, diy_ohe_new] = np.where(meyer.loc[:, diy_ohe]==97, 0, meyer.loc[:, diy_ohe])\n",
    "\n",
    "# check this\n",
    "for i, j in list(zip(diy_ohe, diy_ohe_new))[50:]:\n",
    "  print(i)\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('-'*20)\n",
    "  print(j)\n",
    "  meyer[j].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "  print('')\n",
    "  print('='*20)\n",
    "\n",
    "# I'm looking for valid 7s.\n",
    "# It seems like 7 was also used as an NA code\n",
    "# because why be consistent lolololol!!!!!!!!\n",
    "\n",
    "nat_7s = ['w1q143_7', 'w1q145_7', 'w1q163_7', 'w1q136_7', \n",
    "  'w1q139_7', 'w1q141_7', 'w1q64_7', 'w1q171_7']\n",
    "  \n",
    "nat_7s == [c for c in diy_ohe if c.split('_')[-1]=='7']\n",
    "# True.\n",
    "\n",
    "nat_7s_ei = [''.join([x, '_ei']) for x in nat_7s]\n",
    "\n",
    "# let's turn those 7s into 1s, and then the rest into 0s\n",
    "meyer.loc[:, nat_7s_ei] = np.where(meyer.loc[:, nat_7s]==7, 1, meyer.loc[:, nat_7s])\n",
    "\n",
    "# check this\n",
    "for i in nat_7s_ei:\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "\n",
    "# check one against the documentation\n",
    "meyer['w1q141_7'].value_counts(dropna = False)\n",
    "\n",
    "# check this again, should be no more 97s, but nothing else weird\n",
    "remaining_7s = []\n",
    "value_list_2 = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  if 7 in a:\n",
    "    remaining_7s.append(i)\n",
    "  value_list_2 += a\n",
    "value_list_2 = list(set(value_list_2))\n",
    "sorted(value_list_2)\n",
    "\n",
    "remaining_7s_qs = [x.split('_')[0] for x in remaining_7s]\n",
    "sorted(set(remaining_7s_qs))\n",
    "# I went through VERY CAREFULLY and confirmed that all the \n",
    "# remaining 7s are \"planned missings.\"\n",
    "\n",
    "# let's see if this replaces the 97s right\n",
    "meyer.loc[:, diy_ohe_new] = np.where(meyer.loc[:, diy_ohe_new]==7, 0, meyer.loc[:, diy_ohe_new])\n",
    "\n",
    "# check this\n",
    "for i, j in list(zip(diy_ohe, diy_ohe_new))[:50]:\n",
    "  print(i)\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('-'*20)\n",
    "  print(j)\n",
    "  meyer[j].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "  print('')\n",
    "  print('='*20)\n",
    "\n",
    "meyer[diy_ohe_new] = meyer[diy_ohe_new].fillna(0)\n",
    "\n",
    "# check again!\n",
    "for i, j in list(zip(diy_ohe, diy_ohe_new))[:50]:\n",
    "  print(i)\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('-'*20)\n",
    "  print(j)\n",
    "  meyer[j].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "  print('')\n",
    "  print('='*20)\n",
    "\n",
    "# wait, some of them still have 97s??\n",
    "\n",
    "# let's see if this replaces the 97s right\n",
    "meyer.loc[:, diy_ohe_new] = np.where(meyer.loc[:, diy_ohe_new]==97.0, 0, meyer.loc[:, diy_ohe_new])\n",
    "\n",
    "for i, j in list(zip(diy_ohe, diy_ohe_new))[:50]:\n",
    "  print(i)\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('-'*20)\n",
    "  print(j)\n",
    "  meyer[j].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "  print('')\n",
    "  print('='*20)\n",
    "  \n",
    "# this again too\n",
    "remaining_7s_2 = []\n",
    "remaining_97s = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  if 7 in a:\n",
    "    remaining_7s_2.append(i)\n",
    "  if 97 in a:\n",
    "    remaining_97s.append(i)\n",
    "sorted(set(remaining_7s_2))\n",
    "sorted(set(remaining_97s))\n",
    "\n",
    "# check AGAIN\n",
    "value_list_x = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list_x += a\n",
    "value_list_x = list(set(value_list_x))\n",
    "sorted(value_list_x)\n",
    "\n",
    "# make a backup\n",
    "os.getcwd()\n",
    "meyer.to_csv('meyer_backup_diy_ohe_no_97s_7s.csv', index = False)\n",
    "\n",
    "\n",
    "# reduce to 0s and 1s\n",
    "meyer.loc[:, diy_ohe_new] = np.where(meyer.loc[:, diy_ohe_new]!=0, 1, meyer.loc[:, diy_ohe_new])\n",
    "\n",
    "for i, j in list(zip(diy_ohe, diy_ohe_new))[:50]:\n",
    "  print(i)\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('-'*20)\n",
    "  print(j)\n",
    "  meyer[j].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "  print('')\n",
    "  print('='*20)\n",
    "\n",
    "# no NAs?\n",
    "meyer[diy_ohe_new].isna().sum().sum()\n",
    "# no NAs!\n",
    "\n",
    "# drop the old versions\n",
    "meyer.drop(columns = diy_ohe, inplace = True)\n",
    "\n",
    "# check the dtypes\n",
    "pd.set_option('display.max_rows', None)\n",
    "meyer.dtypes\n",
    "\n",
    "# should be VERY short now!\n",
    "value_list_3 = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list_3 += a\n",
    "value_list_3 = list(set(value_list_3))\n",
    "sorted(value_list_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 3 in full:\n",
    "2024-06-04_imputation_misc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuesday, June 4, 2024\n",
    "# More Imputation!\n",
    "\n",
    "# w1q64_t_verb\n",
    "meyer['w1q64_t_verb'].value_counts(dropna = False)\n",
    "meyer.shape # 250 columns - gonna add one then delete the original\n",
    "# meyer['w1q64_t_num'] = meyer['w1q64_t_verb'].fillna('0')\n",
    "# I got a weird warning here ^, so for future runs V\n",
    "meyer[['w1q64_t_num']] = meyer[['w1q64_t_verb']].fillna('0')\n",
    "# for right now V\n",
    "meyer_c = meyer.copy(deep = True)\n",
    "meyer = meyer_c.copy(deep = True)\n",
    "meyer.shape\n",
    "# thanks to this SO article for advice\n",
    "# https://stackoverflow.com/questions/68292862/performancewarning-dataframe-is-highly-fragmented-this-is-usually-the-result-o\n",
    "\n",
    "# reduce to 0s and 1s\n",
    "meyer.loc[:, 'w1q64_t_num'] = np.where(meyer.loc[:, 'w1q64_t_num']!='0', 1, 0)\n",
    "\n",
    "# check\n",
    "meyer['w1q64_t_num'].value_counts(dropna = False)\n",
    "meyer['w1q64_t_verb'].value_counts(dropna = False)\n",
    "\n",
    "# drop the original\n",
    "meyer.drop(columns = ['w1q64_t_verb'], inplace = True)\n",
    "\n",
    "\n",
    "# gmilesaway2\n",
    "meyer['gmilesaway2'].value_counts(dropna = False)\n",
    "\n",
    "meyer[['gmilesaway2_ei']] = meyer[['gmilesaway2']].fillna(0)\n",
    "meyer['gmilesaway2'].value_counts(dropna = False)\n",
    "meyer['gmilesaway2_ei'].value_counts(dropna = False)\n",
    "\n",
    "# reverse code so 1=close\n",
    "meyer[['gmilesaway2_ei_r']] = 1-meyer[['gmilesaway2_ei']]\n",
    "meyer['gmilesaway2'].value_counts(dropna = False)\n",
    "meyer['gmilesaway2_ei'].value_counts(dropna = False)\n",
    "meyer['gmilesaway2_ei_r'].value_counts(dropna = False)\n",
    "\n",
    "# drop\n",
    "meyer.drop(columns = ['gmilesaway2', 'gmilesaway2_ei'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "\n",
    "# w1q123d & w1q123c\n",
    "# These guys had some missing values, but also had an option (5)\n",
    "# for \"don't know/doesn't apply.\"  I don't know what the truth\n",
    "# is for these missing values, so I'm recoding them to \"don't know.\"\n",
    "\n",
    "# What to do with the 5s \n",
    "# will be a question for the next round of cleaning.\n",
    "\n",
    "meyer[['w1q123d_ei', 'w1q123c_ei']] = meyer[['w1q123d', 'w1q123c']].fillna(5)\n",
    "meyer[['w1q123d_ei', 'w1q123c_ei', 'w1q123d', 'w1q123c']]\n",
    "\n",
    "meyer.drop(columns = ['w1q123d', 'w1q123c'], inplace = True)\n",
    "\n",
    "\n",
    "# w1q179\n",
    "# impute as \"nothing in particular\", seems logical\n",
    "meyer[['w1q179_ei']] = meyer[['w1q179']].fillna(13)\n",
    "meyer[['w1q179', 'w1q179_ei']]\n",
    "\n",
    "meyer.drop(columns = 'w1q179', inplace = True)\n",
    "\n",
    "\n",
    "# all right!  real imputer time!!\n",
    "\n",
    "# I'm going to go ahead and do this on the full dataset, \n",
    "# because screw it.\n",
    "# If I have time, I can go back and re-run it with\n",
    "# a train test split.  But for now I'm just going to do it\n",
    "# all at once, with the justification that for an \n",
    "# inferential model, the TTS isn't that important.\n",
    "# This data was gathered at a specific moment in history,\n",
    "# and it covers some sensitive topics.  There is no \n",
    "# reason to design this process to accommodate for novel\n",
    "# data extending into the future, because there is no\n",
    "# reason to assume that these values wouldn't change\n",
    "# over time.  (Which does kind of invalidate everything\n",
    "# I'm doing on this 2016 dataset but SHHHHHHHHH.)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "si_med = SimpleImputer(strategy = 'median')\n",
    "si_mode = SimpleImputer(strategy = 'most_frequent')\n",
    "\n",
    "s_median = '''\n",
    "w1q50\n",
    "w1q48\n",
    "w1q51\n",
    "w1q112\n",
    "w1q49\n",
    "w1q47\n",
    "w1q46\n",
    "w1q45\n",
    "w1q146c\n",
    "w1q162\n",
    "w1q181\n",
    "w1q146f\n",
    "w1q146g\n",
    "w1q146e'''\n",
    "\n",
    "s_mode = '''\n",
    "w1q175\n",
    "w1q72\n",
    "w1q142d\n",
    "w1q65\n",
    "w1q52\n",
    "w1q142i\n",
    "w1q142g\n",
    "w1q180\n",
    "w1q69\n",
    "w1q135d\n",
    "w1q142h\n",
    "w1q135b\n",
    "w1q135c\n",
    "w1q135e\n",
    "w1q142b\n",
    "w1q142e\n",
    "w1q142f\n",
    "w1q142j\n",
    "w1q142a\n",
    "w1q142k\n",
    "w1q19c\n",
    "w1q19d\n",
    "w1q19b\n",
    "w1q19a\n",
    "w1q75\n",
    "w1q78\n",
    "w1q79\n",
    "w1q76'''\n",
    "\n",
    "print(s_median.replace('\\n', '\", \"'))\n",
    "print(s_mode.replace('\\n', '\", \"'))\n",
    "\n",
    "s_median = [\"w1q50\", \"w1q48\", \"w1q51\", \"w1q112\", \n",
    "  \"w1q49\", \"w1q47\", \"w1q46\", \"w1q45\", \"w1q146c\", \n",
    "  \"w1q162\", \"w1q181\", \"w1q146f\", \"w1q146g\", \"w1q146e\"]\n",
    "  \n",
    "s_mode = [\"w1q175\", \"w1q72\", \"w1q142d\", \"w1q65\", \"w1q52\", \n",
    "  \"w1q142i\", \"w1q142g\", \"w1q180\", \"w1q69\", \"w1q135d\", \n",
    "  \"w1q142h\", \"w1q135b\", \"w1q135c\", \"w1q135e\", \"w1q142b\", \n",
    "  \"w1q142e\", \"w1q142f\", \"w1q142j\", \"w1q142a\", \"w1q142k\", \n",
    "  \"w1q19c\", \"w1q19d\", \"w1q19b\", \"w1q19a\", \"w1q75\", \n",
    "  \"w1q78\", \"w1q79\", \"w1q76\"]\n",
    "\n",
    "s_median_new = [''.join([x, '_ei']) for x in s_median]\n",
    "s_mode_new = [''.join([x, '_ei']) for x in s_mode]\n",
    "\n",
    "# # Safety first\n",
    "# meyer.to_csv('meyer_much_imputation_done_but_not_si_yet.csv', index = False)\n",
    "# \n",
    "# # I think the cleanest way to do this without it breaking everything is to\n",
    "# # break the dataframe apart, use the 2 SIs separately, then re-merge.\n",
    "# # Let me check that the studyID will be suitable for merging on.\n",
    "# \n",
    "# meyer.shape # 1507 rows (250 columns)\n",
    "# meyer['studyid'].nunique() # 1507 unique values\n",
    "# \n",
    "# # split the df up for imputing\n",
    "# meyer_median = meyer[s_median+['studyid']]\n",
    "# meyer_mode = meyer[s_mode+['studyid']]\n",
    "# \n",
    "# # new column names\n",
    "# s_median_new.append('studyid')\n",
    "# s_mode_new.append('studyid')\n",
    "# meyer_median.columns = s_median_new\n",
    "# meyer_mode.columns = s_mode_new\n",
    "# \n",
    "# # do the imputation!\n",
    "# meyer_median = pd.DataFrame(\n",
    "#   si_med.fit_transform(meyer_median), columns = si_med.get_feature_names_out())\n",
    "# \n",
    "# meyer_mode = pd.DataFrame(\n",
    "#   si_mode.fit_transform(meyer_mode), columns = si_mode.get_feature_names_out())\n",
    "# \n",
    "# # is it really that easy?\n",
    "# meyer_median.isna().sum().sum()\n",
    "# meyer_mode.isna().sum().sum()\n",
    "# \n",
    "# mme = meyer_median.describe()\n",
    "# mmo = meyer_mode.describe()\n",
    "# \n",
    "# # No!  No, it is not!!\n",
    "# # The stupid 97s, 98s, and 99s are at it again!\n",
    "\n",
    "s = '''\n",
    "w1q119\n",
    "w1q109\n",
    "w1q105\n",
    "w1q113\n",
    "w1q101'''\n",
    "\n",
    "print(s.replace('\\n', '\", \"'))\n",
    "\n",
    "s = [\"w1q119\", \"w1q109\", \"w1q105\", \"w1q113\", \"w1q101\"]\n",
    "\n",
    "s_ei = [''.join([x, '_ei']) for x in s]\n",
    "\n",
    "meyer[s_ei] = meyer[s].fillna(0)\n",
    "\n",
    "for i, j in zip(s, s_ei):\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('-'*20)\n",
    "  meyer[j].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "  print('')\n",
    "\n",
    "meyer.drop(columns = s, inplace = True)\n",
    "\n",
    "# RIGHT NOW THESE ARE 0S\n",
    "# ONLY 1-3 IS DEFINED IN THE SCALE \n",
    "# (these are the suicide and nssh Qs)\n",
    "# TOMORROW, plot it, and see how the 0 column\n",
    "# compares to the others\n",
    "# and reassign accordingly\n",
    "\n",
    "\n",
    "# w1q32\n",
    "# right or wrong these guys were TREATED as \"no\" in the survey\n",
    "# so I'm going to code them that way.\n",
    "\n",
    "meyer['w1q32'].value_counts(dropna = False)\n",
    "meyer['w1q32'].fillna(2).value_counts(dropna = False)\n",
    "\n",
    "meyer[['w1q32_ei']] = meyer[['w1q32']].fillna(2)\n",
    "\n",
    "meyer[['w1q32_ei']].value_counts(dropna = False)\n",
    "\n",
    "meyer.drop(columns = ['w1q32_ei'], inplace = True)\n",
    "\n",
    "meyer.shape\n",
    "list(meyer.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 4 in full:\n",
    "2024-06-04_simple_imputer_stuff.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written Tuesday, June 4, 2024\n",
    "# FOR Wednesday, June 5, 2024\n",
    "\n",
    "# Much of this was copied from impt_misc\n",
    "\n",
    "# The simple imputer stuff\n",
    "\n",
    "s_median = [\"w1q50\", \"w1q48\", \"w1q51\", \"w1q112\", \n",
    "  \"w1q49\", \"w1q47\", \"w1q46\", \"w1q45\", \"w1q146c\", \n",
    "  \"w1q162\", \"w1q181\", \"w1q146f\", \"w1q146g\", \"w1q146e\"]\n",
    "  \n",
    "s_mode = [\"w1q175\", \"w1q72\", \"w1q142d\", \"w1q65\", \"w1q52\", \n",
    "  \"w1q142i\", \"w1q142g\", \"w1q180\", \"w1q69\", \"w1q135d\", \n",
    "  \"w1q142h\", \"w1q135b\", \"w1q135c\", \"w1q135e\", \"w1q142b\", \n",
    "  \"w1q142e\", \"w1q142f\", \"w1q142j\", \"w1q142a\", \"w1q142k\", \n",
    "  \"w1q19c\", \"w1q19d\", \"w1q19b\", \"w1q19a\", \"w1q75\", \n",
    "  \"w1q78\", \"w1q79\", \"w1q76\"]\n",
    "\n",
    "s_median_new = [''.join([x, '_ei']) for x in s_median]\n",
    "s_mode_new = [''.join([x, '_ei']) for x in s_mode]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Right here I need to figure out how to deal with the 97s-99s.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Safety first\n",
    "# meyer.to_csv('meyer_much_imputation_done_but_not_si_yet.csv', index = False)\n",
    "\n",
    "# I think the cleanest way to do this without it breaking everything is to\n",
    "# break the dataframe apart, use the 2 SIs separately, then re-merge.\n",
    "# Let me check that the studyID will be suitable for merging on.\n",
    "\n",
    "meyer.shape # 1507 rows (250 columns)\n",
    "meyer['studyid'].nunique() # 1507 unique values\n",
    "\n",
    "# split the df up for imputing\n",
    "meyer_median = meyer[s_median+['studyid']]\n",
    "meyer_mode = meyer[s_mode+['studyid']]\n",
    "\n",
    "# new column names\n",
    "s_median_new.append('studyid')\n",
    "s_mode_new.append('studyid')\n",
    "meyer_median.columns = s_median_new\n",
    "meyer_mode.columns = s_mode_new\n",
    "\n",
    "# do the imputation!\n",
    "meyer_median = pd.DataFrame(\n",
    "  si_med.fit_transform(meyer_median), columns = si_med.get_feature_names_out())\n",
    "\n",
    "meyer_mode = pd.DataFrame(\n",
    "  si_mode.fit_transform(meyer_mode), columns = si_mode.get_feature_names_out())\n",
    "\n",
    "# is it really that easy?\n",
    "meyer_median.isna().sum().sum()\n",
    "meyer_mode.isna().sum().sum()\n",
    "\n",
    "mme = meyer_median.describe()\n",
    "mmo = meyer_mode.describe()\n",
    "\n",
    "# No!  No, it is not!!\n",
    "# The stupid 97s, 98s, and 99s are at it again!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSVs are in the main project folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wednesday, June 5, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "I finally finished imputing my null values!  \n",
    "\n",
    "I think when this is all over, I should reload a clean, fresh copy of the dataframe and see\n",
    "- how many NAs there are per ROW -- I wonder if there are a handful of people who skipped a lot of questions and I've ended up imputing basically their whole row\n",
    "- how many cells I've altered.  it all seems so reasonable when I'm going one column at a time, but when I step back and watch every single column turn into an _ei column, it makes me wonder if I'm destroying my dataset.  If the final tally is staggering, then I should probably go with Hank's suggestion of only picking an handful of columns and ignoring the rest.\n",
    "\n",
    "In any case, today I finished doing the imputation as laid out in imputation_plan.xlsx.  Or, mostly.  I changed my mind on a few things as I went, and discovered things along the way.  Most memorably, some column that I meant to impute with the median populated value ended up just being 0s, because once I converted the 97s to 0s, that was the median.  I'm sure there's a way to tell SI to impute only from values in a range, but 0 was faster.  I also cleaned up a bunch more of those 97s et al., and did some subset investigations.  On some poverty and employment measures there were probably more columns that I could have used to predict the missing values, but using measures of central tendency or just the columns that came most readily to mind was fastest, and will have to do for this iteration.\n",
    "\n",
    "Once all the imputation of truly *missing* values was done, I looped back around to look for any of those 7/97/98/99 values that *should* be NAs but aren't.  I did this by looking through every W1 listing in the 831 page documentation, and making a list of which variables have defined values like that, and what they are.  This list is called `problem_children`, and is a list of tuples with structure `(column_name, [list of relevant value(s)])`.\n",
    "\n",
    "I heard back from the people at ICPSR, and they said no to posting my dataset on github.  They are fine with me posting a link though, and that should suffice.\n",
    "\n",
    "**I created new CSVs at various points throughout my process, so starting tomorrow I can load from one of those directly.  The most up to date one is `2024-06-05_all-imputation-done.csv`.**\n",
    "\n",
    "\n",
    "files created: \n",
    "- 2024-06-05_finishing-imputation.py\n",
    "- 2024-06-05_making-a-list-of-potential-7s-98s-etc-to-check.py\n",
    "- MAJOR modifications: 2024-06-04_simple_imputer_stuff.py\n",
    "- MINOR modifications:\n",
    "modified:   2024-06-04_imputation_misc.py  (this is the only one that warrants re-posting)\n",
    "modified:   ../../01_notebooks/notes_plus_all_code.ipynb\n",
    "modified:   ../01_notes/imputation_plan.xlsx\n",
    "modified:   2024-05-31_continuing_messing_with_meyer_columns.py\n",
    "modified:   2024-06-04_imputation_dealing-with-7s-and-97s-indiy-ohe-columns.py\n",
    "\n",
    "- all of the following CSVs:\n",
    "2024-06-05_all-imputation-done.csv\n",
    "2024-06-05_all-imputation-done_reordered.csv\n",
    "2024-06-05_all-imputation-except-poverty-done.csv\n",
    "2024-06-05_meyer_backup_diy_ohe_no_97s_7s.csv\n",
    "2024-06-05_meyer_much_imputation_done_but_not_si_yet.csv\n",
    "2024-06-05_most-imputation-done_midway-through-nativity.csv\n",
    "2024-06-05_pre-si-remerging.csv\n",
    "2024-06-05_si-imputation-done_remerged.csv\n",
    "2024-06-05_values-imputed-with-median.csv\n",
    "2024-06-05_values-imputed-with-mode.csv\n",
    "mid_2024-06-05_remaining_nas.csv  (the df under this csv was created in a script, but the .to_csv itself was only in the console)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 1 in full:\n",
    "2024-06-04_imputation_misc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuesday, June 4, 2024\n",
    "# More Imputation!\n",
    "\n",
    "# w1q64_t_verb\n",
    "meyer['w1q64_t_verb'].value_counts(dropna = False)\n",
    "meyer.shape # 250 columns - gonna add one then delete the original\n",
    "# meyer['w1q64_t_num'] = meyer['w1q64_t_verb'].fillna('0')\n",
    "# I got a weird warning here ^, so for future runs V\n",
    "meyer[['w1q64_t_num']] = meyer[['w1q64_t_verb']].fillna('0')\n",
    "# for right now V\n",
    "meyer_c = meyer.copy(deep = True)\n",
    "meyer = meyer_c.copy(deep = True)\n",
    "meyer.shape\n",
    "# thanks to this SO article for advice\n",
    "# https://stackoverflow.com/questions/68292862/performancewarning-dataframe-is-highly-fragmented-this-is-usually-the-result-o\n",
    "\n",
    "# reduce to 0s and 1s\n",
    "meyer.loc[:, 'w1q64_t_num'] = np.where(meyer.loc[:, 'w1q64_t_num']!='0', 1, 0)\n",
    "\n",
    "# check\n",
    "meyer['w1q64_t_num'].value_counts(dropna = False)\n",
    "meyer['w1q64_t_verb'].value_counts(dropna = False)\n",
    "\n",
    "# drop the original\n",
    "meyer.drop(columns = ['w1q64_t_verb'], inplace = True)\n",
    "\n",
    "\n",
    "# gmilesaway2\n",
    "meyer['gmilesaway2'].value_counts(dropna = False)\n",
    "\n",
    "meyer[['gmilesaway2_ei']] = meyer[['gmilesaway2']].fillna(0)\n",
    "meyer['gmilesaway2'].value_counts(dropna = False)\n",
    "meyer['gmilesaway2_ei'].value_counts(dropna = False)\n",
    "\n",
    "# reverse code so 1=close\n",
    "meyer[['gmilesaway2_ei_r']] = 1-meyer[['gmilesaway2_ei']]\n",
    "meyer['gmilesaway2'].value_counts(dropna = False)\n",
    "meyer['gmilesaway2_ei'].value_counts(dropna = False)\n",
    "meyer['gmilesaway2_ei_r'].value_counts(dropna = False)\n",
    "\n",
    "# drop\n",
    "meyer.drop(columns = ['gmilesaway2', 'gmilesaway2_ei'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "\n",
    "# w1q123d & w1q123c\n",
    "# These guys had some missing values, but also had an option (5)\n",
    "# for \"don't know/doesn't apply.\"  I don't know what the truth\n",
    "# is for these missing values, so I'm recoding them to \"don't know.\"\n",
    "\n",
    "# What to do with the 5s \n",
    "# will be a question for the next round of cleaning.\n",
    "\n",
    "meyer[['w1q123d_ei', 'w1q123c_ei']] = meyer[['w1q123d', 'w1q123c']].fillna(5)\n",
    "meyer[['w1q123d_ei', 'w1q123c_ei', 'w1q123d', 'w1q123c']]\n",
    "\n",
    "meyer.drop(columns = ['w1q123d', 'w1q123c'], inplace = True)\n",
    "\n",
    "\n",
    "# w1q179\n",
    "# impute as \"nothing in particular\", seems logical\n",
    "meyer[['w1q179_ei']] = meyer[['w1q179']].fillna(13)\n",
    "meyer[['w1q179', 'w1q179_ei']]\n",
    "\n",
    "meyer.drop(columns = 'w1q179', inplace = True)\n",
    "\n",
    "\n",
    "# all right!  real imputer time!!\n",
    "\n",
    "# I'm going to go ahead and do this on the full dataset, \n",
    "# because screw it.\n",
    "# If I have time, I can go back and re-run it with\n",
    "# a train test split.  But for now I'm just going to do it\n",
    "# all at once, with the justification that for an \n",
    "# inferential model, the TTS isn't that important.\n",
    "# This data was gathered at a specific moment in history,\n",
    "# and it covers some sensitive topics.  There is no \n",
    "# reason to design this process to accommodate for novel\n",
    "# data extending into the future, because there is no\n",
    "# reason to assume that these values wouldn't change\n",
    "# over time.  (Which does kind of invalidate everything\n",
    "# I'm doing on this 2016 dataset but SHHHHHHHHH.)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "si_med = SimpleImputer(strategy = 'median')\n",
    "si_mode = SimpleImputer(strategy = 'most_frequent')\n",
    "\n",
    "s_median = '''\n",
    "w1q50\n",
    "w1q48\n",
    "w1q51\n",
    "w1q112\n",
    "w1q49\n",
    "w1q47\n",
    "w1q46\n",
    "w1q45\n",
    "w1q146c\n",
    "w1q162\n",
    "w1q181\n",
    "w1q146f\n",
    "w1q146g\n",
    "w1q146e'''\n",
    "\n",
    "s_mode = '''\n",
    "w1q175\n",
    "w1q72\n",
    "w1q142d\n",
    "w1q65\n",
    "w1q52\n",
    "w1q142i\n",
    "w1q142g\n",
    "w1q180\n",
    "w1q69\n",
    "w1q135d\n",
    "w1q142h\n",
    "w1q135b\n",
    "w1q135c\n",
    "w1q135e\n",
    "w1q142b\n",
    "w1q142e\n",
    "w1q142f\n",
    "w1q142j\n",
    "w1q142a\n",
    "w1q142k\n",
    "w1q19c\n",
    "w1q19d\n",
    "w1q19b\n",
    "w1q19a\n",
    "w1q75\n",
    "w1q78\n",
    "w1q79\n",
    "w1q76'''\n",
    "\n",
    "print(s_median.replace('\\n', '\", \"'))\n",
    "print(s_mode.replace('\\n', '\", \"'))\n",
    "\n",
    "s_median = [\"w1q50\", \"w1q48\", \"w1q51\", \"w1q112\", \n",
    "  \"w1q49\", \"w1q47\", \"w1q46\", \"w1q45\", \"w1q146c\", \n",
    "  \"w1q162\", \"w1q181\", \"w1q146f\", \"w1q146g\", \"w1q146e\"]\n",
    "  \n",
    "s_mode = [\"w1q175\", \"w1q72\", \"w1q142d\", \"w1q65\", \"w1q52\", \n",
    "  \"w1q142i\", \"w1q142g\", \"w1q180\", \"w1q69\", \"w1q135d\", \n",
    "  \"w1q142h\", \"w1q135b\", \"w1q135c\", \"w1q135e\", \"w1q142b\", \n",
    "  \"w1q142e\", \"w1q142f\", \"w1q142j\", \"w1q142a\", \"w1q142k\", \n",
    "  \"w1q19c\", \"w1q19d\", \"w1q19b\", \"w1q19a\", \"w1q75\", \n",
    "  \"w1q78\", \"w1q79\", \"w1q76\"]\n",
    "\n",
    "s_median_new = [''.join([x, '_ei']) for x in s_median]\n",
    "s_mode_new = [''.join([x, '_ei']) for x in s_mode]\n",
    "\n",
    "# # Safety first\n",
    "# meyer.to_csv('meyer_much_imputation_done_but_not_si_yet.csv', index = False)\n",
    "# \n",
    "# # I think the cleanest way to do this without it breaking everything is to\n",
    "# # break the dataframe apart, use the 2 SIs separately, then re-merge.\n",
    "# # Let me check that the studyID will be suitable for merging on.\n",
    "# \n",
    "# meyer.shape # 1507 rows (250 columns)\n",
    "# meyer['studyid'].nunique() # 1507 unique values\n",
    "# \n",
    "# # split the df up for imputing\n",
    "# meyer_median = meyer[s_median+['studyid']]\n",
    "# meyer_mode = meyer[s_mode+['studyid']]\n",
    "# \n",
    "# # new column names\n",
    "# s_median_new.append('studyid')\n",
    "# s_mode_new.append('studyid')\n",
    "# meyer_median.columns = s_median_new\n",
    "# meyer_mode.columns = s_mode_new\n",
    "# \n",
    "# # do the imputation!\n",
    "# meyer_median = pd.DataFrame(\n",
    "#   si_med.fit_transform(meyer_median), columns = si_med.get_feature_names_out())\n",
    "# \n",
    "# meyer_mode = pd.DataFrame(\n",
    "#   si_mode.fit_transform(meyer_mode), columns = si_mode.get_feature_names_out())\n",
    "# \n",
    "# # is it really that easy?\n",
    "# meyer_median.isna().sum().sum()\n",
    "# meyer_mode.isna().sum().sum()\n",
    "# \n",
    "# mme = meyer_median.describe()\n",
    "# mmo = meyer_mode.describe()\n",
    "# \n",
    "# # No!  No, it is not!!\n",
    "# # The stupid 97s, 98s, and 99s are at it again!\n",
    "\n",
    "\n",
    "# Suicide and NSSH Qs\n",
    "\n",
    "s = '''\n",
    "w1q119\n",
    "w1q109\n",
    "w1q105\n",
    "w1q113\n",
    "w1q101'''\n",
    "\n",
    "print(s.replace('\\n', '\", \"'))\n",
    "\n",
    "s = [\"w1q119\", \"w1q109\", \"w1q105\", \"w1q113\", \"w1q101\"]\n",
    "\n",
    "s_ei = [''.join([x, '_ei']) for x in s]\n",
    "\n",
    "meyer[s_ei] = meyer[s].fillna(0)\n",
    "\n",
    "for i, j in zip(s, s_ei):\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('-'*20)\n",
    "  meyer[j].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "  print('')\n",
    "\n",
    "meyer.drop(columns = s, inplace = True)\n",
    "\n",
    "# RIGHT NOW THESE ARE 0S\n",
    "# ONLY 1-3 IS DEFINED IN THE SCALE \n",
    "# (these are the suicide and nssh Qs)\n",
    "# TOMORROW, plot it, and see how the 0 column\n",
    "# compares to the others\n",
    "# and reassign accordingly\n",
    "\n",
    "\n",
    "# w1q32\n",
    "# right or wrong these guys were TREATED as \"no\" in the survey\n",
    "# so I'm going to code them that way.\n",
    "\n",
    "meyer['w1q32'].value_counts(dropna = False)\n",
    "meyer['w1q32'].fillna(2).value_counts(dropna = False)\n",
    "\n",
    "meyer[['w1q32_ei']] = meyer[['w1q32']].fillna(2)\n",
    "\n",
    "meyer[['w1q32_ei']].value_counts(dropna = False)\n",
    "\n",
    "meyer.drop(columns = ['w1q32'], inplace = True)\n",
    "\n",
    "meyer.shape\n",
    "list(meyer.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 2 in full:\n",
    "2024-06-04_simple_imputer_stuff.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written Tuesday, June 4, 2024\n",
    "# FOR Wednesday, June 5, 2024\n",
    "\n",
    "# from the backup\n",
    "os.getcwd()\n",
    "meyer = pd.read_csv('2024-06-05_meyer_much_imputation_done_but_not_si_yet.csv',\n",
    "  sep = ',', low_memory=False, na_values = ' ')\n",
    "  \n",
    "# THERE WAS MORE STUFF ON THAT SCRIPT AFTER SAVING THE CSV\n",
    "\n",
    "# Much of this was copied from impt_misc\n",
    "\n",
    "# The simple imputer stuff\n",
    "\n",
    "s_median = [\"w1q50\", \"w1q48\", \"w1q51\", \"w1q112\", \n",
    "  \"w1q49\", \"w1q47\", \"w1q46\", \"w1q45\", \"w1q146c\", \n",
    "  \"w1q162\", \"w1q181\", \"w1q146f\", \"w1q146g\", \"w1q146e\", # original ^ added V\n",
    "  \"w1q103\", \"w1q104\", \"w1q106\", \"w1q107\", \"w1q108\", \n",
    "  \"w1q111\", \"w1q115\", \"w1q116\", \"w1q117\", \"w1q134\", \n",
    "  \"w1q146a\", \"w1q146h\", \"w1q146i\", \"w1q146j\", \"w1q146k\", \n",
    "  \"w1q146l\", \"w1q169\"]\n",
    "\n",
    "s_mode = [\"w1q175\", \"w1q72\", \"w1q142d\", \"w1q65\", \"w1q52\", \n",
    "  \"w1q142i\", \"w1q142g\", \"w1q180\", \"w1q69\", \"w1q135d\", \n",
    "  \"w1q142h\", \"w1q135b\", \"w1q135c\", \"w1q135e\", \"w1q142b\", \n",
    "  \"w1q142e\", \"w1q142f\", \"w1q142j\", \"w1q142a\", \"w1q142k\", \n",
    "  \"w1q19c\", \"w1q19d\", \"w1q19b\", \"w1q19a\", \"w1q75\", \n",
    "  \"w1q78\", \"w1q79\", \"w1q76\",                    # original ^ added V\n",
    "  \"w1q135a\", \"w1q135f\", \"w1q142c\", \"w1q35\", \"w1q36\", \n",
    "  \"w1q37\", \"w1q38\", \"w1q124\", \"w1q89\"]\n",
    "\n",
    "# agh I found more\n",
    "\n",
    "s_median_2 = '''\n",
    "w1q103\n",
    "w1q104\n",
    "w1q106\n",
    "w1q107\n",
    "w1q108\n",
    "w1q111\n",
    "w1q115\n",
    "w1q116\n",
    "w1q117\n",
    "w1q134\n",
    "w1q146a\n",
    "w1q146h\n",
    "w1q146i\n",
    "w1q146j\n",
    "w1q146k\n",
    "w1q146l\n",
    "w1q169'''\n",
    "\n",
    "s_mode_2 = '''\n",
    "w1q135a\n",
    "w1q135f\n",
    "w1q142c\n",
    "w1q35\n",
    "w1q36\n",
    "w1q37\n",
    "w1q38\n",
    "w1q124\n",
    "w1q89'''\n",
    "\n",
    "print(s_median_2.replace('\\n', '\", \"'))\n",
    "print(s_mode_2.replace('\\n', '\", \"'))\n",
    "\n",
    "s_97s = []\n",
    "s_98s = []\n",
    "s_99s = []\n",
    "for i in s_median:\n",
    "  a = list(meyer[i].unique())\n",
    "  if ((97 in a) | (97. in a)):\n",
    "    s_97s.append(i)\n",
    "  if ((98 in a) | (98. in a)):\n",
    "    s_98s.append(i)\n",
    "  if ((99 in a) | (99. in a)):\n",
    "    s_99s.append(i)\n",
    "    \n",
    "s_97s.sort()\n",
    "s_98s.sort()\n",
    "s_99s.sort()\n",
    "\n",
    "for i in s_mode:\n",
    "  a = list(meyer[i].unique())\n",
    "  if ((97 in a) | (97. in a)):\n",
    "    s_97s.append(i)\n",
    "  if ((98 in a) | (98. in a)):\n",
    "    s_98s.append(i)\n",
    "  if ((99 in a) | (99. in a)):\n",
    "    s_99s.append(i)\n",
    "\n",
    "s_97s = ['w1q102', 'w1q103', 'w1q104', \n",
    "  'w1q106', 'w1q107', 'w1q108', 'w1q110', 'w1q111', 'w1q112',   # impute to 0s; \n",
    "  'w1q114', 'w1q115', 'w1q116', 'w1q117', 'w1q134']    # these are (mostly) age Qs\n",
    "s_98s = ['w1q45', 'w1q46', 'w1q47', 'w1q48', 'w1q49', 'w1q50', 'w1q51']  # NA\n",
    "s_99s = ['w1q45', 'w1q46', 'w1q47', 'w1q48', 'w1q49', 'w1q50', 'w1q51']  # age + 2\n",
    "\n",
    "s_median_new = [''.join([x, '_ei']) for x in s_median]\n",
    "s_mode_new = [''.join([x, '_ei']) for x in s_mode]\n",
    "\n",
    "s_97s_new = [''.join([x, '_ei']) for x in s_97s]\n",
    "s_98s_new = [''.join([x, '_ei']) for x in s_98s]\n",
    "s_99s_new = [''.join([x, '_ei']) for x in s_99s]\n",
    "\n",
    "# Replace these 97s with 0s\n",
    "meyer.loc[:, s_97s_new] = np.where(meyer.loc[:, s_97s]==97, 0, meyer.loc[:, s_97s])\n",
    "\n",
    "# Take a peek\n",
    "meyer[['w1q102', 'w1q103', 'w1q102_ei', 'w1q103_ei']]\n",
    "\n",
    "\n",
    "# Replace these 98s with NAs\n",
    "meyer.loc[:, s_98s_new] = np.where(meyer.loc[:, s_98s]==98, np.nan, meyer.loc[:, s_98s])\n",
    "\n",
    "# Take a peek\n",
    "meyer[['w1q45', 'w1q46', 'w1q45_ei', 'w1q46_ei']]\n",
    "\n",
    "\n",
    "# Replace these 99s with NAs\n",
    "meyer.loc[:, s_99s_new] = np.where(meyer.loc[:, s_99s]==99, np.nan, meyer.loc[:, s_99s_new])\n",
    "\n",
    "# Take a peek\n",
    "meyer[['w1q45', 'w1q46', 'w1q45_ei', 'w1q46_ei']]\n",
    "\n",
    "\n",
    "xs_97s = []\n",
    "xs_98s = []\n",
    "xs_99s = []\n",
    "\n",
    "# for i in s_97s_new:\n",
    "# for i in s_98s_new:\n",
    "for i in s_99s_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  if ((97 in a) | (97. in a)):\n",
    "    xs_97s.append(i)\n",
    "  if ((98 in a) | (98. in a)):\n",
    "    xs_98s.append(i)\n",
    "  if ((99 in a) | (99. in a)):\n",
    "    xs_99s.append(i)\n",
    "# none left!\n",
    "\n",
    "# Right here I need to figure out how to deal with the 97s-99s.\n",
    "# Ok here's hoping that's settled!\n",
    "\n",
    "# put them in order.  checking them made me crazy otherwise\n",
    "s_mode.sort()\n",
    "s_mode_new.sort()\n",
    "s_median.sort()\n",
    "s_median_new.sort()\n",
    "\n",
    "# make sure they're in order\n",
    "for i, j in list(zip(s_mode, s_mode_new)):\n",
    "  print(i, j)\n",
    "for i, j in list(zip(s_median, s_median_new)):\n",
    "  print(i, j)\n",
    "\n",
    "# They are!  Ok!  Let's go again!!\n",
    "\n",
    "# Safety first\n",
    "# meyer.to_csv('2024-06-05_meyer_much_imputation_done_but_not_si_yet.csv', index = False)\n",
    "\n",
    "# I think the cleanest way to do this without it breaking everything is to\n",
    "# break the dataframe apart, use the 2 SIs separately, then re-merge.\n",
    "# Let me check that the studyID will be suitable for merging on.\n",
    "\n",
    "meyer.shape # (1507, 271)\n",
    "# ah crap gotta drop the original 97s-99s\n",
    "\n",
    "drop_90s = s_97s + s_98s # s_99s = s_98s\n",
    "meyer.drop(columns = drop_90s, inplace = True)\n",
    "\n",
    "\n",
    "meyer.shape # 1507 rows (250 columns)\n",
    "meyer['studyid'].nunique() # 1507 unique values\n",
    "\n",
    "# update the lists of columns\n",
    "med_cols = [c if c in list(meyer.columns) else ''.join([c, '_ei']) for c in s_median]\n",
    "mode_cols = [c if c in list(meyer.columns) else ''.join([c, '_ei']) for c in s_mode]\n",
    "\n",
    "# split the df up for imputing\n",
    "meyer_median = meyer[med_cols+['studyid']]\n",
    "meyer_mode = meyer[mode_cols+['studyid']]\n",
    "\n",
    "# new column names\n",
    "s_median_new.append('studyid')\n",
    "s_mode_new.append('studyid')\n",
    "meyer_median.columns = s_median_new\n",
    "meyer_mode.columns = s_mode_new\n",
    "\n",
    "# Instantiate\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "si_med = SimpleImputer(strategy = 'median')\n",
    "si_mode = SimpleImputer(strategy = 'most_frequent')\n",
    "\n",
    "# do the imputation!\n",
    "meyer_median = pd.DataFrame(\n",
    "  si_med.fit_transform(meyer_median), columns = si_med.get_feature_names_out())\n",
    "\n",
    "meyer_mode = pd.DataFrame(\n",
    "  si_mode.fit_transform(meyer_mode), columns = si_mode.get_feature_names_out())\n",
    "\n",
    "# is it really that easy?\n",
    "meyer_median.isna().sum().sum()\n",
    "meyer_mode.isna().sum().sum()\n",
    "\n",
    "mme = meyer_median.describe()\n",
    "mmo = meyer_mode.describe()\n",
    "\n",
    "# Yes!  It is!  (This time.)\n",
    "\n",
    "# BUT because I imputed the 97s to be 0s, and there were so many 97s, that made the \n",
    "# median and mode 0.  So most of these guys just ended up imputed with 0s.\n",
    "# It's not exactly what I wanted (the true missings are people who DID have the \n",
    "# relevant thoughts, but just wouldn't say when), but given that there are so few \n",
    "# true missings, and that this has already eaten up a ton of time, I think it'll\n",
    "# be fine to leave for this iteration.  Put it in the next steps.\n",
    "\n",
    "# OK LAST THING RE-MERGE WOOHOO!\n",
    "meyer.shape # full thing has 250 columns\n",
    "len(s_median) # 31, including ID\n",
    "len(s_mode) # 37, including ID\n",
    "\n",
    "# also let me check some stuff\n",
    "# are the column names what I think they are?\n",
    "s_median_new==list(meyer_median.columns) # True\n",
    "s_mode_new==list(meyer_mode.columns) # True\n",
    "# is studyid the only column name that doesn't end in _ei\n",
    "[c for c in list(meyer_median.columns) if c.split('_')[-1]!='ei']\n",
    "[c for c in list(meyer_mode.columns) if c.split('_')[-1]!='ei']\n",
    "\n",
    "# ok let's GOOOO\n",
    "# I'm just going to combine all the possible columns that I might\n",
    "# now need to drop, rather than trying to remember which is which.\n",
    "# And then I'll filter it down to only the ones actually in meyer.columns.\n",
    "\n",
    "# drop_cols = s_median + s_mode + s_median_new + s_mode_new\n",
    "# len(drop_cols) # 138\n",
    "# \n",
    "# drop_cols = [c for c in drop_cols if c in list(meyer.columns)]\n",
    "# drop_cols = list(set(drop_cols))\n",
    "# drop_cols.remove('studyid')\n",
    "# len(drop_cols) # 68\n",
    "# ((len(s_median_new)-1) + (len(s_mode_new)-1)) # 66\n",
    "# # ooooh right I added some other suicide questions in and fixed their 97s\n",
    "# x = [c for c in drop_cols if c not in s_mode]\n",
    "# x = [c for c in x if c not in s_mode_new]\n",
    "# x = [c for c in x if c not in s_median_new]\n",
    "# x = [c for c in x if c not in s_median]\n",
    "# x # empty\n",
    "# x = [c for c in drop_cols if c not in s_mode_new]\n",
    "# x = [c for c in x if c not in s_median_new]\n",
    "# x\n",
    "\n",
    "# oh. duh. new approach\n",
    "drop_cols = med_cols + mode_cols\n",
    "len(drop_cols) # 68\n",
    "compare = [c for c in drop_cols if c in list(meyer.columns)]\n",
    "len(compare) # 68\n",
    "len(list(meyer_median.columns) + list(meyer_mode.columns)) # 70\n",
    "\n",
    "# safety first\n",
    "# meyer.to_csv('2024-06-05_pre-si-remerging.csv', index = False)\n",
    "# meyer_median.to_csv('2024-06-05_values-imputed-with-median.csv', index = False)\n",
    "# meyer_mode.to_csv('2024-06-05_values-imputed-with-mode.csv', index = False)\n",
    "\n",
    "# drop cols\n",
    "meyer.drop(columns = drop_cols, inplace = True) # [1507 rows x 182 columns]\n",
    "\n",
    "# merge\n",
    "meyer_m = meyer.merge(\n",
    "  meyer_median, left_on = 'studyid', right_on = 'studyid', how = 'left').merge(\n",
    "    meyer_mode, left_on = 'studyid', right_on = 'studyid', how = 'left')\n",
    "\n",
    "meyer_m.shape # (1507, 250)\n",
    "\n",
    "meyer_m.isna().sum()\n",
    "\n",
    "meyer = meyer_m.copy(deep = True)\n",
    "\n",
    "del meyer_m\n",
    "\n",
    "# meyer.to_csv('2024-06-05_si-imputation-done_remerged.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 3 in full:\n",
    "2024-06-05_finishing-imputation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wednesday, June 5, 2024\n",
    "# Imputation Regrouping\n",
    "\n",
    "# Most of my imputation is now done!  \n",
    "# I really messed up my spreadsheet though, so I'm\n",
    "# just going to make a new one.\n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "meyer = pd.read_csv('2024-06-05_si-imputation-done_remerged.csv')\n",
    "\n",
    "remaining_nas = pd.DataFrame(meyer.isna().sum())\n",
    "original_nas = pd.read_excel('./dsb318-capstone/04_scratch_work/01_notes/imputation_plan.xlsx')\n",
    "  \n",
    "remaining_nas.shape\n",
    "original_nas.shape\n",
    "\n",
    "nas = remaining_nas.merge(\n",
    "  original_nas, left_on = remaining_nas.index, right_on = 'column_name', how = 'left')\n",
    "\n",
    "nas.index = list(remaining_nas.index)\n",
    "\n",
    "nas = nas[nas[0]!=0]  # that column is the most up to date count\n",
    "# if it's currently 0, drop it\n",
    "\n",
    "\n",
    "# my spreadsheet got messed up, so I'm going back around on some of these\n",
    "\n",
    "for i in nas['column_name']:\n",
    "  print(f\"meyer[['{i}_ei']] = meyer[['{i}']].fillna(_)\")\n",
    "\n",
    "\n",
    "# The easy-er ones\n",
    "\n",
    "meyer[['w1q01_ei']] = meyer[['w1q01']].fillna(7)\n",
    "meyer[['w1q02_ei']] = meyer[['w1q02']].fillna(8)\n",
    "meyer[['w1q33_ei']] = meyer[['w1q33']].fillna(0)\n",
    "meyer[['w1q123a_ei']] = meyer[['w1q123a']].fillna(5)\n",
    "meyer[['w1q123b_ei']] = meyer[['w1q123b']].fillna(5)\n",
    "meyer[['w1q114_ei']] = meyer[['w1q114_ei']].fillna(0) # note that this guy already existed\n",
    "meyer[['w1q34_ei']] = meyer[['w1q34']].fillna(0) \n",
    "# I was originally going to impute this ^ from other info, but a 0 is fine and faster\n",
    "meyer[['w1q121_ei']] = meyer[['w1q121']].fillna(0) # nssh age\n",
    "meyer[['w1q122_ei']] = meyer[['w1q122']].fillna(0) # nssh age\n",
    "meyer.shape # (1507, 258)\n",
    "\n",
    "drop_cols = ['w1q01', 'w1q02', 'w1q33', 'w1q123a', \n",
    "  'w1q123b', 'w1q34', 'w1q121', 'w1q122']\n",
    "\n",
    "meyer.drop(columns = drop_cols, inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "# ooh boy here we go\n",
    "\n",
    "# impute 2 if sum(139)>0, else mode\n",
    "s = ''\n",
    "for i in list(range(1, 11)):\n",
    "  s += f'meyer[\"w1q139_{i}_ei\"] + '\n",
    "\n",
    "meyer['sum_w1q139'] = meyer[\"w1q139_1_ei\"] + meyer[\n",
    "  \"w1q139_2_ei\"] + meyer[\"w1q139_3_ei\"] + meyer[\n",
    "  \"w1q139_4_ei\"] + meyer[\"w1q139_5_ei\"] + meyer[\n",
    "  \"w1q139_6_ei\"] + meyer[\"w1q139_7_ei\"] + meyer[\n",
    "  \"w1q139_8_ei\"] + meyer[\"w1q139_9_ei\"] + meyer[\"w1q139_10_ei\"]\n",
    "\n",
    "meyer['sum_w1q139'].value_counts(dropna = False)\n",
    "\n",
    "meyer[['w1q137_ei']] = meyer[['w1q137']]\n",
    "meyer[['w1q138_ei']] = meyer[['w1q138']]\n",
    "\n",
    "cond1 = meyer['w1q137'].isna()\n",
    "cond2 = meyer['w1q138'].isna()\n",
    "\n",
    "meyer.loc[cond1, 'w1q137_ei'] = np.where(meyer.loc[cond1, 'sum_w1q139']>0, 2, 1)\n",
    "meyer.loc[cond2, 'w1q138_ei'] = np.where(meyer.loc[cond2, 'sum_w1q139']>0, 2, 1)\n",
    "\n",
    "meyer['w1q137'].value_counts(dropna = False)\n",
    "meyer['w1q137_ei'].value_counts(dropna = False)\n",
    "meyer['w1q138'].value_counts(dropna = False)\n",
    "meyer['w1q138_ei'].value_counts(dropna = False)\n",
    "\n",
    "# nice\n",
    "meyer.drop(columns = ['w1q137', 'w1q138', 'sum_w1q139'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "\n",
    "# impute 2 if sum(141)>0, else mode\n",
    "meyer['sum_w1q141'] = meyer[\"w1q141_1_ei\"] + meyer[\n",
    "  \"w1q141_2_ei\"] + meyer[\"w1q141_3_ei\"] + meyer[\n",
    "  \"w1q141_4_ei\"] + meyer[\"w1q141_5_ei\"] + meyer[\n",
    "  \"w1q141_6_ei\"] + meyer[\"w1q141_7_ei\"] + meyer[\n",
    "  \"w1q141_8_ei\"] + meyer[\"w1q141_9_ei\"] + meyer[\"w1q141_10_ei\"]\n",
    "\n",
    "meyer['sum_w1q141'].value_counts(dropna = False)\n",
    "\n",
    "meyer[['w1q140_ei']] = meyer[['w1q140']]\n",
    "\n",
    "cond1 = meyer['w1q140'].isna()\n",
    "\n",
    "meyer.loc[cond1, 'w1q140_ei'] = np.where(meyer.loc[cond1, 'sum_w1q141']>0, 2, 1)\n",
    "\n",
    "meyer['w1q140'].value_counts(dropna = False)\n",
    "meyer['w1q140_ei'].value_counts(dropna = False)\n",
    "\n",
    "# nice\n",
    "meyer.drop(columns = ['w1q140', 'sum_w1q141'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "\n",
    "# subset investigation; there were exactly 48 do not knows on q2\n",
    "\n",
    "cond1 = meyer['w1q03'].isna()\n",
    "\n",
    "test = meyer.loc[cond1, ['w1q02_ei', 'w1q03']]\n",
    "test['w1q02_ei'].value_counts(dropna = False, normalize = True) # why so many 8s?\n",
    "meyer['w1q02_ei'].value_counts(dropna = False, normalize = True)  # close enough for now\n",
    "\n",
    "# I conducted that test to see if the 48 NAs in Q3 were the same\n",
    "# people as the 48 \"don't knows\" in  Q2, or any other pattern.\n",
    "# I did not find one, and therefore imputed the mode, which is 2.\n",
    "\n",
    "meyer[['w1q03_ei']] = meyer[['w1q03']].fillna(2)\n",
    "\n",
    "meyer.shape\n",
    "meyer.drop(columns = 'w1q03', inplace = True)\n",
    "\n",
    "\n",
    "# impute from 142b or .c=True and 142e=False; and/or w1q171_x\n",
    "meyer['w1q142b_ei'].value_counts(dropna = False)\n",
    "meyer['w1q142c_ei'].value_counts(dropna = False)\n",
    "meyer['w1q146d'].value_counts(dropna = False)\n",
    "\n",
    "jobless_142s = ((meyer['w1q142b_ei']==1) | (meyer['w1q142c_ei']==1))\n",
    "cond1 = meyer['w1q146d'].isna()\n",
    "\n",
    "meyer[['w1q146d_ei']] = meyer[['w1q146d']]\n",
    "\n",
    "# meyer.loc[cond1, 'w1q146d_ei'] = np.where(meyer.loc[cond1, 'sum_w1q141']>0, 2, 1)\n",
    "\n",
    "# trying to do the np.where is making my head hurt, so\n",
    "for i in list(range(1507)):\n",
    "  if pd.notna(meyer.loc[i, 'w1q146d_ei'])==True:\n",
    "    continue\n",
    "  elif meyer.loc[i, 'w1q142c_ei']==1:\n",
    "    meyer.loc[i, 'w1q146d_ei']=1\n",
    "  elif meyer.loc[i, 'w1q142b_ei']==1:\n",
    "    meyer.loc[i, 'w1q146d_ei']=1\n",
    "  else:\n",
    "    meyer.loc[i, 'w1q146d_ei']=0\n",
    "\n",
    "meyer['w1q142b_ei'].value_counts(dropna = False)\n",
    "meyer['w1q142c_ei'].value_counts(dropna = False)\n",
    "meyer['w1q146d'].value_counts(dropna = False)\n",
    "meyer['w1q146d_ei'].value_counts(dropna = False)\n",
    "\n",
    "meyer.drop(columns = 'w1q146d', inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "\n",
    "# impute from poverty\n",
    "meyer[['w1q146b_ei']] = meyer[['w1q146b']]\n",
    "\n",
    "money_cols = ['w1q146b', 'w1poverty_i', # 'w1povertycat_i', \n",
    "  'w1q142h_ei', 'w1age', 'geducation', 'w1q175_ei']\n",
    "\n",
    "[c for c in list(meyer.columns) if c.split('_')[-1] not in ['ei', 'i']]\n",
    "\n",
    "# 142h: During the last year have you experienced a major financial crisis; 1=y, 2=n\n",
    "# 175: under water w/ debt; 1=n, 2=y  --- student loans?  maybe but that's getting too bespoke\n",
    "\n",
    "meyer['w1q175_ei'].value_counts()\n",
    "meyer['w1q146b'].value_counts()\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "meyer.loc[(meyer['w1q146b'].isna()), money_cols]\n",
    "\n",
    "a = sorted(list(meyer.columns))\n",
    "\n",
    "# I checked several columns that also have to do with money.\n",
    "# The census questions can address most of these NAs. \n",
    "# I imputed a bunch of values for w1q175, so I'm hesitant\n",
    "# to use it to impute others.  w1q142h_ei (major financial\n",
    "# crisis) is a 2 (no) for all of the remaining NAs.\n",
    "# For the rest I'm going to impute 0.  It's a close tie\n",
    "# between 0 (not true that they don't have enough money\n",
    "# to make ends meet) and 1+2 (somewhat or very true), and\n",
    "# I'm tempted to impute a 1 (somewhat true that they don't\n",
    "# have enough money to make ends meet), but it feels less\n",
    "# presumptuous to impute a 0.\n",
    "\n",
    "# trying to do the np.where is making my head hurt, so\n",
    "for i in list(range(1507)):\n",
    "  if pd.notna(meyer.loc[i, 'w1q146b_ei'])==True:\n",
    "    continue\n",
    "  elif meyer.loc[i, 'w1poverty_i']==1:  # census poverty yes\n",
    "    meyer.loc[i, 'w1q146b_ei']=1\n",
    "  else:\n",
    "    meyer.loc[i, 'w1q146b_ei']=0\n",
    "\n",
    "meyer['w1q146b'].value_counts(dropna = False)\n",
    "meyer['w1q146b_ei'].value_counts(dropna = False)\n",
    "\n",
    "meyer.drop(columns = 'w1q146b', inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "\n",
    "\n",
    "# subset investigation\n",
    "# these are the Qs about US nativity\n",
    "\n",
    "meyer = pd.read_csv('2024-06-05_most-imputation-done_midway-through-nativity.csv')\n",
    "\n",
    "meyer[['w1q166_ei']] = meyer[['w1q166']]\n",
    "meyer[['w1q167_ei']] = meyer[['w1q167']]\n",
    "meyer[['w1q168_ei']] = meyer[['w1q168']]\n",
    "\n",
    "test = meyer[['w1q166', 'w1q167', 'w1q168']]\n",
    "test1 = test[test['w1q166'].isna()] # u born here?\n",
    "test2 = test[test['w1q167'].isna()] # u live here 6-13?\n",
    "test3 = test[test['w1q168'].isna()] # parents NOT born here?\n",
    "\n",
    "# if they left all 3 blank, I'm imputing the mode\n",
    "a = meyer['w1q166'].isna()\n",
    "b = meyer['w1q167'].isna()\n",
    "c = meyer['w1q168'].isna()\n",
    "\n",
    "meyer.loc[(a & b & c), 'w1q166_ei'] = 1\n",
    "meyer.loc[(a & b & c), 'w1q167_ei'] = 1\n",
    "meyer.loc[(a & b & c), 'w1q168_ei'] = 3\n",
    "\n",
    "# look again\n",
    "test = meyer[['w1q166_ei', 'w1q167_ei', 'w1q168_ei']]\n",
    "test1 = test[test['w1q166_ei'].isna()] # u born here?\n",
    "test2 = test[test['w1q167_ei'].isna()] # u live here 6-13?\n",
    "test3 = test[test['w1q168_ei'].isna()] # parents NOT born here?\n",
    "\n",
    "# ok now the rest\n",
    "# looking at the pattern of associated 167s and 168s, I think \n",
    "# it's safe to impute the remaining (n=5) NAs in 166 as 1 \n",
    "\n",
    "meyer[['w1q166_ei']] = meyer[['w1q166_ei']].fillna(1)\n",
    "\n",
    "# looking at the pattern of associated 166s and 168s, I think \n",
    "# it's safe to impute the remaining (n=5) NAs in 167 thusly:\n",
    "a = meyer['w1q166']==2 # I wasn't born here\n",
    "b = meyer['w1q167'].isna()\n",
    "c = ((meyer['w1q168'].notna()) & (meyer['w1q168']<3)) # 1+ parent not born here\n",
    "\n",
    "meyer.loc[(a & b & c), 'w1q167_ei'] = 2 # under those conditions, impute NO\n",
    "meyer[['w1q167_ei']] = meyer[['w1q167_ei']].fillna(1) # otherwise yes\n",
    "\n",
    "# looking at the pattern of associated 166s and 167s, I think \n",
    "# it's safe to impute the remaining (n=5) NAs in 168 as 1 \n",
    "\n",
    "meyer[['w1q168_ei']] = meyer[['w1q168_ei']].fillna(1)\n",
    "\n",
    "meyer.drop(columns = ['w1q166', 'w1q167', 'w1q168'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "# let's back this guy up.  I'm getting nervous.\n",
    "# meyer.to_csv('2024-06-05_most-imputation-done_midway-through-nativity.csv', index = False)\n",
    "\n",
    "# let's back this guy up.  I'm getting nervous.\n",
    "# meyer.to_csv('2024-06-05_all-imputation-except-poverty-done.csv', index = False)\n",
    "\n",
    "# subset investigation\n",
    "meyer[['w1poverty_i_ei']] = meyer[['w1poverty_i']].fillna(0)\n",
    "meyer[['w1povertycat_i_ei']] = meyer[['w1povertycat_i']].fillna(4)\n",
    "\n",
    "meyer.drop(columns = ['w1poverty_i', 'w1povertycat_i'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "meyer.isna().sum().sum() # 0!\n",
    "\n",
    "meyer.to_csv('2024-06-05_all-imputation-done.csv', index = False)\n",
    "\n",
    "# Reorder the columns\n",
    "ordered_cols = sorted(list(meyer.columns))\n",
    "ordered_cols.remove('studyid')\n",
    "ordered_cols = ['studyid'] + ordered_cols\n",
    "len(ordered_cols) # 250\n",
    "\n",
    "meyer = meyer[ordered_cols]\n",
    "meyer.shape\n",
    "\n",
    "meyer.to_csv('2024-06-05_all-imputation-done_reordered.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 4 in full:\n",
    "2024-06-05_making-a-list-of-potential-7s-98s-etc-to-check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wednesday, June 5, 2024\n",
    "# Wrapping up loose values\n",
    "\n",
    "# All of my imputation is done, hooray!!\n",
    "# Now I'm just going back through and looking for all of the\n",
    "# \"non-NA NAs\" (e.g., 97, 98, 99) that I need to replace with \n",
    "# real imputations, lest they through the entire column off.\n",
    "\n",
    "for i in list(meyer.columns):\n",
    "  print(i)\n",
    "  \n",
    "problem_children = [('w1q01_ei', [98]), ('w1q02_ei', [98]), \n",
    "  ('w1q33_ei', [97]), ('w1q34_ei', [97]), ('w1q35_ei', [97]), \n",
    "  ('w1q36_ei', [97]), ('w1q45_ei', [98, 99]), ('w1q46_ei', [98, 99]), \n",
    "  ('w1q47_ei', [98, 99]), ('w1q48_ei', [98, 99]), ('w1q49_ei', [98, 99]), \n",
    "  ('w1q50_ei', [98, 99]), ('w1q51_ei', [98, 99]), ('w1q89_ei', [7]), \n",
    "  ('w1q102_ei', [97]), ('w1q103_ei', [97]), ('w1q104_ei', [97]), \n",
    "  ('w1q106_ei', [97]), ('w1q107_ei', [97]), ('w1q108_ei', [97]),\n",
    "  ('w1q110_ei', [97]), ('w1q111_ei', [0, 97]), ('w1q112_ei', [97]),\n",
    "  ('w1q114_ei', [6, 7, 8, 9, 97]), ('w1q115_ei', [97]), ('w1q116_ei', [97]),\n",
    "  ('w1q117_ei', [97]), ('w1q120_ei', [97]), ('w1q121_ei', [97]),\n",
    "  ('w1q122_ei', [97]), ('w1q123a_ei', [5]), ('w1q123b_ei', [5]), \n",
    "  ('w1q123c_ei', [5]), ('w1q123d_ei', [5]), ('w1q134_ei', [97]), \n",
    "  ('w1q168_ei', [97]), ('w1q136_10_ei', [7, 97]), ('w1q136_1_ei', [7, 97]), \n",
    "  ('w1q136_2_ei', [7, 97]), ('w1q136_3_ei', [7, 97]), \n",
    "  ('w1q136_4_ei', [7, 97]), ('w1q136_5_ei', [7, 97]), \n",
    "  ('w1q136_6_ei', [7, 97]), ('w1q136_7_ei', [7, 97]), \n",
    "  ('w1q136_8_ei', [7, 97]), ('w1q136_9_ei', [7, 97]), \n",
    "  ('w1q139_10_ei', [7, 97]), ('w1q139_1_ei', [7, 97]), \n",
    "  ('w1q139_2_ei', [7, 97]), ('w1q139_3_ei', [7, 97]), \n",
    "  ('w1q139_4_ei', [7, 97]), ('w1q139_5_ei', [7, 97]), \n",
    "  ('w1q139_6_ei', [7, 97]), ('w1q139_7_ei', [7, 97]), \n",
    "  ('w1q139_8_ei', [7, 97]), ('w1q139_9_ei', [7, 97]), \n",
    "  ('w1q141_10_ei', [7, 97]), ('w1q141_1_ei', [7, 97]), \n",
    "  ('w1q141_2_ei', [7, 97]), ('w1q141_3_ei', [7, 97]), \n",
    "  ('w1q141_4_ei', [7, 97]), ('w1q141_5_ei', [7, 97]), \n",
    "  ('w1q141_6_ei', [7, 97]), ('w1q141_7_ei', [7, 97]), \n",
    "  ('w1q141_8_ei', [7, 97]), ('w1q141_9_ei', [7, 97]), \n",
    "  ('w1q143_10_ei', [7, 97]), ('w1q143_1_ei', [7, 97]), \n",
    "  ('w1q143_2_ei', [7, 97]), ('w1q143_3_ei', [7, 97]), \n",
    "  ('w1q143_4_ei', [7, 97]), ('w1q143_5_ei', [7, 97]), \n",
    "  ('w1q143_6_ei', [7, 97]), ('w1q143_7_ei', [7, 97]), \n",
    "  ('w1q143_8_ei', [7, 97]), ('w1q143_9_ei', [7, 97]), \n",
    "  ('w1q145_10_ei', [7, 97]), ('w1q145_1_ei', [7, 97]), \n",
    "  ('w1q145_2_ei', [7, 97]), ('w1q145_3_ei', [7, 97]), \n",
    "  ('w1q145_4_ei', [7, 97]), ('w1q145_5_ei', [7, 97]), \n",
    "  ('w1q145_6_ei', [7, 97]), ('w1q145_7_ei', [7, 97]), \n",
    "  ('w1q145_8_ei', [7, 97]), ('w1q145_9_ei', [7, 97]), \n",
    "  ('w1q163_10_ei', [7, 97]), ('w1q163_1_ei', [7, 97]), \n",
    "  ('w1q163_2_ei', [7, 97]), ('w1q163_3_ei', [7, 97]), \n",
    "  ('w1q163_4_ei', [7, 97]), ('w1q163_5_ei', [7, 97]), \n",
    "  ('w1q163_6_ei', [7, 97]), ('w1q163_7_ei', [7, 97]), \n",
    "  ('w1q163_8_ei', [7, 97]), ('w1q163_9_ei', [7, 97])]\n",
    "\n",
    "\n",
    "s = '''\n",
    "w1q136_10_ei\n",
    "w1q136_1_ei\n",
    "w1q136_2_ei\n",
    "w1q136_3_ei\n",
    "w1q136_4_ei\n",
    "w1q136_5_ei\n",
    "w1q136_6_ei\n",
    "w1q136_7_ei\n",
    "w1q136_8_ei\n",
    "w1q136_9_ei\n",
    "w1q139_10_ei\n",
    "w1q139_1_ei\n",
    "w1q139_2_ei\n",
    "w1q139_3_ei\n",
    "w1q139_4_ei\n",
    "w1q139_5_ei\n",
    "w1q139_6_ei\n",
    "w1q139_7_ei\n",
    "w1q139_8_ei\n",
    "w1q139_9_ei\n",
    "w1q141_10_ei\n",
    "w1q141_1_ei\n",
    "w1q141_2_ei\n",
    "w1q141_3_ei\n",
    "w1q141_4_ei\n",
    "w1q141_5_ei\n",
    "w1q141_6_ei\n",
    "w1q141_7_ei\n",
    "w1q141_8_ei\n",
    "w1q141_9_ei\n",
    "w1q143_10_ei\n",
    "w1q143_1_ei\n",
    "w1q143_2_ei\n",
    "w1q143_3_ei\n",
    "w1q143_4_ei\n",
    "w1q143_5_ei\n",
    "w1q143_6_ei\n",
    "w1q143_7_ei\n",
    "w1q143_8_ei\n",
    "w1q143_9_ei\n",
    "w1q145_10_ei\n",
    "w1q145_1_ei\n",
    "w1q145_2_ei\n",
    "w1q145_3_ei\n",
    "w1q145_4_ei\n",
    "w1q145_5_ei\n",
    "w1q145_6_ei\n",
    "w1q145_7_ei\n",
    "w1q145_8_ei\n",
    "w1q145_9_ei\n",
    "w1q163_10_ei\n",
    "w1q163_1_ei\n",
    "w1q163_2_ei\n",
    "w1q163_3_ei\n",
    "w1q163_4_ei\n",
    "w1q163_5_ei\n",
    "w1q163_6_ei\n",
    "w1q163_7_ei\n",
    "w1q163_8_ei\n",
    "w1q163_9_ei'''\n",
    "\n",
    "k = s.replace('\\n', '\", \"')\n",
    "print(k)\n",
    "\n",
    "s = [\"w1q136_10_ei\", \"w1q136_1_ei\", \"w1q136_2_ei\", \"w1q136_3_ei\", \n",
    "  \"w1q136_4_ei\", \"w1q136_5_ei\", \"w1q136_6_ei\", \"w1q136_7_ei\", \n",
    "  \"w1q136_8_ei\", \"w1q136_9_ei\", \"w1q139_10_ei\", \"w1q139_1_ei\", \n",
    "  \"w1q139_2_ei\", \"w1q139_3_ei\", \"w1q139_4_ei\", \"w1q139_5_ei\", \n",
    "  \"w1q139_6_ei\", \"w1q139_7_ei\", \"w1q139_8_ei\", \"w1q139_9_ei\", \n",
    "  \"w1q141_10_ei\", \"w1q141_1_ei\", \"w1q141_2_ei\", \"w1q141_3_ei\", \n",
    "  \"w1q141_4_ei\", \"w1q141_5_ei\", \"w1q141_6_ei\", \"w1q141_7_ei\", \n",
    "  \"w1q141_8_ei\", \"w1q141_9_ei\", \"w1q143_10_ei\", \"w1q143_1_ei\", \n",
    "  \"w1q143_2_ei\", \"w1q143_3_ei\", \"w1q143_4_ei\", \"w1q143_5_ei\", \n",
    "  \"w1q143_6_ei\", \"w1q143_7_ei\", \"w1q143_8_ei\", \"w1q143_9_ei\", \n",
    "  \"w1q145_10_ei\", \"w1q145_1_ei\", \"w1q145_2_ei\", \"w1q145_3_ei\", \n",
    "  \"w1q145_4_ei\", \"w1q145_5_ei\", \"w1q145_6_ei\", \"w1q145_7_ei\", \n",
    "  \"w1q145_8_ei\", \"w1q145_9_ei\", \"w1q163_10_ei\", \"w1q163_1_ei\", \n",
    "  \"w1q163_2_ei\", \"w1q163_3_ei\", \"w1q163_4_ei\", \"w1q163_5_ei\", \n",
    "  \"w1q163_6_ei\", \"w1q163_7_ei\", \"w1q163_8_ei\", \"w1q163_9_ei\"]\n",
    "\n",
    "g = ''\n",
    "for i in s:\n",
    "  g += f\"('{i}', [7, 97]), \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSVs are in the main project folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thursday, June 6, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "Today I finished tracking down and handling all the non-NA NAs (7, 97, 98, 99), dropped some more columns, and did all the recoding and transformations I could think of in preparation to combine the columns via `feat_eng_dict`.  I even created a function today!\n",
    "\n",
    "I created a new set-up file today, in which I imported the most up to date version of the CSV that I saved yesterday.  I plan to do the same tomorrow.\n",
    "\n",
    "Today I finally chose a y variable - **kessler6**.  This is a general mental health measure.  I dropped a few rows because they had more than 1 missing value in the features that comprise this scale.  The original authors imputed values for everyone, but I didn't want to have y values with more than 1 component part that had to be imputed.  Interestingly, in doing so, I think (although I'm not sure) that I may have dropped the rows from which many of the other missing values came, too.  \n",
    "\n",
    "Because I was already well underway with my cleaning, I had to go about dropping these rows in a rather roundabout way.  (This would probably be best to whitewash in the final report.)  I reimported a clean copy of the dataset, located all the rows with more than one NA in that set of columns,and then extracted the corresponding `studyid`.  I then dropped the rows with that `studyid` in my working copy of the dataframe.  At the end of the day, my dataset is **(1494, 228)**.\n",
    "\n",
    "**I discovered that the kessler values *are not normally distributed*.**  I tried a few different things, and taking the square root of them showed the most improvement, although the qqplot still looks not-great (not as bad as project 4 though!).  **I created a column for the square root of these values, but *did not delete the original values*.  It will be important to not include them, or the `studyid` in my X matrix!**\n",
    "\n",
    "All of the non-NA NAs were handled a little differently, but (I think) I sufficiently commented the code to explain what I did and why.  Where possible/necessary/I remembered, I tried to mirror what I did with the actual NAs when dealing with these values.\n",
    "\n",
    "I then went through all the columns indicated in my `feat_eng_dict`, and recoded or transformed them as necessary.  I also updated the names of the columns in the dictionary, because all but literally 1 of them have had `'_ei'` and sometimes `'_r'` tacked onto them by now.  What I did with each batch of columns is marked in comments in the code.  This is also the script where I defined and used a function.  Also in this script, I OHE'd the columns about what religion someone was raised in or practices now.  I first collapsed these categories into fewer bins, then OHE'd them.  I chose to drop the resulting column that corresponded to, more or less, \"not religious,\" because it seemed most intuitive.  The other 2 categories are for Christian-ish religions, and other.  Despite it being somewhat disrespectful, I chose to collapse all other religious practices into \"other\" because these categories were all so small on their own.  Even combined, they are dwarfed by the other two options.\n",
    "\n",
    "Finally, when all that was done, I updated my dictionary and saved a fresh copy of it to use tomorrow.\n",
    "\n",
    "CSVs created (in the main project folder):\n",
    "2024-06-06_bad-y-rows-dropped.csv\n",
    "2024-06-06_bad-y-rows-dropped_reindexed.csv\n",
    "2024-06-06_non-NA-NAs-fixed.csv\n",
    "2024-06-06_recodes-for-feat-eng-dict-done_not-ohe-yet.csv\n",
    "2024-06-06_recodes-for-feat-eng-dict-done_religious-ohe-done.csv\n",
    "2024-06-06_recodes-for-feat-eng-dict-done_religious-ohe-done_reordered.csv\n",
    "2024-06-06_sqrt-kessler.csv\n",
    "\n",
    "`.py` files created: \n",
    "1. 2024-06-06_set-up.py\n",
    "2. 2024-06-06_non-NA-NAs.py\n",
    "3. 2024-06-06_y-variable.py\n",
    "4. 2024-06-06_column_recoding_for_feat_eng_dict_combinations.py\n",
    "5. 2024-06-06_updated_feat_eng_dict.py\n",
    "\n",
    "other files created: (neither of these warrant inclusion here)\n",
    "- 04_scratch_work/01_notes/2024-06-06_mid-modification_feat_eng_dict.txt\n",
    "- 04_scratch_work/01_notes/2024-06-06_post-modification_feat_eng_dict_scratch_notes.txt\n",
    "\n",
    "I managed not to modify any pre-existing scripts today, hooray!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 1 in full:\n",
    "2024-06-06_set-up.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capstone Setup\n",
    "\n",
    "# Module Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, date, time\n",
    "from string import capwords\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Working Directory\n",
    "os.getcwd()\n",
    "#os.chdir('C:/Users/emily/Git_Stuff/General_Assembly/04_Projects/project-capstone')\n",
    "os.listdir()\n",
    "\n",
    "# Import Data\n",
    "\n",
    "# THIS IS THE ORIGINAL\n",
    "# meyer = pd.read_csv(\n",
    "#   './potential_datasets/2024_05_23_download_ICPSR_Meyer_2023_generations_data_attempt_2/ICPSR_37166/DS0007/37166-0007-Data.tsv', \n",
    "#   sep = '\\t', low_memory=False, na_values = ' ') # Thanks to ibrahim rupawala for highlighting the na_values argument\n",
    "#   # https://stackoverflow.com/questions/13445241/replacing-blank-values-white-space-with-nan-in-pandas/47105408#47105408\n",
    "\n",
    "# THIS IS THE MOST UP TO DATE VERSION FOR WORKING ON\n",
    "meyer = pd.read_csv('2024-06-05_all-imputation-done_reordered.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 2 in full:\n",
    "2024-06-06_non-NA-NAs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thursday, June 6, 2024\n",
    "# \"Imputation\" of non-NA NAs (97s, etc.)\n",
    "\n",
    "# Wait, why did I make problem_children a list rather than a dictionary?\n",
    "\n",
    "s = \"\"\"\n",
    "('w1q01_ei', [98]), ('w1q02_ei', [98]), \n",
    "  ('w1q33_ei', [97]), ('w1q34_ei', [97]), ('w1q35_ei', [97]), \n",
    "  ('w1q36_ei', [97]), ('w1q45_ei', [98, 99]), ('w1q46_ei', [98, 99]), \n",
    "  ('w1q47_ei', [98, 99]), ('w1q48_ei', [98, 99]), ('w1q49_ei', [98, 99]), \n",
    "  ('w1q50_ei', [98, 99]), ('w1q51_ei', [98, 99]), ('w1q89_ei', [7]), \n",
    "  ('w1q102_ei', [97]), ('w1q103_ei', [97]), ('w1q104_ei', [97]), \n",
    "  ('w1q106_ei', [97]), ('w1q107_ei', [97]), ('w1q108_ei', [97]),\n",
    "  ('w1q110_ei', [97]), ('w1q111_ei', [0, 97]), ('w1q112_ei', [97]),\n",
    "  ('w1q114_ei', [6, 7, 8, 9, 97]), ('w1q115_ei', [97]), ('w1q116_ei', [97]),\n",
    "  ('w1q117_ei', [97]), ('w1q120_ei', [97]), ('w1q121_ei', [97]),\n",
    "  ('w1q122_ei', [97]), ('w1q123a_ei', [5]), ('w1q123b_ei', [5]), \n",
    "  ('w1q123c_ei', [5]), ('w1q123d_ei', [5]), ('w1q134_ei', [97]), \n",
    "  ('w1q168_ei', [97]), ('w1q136_10_ei', [7, 97]), ('w1q136_1_ei', [7, 97]), \n",
    "  ('w1q136_2_ei', [7, 97]), ('w1q136_3_ei', [7, 97]), \n",
    "  ('w1q136_4_ei', [7, 97]), ('w1q136_5_ei', [7, 97]), \n",
    "  ('w1q136_6_ei', [7, 97]), ('w1q136_7_ei', [7, 97]), \n",
    "  ('w1q136_8_ei', [7, 97]), ('w1q136_9_ei', [7, 97]), \n",
    "  ('w1q139_10_ei', [7, 97]), ('w1q139_1_ei', [7, 97]), \n",
    "  ('w1q139_2_ei', [7, 97]), ('w1q139_3_ei', [7, 97]), \n",
    "  ('w1q139_4_ei', [7, 97]), ('w1q139_5_ei', [7, 97]), \n",
    "  ('w1q139_6_ei', [7, 97]), ('w1q139_7_ei', [7, 97]), \n",
    "  ('w1q139_8_ei', [7, 97]), ('w1q139_9_ei', [7, 97]), \n",
    "  ('w1q141_10_ei', [7, 97]), ('w1q141_1_ei', [7, 97]), \n",
    "  ('w1q141_2_ei', [7, 97]), ('w1q141_3_ei', [7, 97]), \n",
    "  ('w1q141_4_ei', [7, 97]), ('w1q141_5_ei', [7, 97]), \n",
    "  ('w1q141_6_ei', [7, 97]), ('w1q141_7_ei', [7, 97]), \n",
    "  ('w1q141_8_ei', [7, 97]), ('w1q141_9_ei', [7, 97]), \n",
    "  ('w1q143_10_ei', [7, 97]), ('w1q143_1_ei', [7, 97]), \n",
    "  ('w1q143_2_ei', [7, 97]), ('w1q143_3_ei', [7, 97]), \n",
    "  ('w1q143_4_ei', [7, 97]), ('w1q143_5_ei', [7, 97]), \n",
    "  ('w1q143_6_ei', [7, 97]), ('w1q143_7_ei', [7, 97]), \n",
    "  ('w1q143_8_ei', [7, 97]), ('w1q143_9_ei', [7, 97]), \n",
    "  ('w1q145_10_ei', [7, 97]), ('w1q145_1_ei', [7, 97]), \n",
    "  ('w1q145_2_ei', [7, 97]), ('w1q145_3_ei', [7, 97]), \n",
    "  ('w1q145_4_ei', [7, 97]), ('w1q145_5_ei', [7, 97]), \n",
    "  ('w1q145_6_ei', [7, 97]), ('w1q145_7_ei', [7, 97]), \n",
    "  ('w1q145_8_ei', [7, 97]), ('w1q145_9_ei', [7, 97]), \n",
    "  ('w1q163_10_ei', [7, 97]), ('w1q163_1_ei', [7, 97]), \n",
    "  ('w1q163_2_ei', [7, 97]), ('w1q163_3_ei', [7, 97]), \n",
    "  ('w1q163_4_ei', [7, 97]), ('w1q163_5_ei', [7, 97]), \n",
    "  ('w1q163_6_ei', [7, 97]), ('w1q163_7_ei', [7, 97]), \n",
    "  ('w1q163_8_ei', [7, 97]), ('w1q163_9_ei', [7, 97])\"\"\"\n",
    "  \n",
    "d = s.replace(\"(\", \"\").replace(\")\", \"\").replace(\"', [\", \"': [\").replace(' \\n', '').replace('\\n', '')\n",
    "d\n",
    "\n",
    "# w1q120 is the only one without an _ei\n",
    "problem_children = {'w1q01_ei': [98], 'w1q02_ei': [98],  'w1q33_ei': [97], \n",
    "  'w1q34_ei': [97], 'w1q35_ei': [97],  'w1q36_ei': [97], 'w1q45_ei': [98, 99], \n",
    "  'w1q46_ei': [98, 99],  'w1q47_ei': [98, 99], 'w1q48_ei': [98, 99], \n",
    "  'w1q49_ei': [98, 99],  'w1q50_ei': [98, 99], 'w1q51_ei': [98, 99], \n",
    "  'w1q89_ei': [7],  'w1q102_ei': [97], 'w1q103_ei': [97], 'w1q104_ei': [97],  \n",
    "  'w1q106_ei': [97], 'w1q107_ei': [97], 'w1q108_ei': [97],  'w1q110_ei': [97], \n",
    "  'w1q111_ei': [0, 97], 'w1q112_ei': [97],  'w1q114_ei': [6, 7, 8, 9, 97], \n",
    "  'w1q115_ei': [97], 'w1q116_ei': [97],  'w1q117_ei': [97], 'w1q120': [97], \n",
    "  'w1q121_ei': [97],  'w1q122_ei': [97], 'w1q123a_ei': [5], 'w1q123b_ei': [5],  \n",
    "  'w1q123c_ei': [5], 'w1q123d_ei': [5], 'w1q134_ei': [97],  'w1q168_ei': [97], \n",
    "  'w1q136_10_ei': [7, 97], 'w1q136_1_ei': [7, 97],  'w1q136_2_ei': [7, 97], \n",
    "  'w1q136_3_ei': [7, 97],  'w1q136_4_ei': [7, 97], 'w1q136_5_ei': [7, 97],  \n",
    "  'w1q136_6_ei': [7, 97], 'w1q136_7_ei': [7, 97],  'w1q136_8_ei': [7, 97], \n",
    "  'w1q136_9_ei': [7, 97],  'w1q139_10_ei': [7, 97], 'w1q139_1_ei': [7, 97],  \n",
    "  'w1q139_2_ei': [7, 97], 'w1q139_3_ei': [7, 97],  'w1q139_4_ei': [7, 97], \n",
    "  'w1q139_5_ei': [7, 97],  'w1q139_6_ei': [7, 97], 'w1q139_7_ei': [7, 97],  \n",
    "  'w1q139_8_ei': [7, 97], 'w1q139_9_ei': [7, 97],  'w1q141_10_ei': [7, 97], \n",
    "  'w1q141_1_ei': [7, 97],  'w1q141_2_ei': [7, 97], 'w1q141_3_ei': [7, 97],  \n",
    "  'w1q141_4_ei': [7, 97], 'w1q141_5_ei': [7, 97],  'w1q141_6_ei': [7, 97], \n",
    "  'w1q141_7_ei': [7, 97],  'w1q141_8_ei': [7, 97], 'w1q141_9_ei': [7, 97],  \n",
    "  'w1q143_10_ei': [7, 97], 'w1q143_1_ei': [7, 97],  'w1q143_2_ei': [7, 97], \n",
    "  'w1q143_3_ei': [7, 97],  'w1q143_4_ei': [7, 97], 'w1q143_5_ei': [7, 97],  \n",
    "  'w1q143_6_ei': [7, 97], 'w1q143_7_ei': [7, 97],  'w1q143_8_ei': [7, 97], \n",
    "  'w1q143_9_ei': [7, 97],  'w1q145_10_ei': [7, 97], 'w1q145_1_ei': [7, 97],  \n",
    "  'w1q145_2_ei': [7, 97], 'w1q145_3_ei': [7, 97],  'w1q145_4_ei': [7, 97], \n",
    "  'w1q145_5_ei': [7, 97],  'w1q145_6_ei': [7, 97], 'w1q145_7_ei': [7, 97],  \n",
    "  'w1q145_8_ei': [7, 97], 'w1q145_9_ei': [7, 97],  'w1q163_10_ei': [7, 97], \n",
    "  'w1q163_1_ei': [7, 97],  'w1q163_2_ei': [7, 97], 'w1q163_3_ei': [7, 97],  \n",
    "  'w1q163_4_ei': [7, 97], 'w1q163_5_ei': [7, 97],  'w1q163_6_ei': [7, 97], \n",
    "  'w1q163_7_ei': [7, 97],  'w1q163_8_ei': [7, 97], 'w1q163_9_ei': [7, 97]}\n",
    "\n",
    "# # testing syntax to make sure I understand it\n",
    "# for i in ['cat', 'dog', 'bird', 'goose', 'ostrich', 'oliphaunt']:\n",
    "#   for j in i:\n",
    "#     if j != 'o':\n",
    "#       print(j)\n",
    "#       break\n",
    "#   print(f'finished {i}')\n",
    "\n",
    "column_jail = []\n",
    "for c, v in problem_children.items():\n",
    "  a = list(meyer[c].unique())\n",
    "  for i in v:\n",
    "    if i in a:\n",
    "      column_jail.append((c, v, i))\n",
    "      break\n",
    "\n",
    "for i in column_jail:\n",
    "  if i[0]=='w1q02_ei':\n",
    "    continue\n",
    "  meyer[i[0]].value_counts(dropna = False)\n",
    "  meyer[i[0]].value_counts(dropna = False, normalize = True)\n",
    "  print('')\n",
    "  print('='*20)\n",
    "  print('')\n",
    "\n",
    "# Here are the remaining values, and what I intend to do about them.\n",
    "\n",
    "# [('w1q01_ei', [98], 98), 98.0      2      0.001327\n",
    "# For only 2 values, imputing the median seems fine.\n",
    "\n",
    "# Because I *already* imputed values, RE-fitting the si may well end up with a DIFFERENT \n",
    "# median.  If I have time, I should just go back and skip si, and instead use .fillna() \n",
    "# to put in the POPULATED median, which is provided in the documentation (and I could \n",
    "# easily verify myself).  I'm not going to take the time NOW to go back and change all \n",
    "# that, but I am going to do it for these 2 values.\n",
    "cond = meyer['w1q01_ei']==98.0\n",
    "meyer.loc[cond, 'w1q01_ei']=7\n",
    "\n",
    "# ('w1q02_ei', [98], 98), 98.0     48    0.031851\n",
    "# I'm just going to drop this column. I've done SO MUCH imputation and it's really making\n",
    "#  me nervous.  I'm not planning to really hone in deep on optimism for the future or \n",
    "# whatever (THIS TIME), so I'm just going to chuck it for now.\n",
    "meyer.drop(columns = 'w1q02_ei', inplace = True)\n",
    "\n",
    "# ('w1q33_ei', [97], 97), \n",
    "# The Q is how long someone's been with their partner, and the 97s are people who said\n",
    "# they don't have partners.  The logical imputation is 0.  There are no natural 0s.\n",
    "cond = meyer['w1q33_ei']==97.0\n",
    "meyer.loc[cond, 'w1q33_ei']=0\n",
    "\n",
    "\n",
    "# ('w1q89_ei', [7], 7), \n",
    "# This is the question about how often people smoke cigarettes, and the 7s are people\n",
    "# who said on the previous question that they do not smoke (or failed to answer the \n",
    "# question, whom I (think I) imputed as no). Therefore, I'll impute \"not at all\" here.\n",
    "cond = meyer['w1q89_ei']==7.0\n",
    "meyer.loc[cond, 'w1q89_ei']=3\n",
    "\n",
    "\n",
    "# ('w1q111_ei', [0, 97], 0), \n",
    "# Ok, truly F these age-when columns.  They are way too complicated to get into in the \n",
    "# time I have.  They're colinear(ish) with the current-age column, or at least not\n",
    "# independent.  It is not possible for a 19 year old person to have made their first\n",
    "# suicide attempt at 25.  Plus, I highly doubt the relationship between age of attempt(s)\n",
    "# and overall mental health is linear.  I'd be way more worried about someone who made \n",
    "# their first suicide attempt at 11 OR at 35 than I would be about someone who made their\n",
    "# first attempt at 19.  A lot of people have suicidal ideation in their late teens/early\n",
    "# 20s, because they're old enough to feel the weight of the world but not old enough for \n",
    "# their brains to be done cooking.  I'd have to do a ton of research and reading to be sure\n",
    "# (and I don't have time for that either), but my intuition says it's probably a parabolic\n",
    "# relationship between the age of attempt and the redness of the flag.  PLUS, imputating\n",
    "# these guys has been the bane of my existence for over a week.  I think what I did already\n",
    "# was impute the missings and 97s as 0s, but now I'm thinking that was a bad idea!  A true\n",
    "# value of 0 (if it were even psychologically possible, which it isn't - I HIGHLY doubt a\n",
    "# baby can understand the concept of suicide well enough to have suicidal ideation, but\n",
    "# even if they did, there's no way that person would remember it as an adult), would be \n",
    "# EXTREMELY alarming, in a way that a linear model is just not equipped to deal with.  \n",
    "# Furthermore, getting back to the point about the curvilinear nature of this, I don't think\n",
    "# there's really ANY good value to impute here!  If one wanted to look at age effects, one \n",
    "# would really have to limit the sample to people who have made attempts.  \n",
    "\n",
    "# I also considered a scheme wherein I would subtract the min age from the max age to get a \n",
    "# range of time that they were suicidal, and if the min and max were both populated but\n",
    "# neither was 0 (i.e., they made multiple attempts at the same age) I would sub in a 1 (for\n",
    "# that 1 year of suicidality), and if one or the other (but not both!) was 0, or they only \n",
    "# answered the \"how old were you?\" question (i.e., they indicated that they'd only made one \n",
    "# attempt), I would also sub in 1, for that 1 year where they were feeling that way, but if\n",
    "# neither was populated or they were both 0 or whatever (i.e., for people who made no \n",
    "# attempts ever) I would keep a 0 - so it'd be 0 for people who never attempted, and a \n",
    "# minimum of 1 for people who did ever attempt - but in addition to being way too complicated\n",
    "# and messy for the amount of time I have left, that runs the risk of dramatically \n",
    "# misrepresenting people's actual relationship with suicidality.  Someone who made one attempt\n",
    "# at 17 because of teen stuff and another, more or less completely unrelated, attempt at 47 \n",
    "# because they were going through a messy divorce would be indicated to have been suicidal \n",
    "# for THIRTY YEARS, which isn't true, and would make it look like they were 6x more impacted \n",
    "# by sucidality than someone who made their first attempt 18 at their last attempt at 23, \n",
    "# even if the latter person made 20 attempts in that time, and they were all due to the same\n",
    "# ongoing issues.  I know I'm probably overthinking this, but to do a range seems so reductive\n",
    "# as to be practically meaningless - and definitely not worth the time, effort, and \n",
    "# dimensionality it would take to accomplish it.  I am going to just drop these age columns.\n",
    "age_cols = ['w1q102_ei', 'w1q103_ei', 'w1q104_ei',  # these are the suicide/nssh age Qs\n",
    "  'w1q106_ei', 'w1q107_ei', 'w1q108_ei', 'w1q110_ei', 'w1q111_ei', 'w1q112_ei', \n",
    "  'w1q115_ei', 'w1q116_ei', 'w1q117_ei', 'w1q120', 'w1q121_ei', 'w1q122_ei',\n",
    "  'w1q134_ei']  # this how old were you when you got conversion therapy \n",
    "len(age_cols)\n",
    "meyer.drop(columns = age_cols, inplace = True)\n",
    "\n",
    "\n",
    "# ('w1q114_ei', [6, 7, 8, 9, 97], 6), \n",
    "# This is the one asking how many suicide attempts people made, and they used a messy scale.\n",
    "# I get why they did it, I probably would have done the same, but it still makes it ordinal\n",
    "# rather than truly linear.  I could leave it as is, or change these values to the minimum\n",
    "# in their range (or something like that).  I could also make it categorical, like\n",
    "# 0 attempts, 1 attempt, multiple attempts.\n",
    "np.linspace(6, 21, 4) # oh it's exactly what they have.\n",
    "\n",
    "# I'm going to make columns for all of those options and see which one correlates best.\n",
    "# Oh wait no I'm not!  All I'm doing with these is adding them up into a composite score.\n",
    "# I'm going to reassign those values to the minimum value in their range so that it's \n",
    "# sort of linear, but then that's enough.  That's fine.\n",
    "cond7 = meyer['w1q114_ei']==7.0\n",
    "meyer.loc[cond7, 'w1q114_ei']=11\n",
    "\n",
    "cond8 = meyer['w1q114_ei']==8.0\n",
    "meyer.loc[cond8, 'w1q114_ei']=16\n",
    "\n",
    "cond9 = meyer['w1q114_ei']==9.0\n",
    "meyer.loc[cond9, 'w1q114_ei']=21\n",
    "\n",
    "\n",
    "# All of the 123 questions are about outness, but they're coded weird. It's are you out to...\n",
    "# 1 = All, 2 = Most, 3 = Some, 4 = None, 5 = don't know/does not apply/[missing value]\n",
    "# So firstly, it's counter-intuitive.  Higher numbers = less out.  Sort of.  Secondly, 5 is\n",
    "# in a weird place on the scale  I think I'm going to recode it like this:\n",
    "# 4 -> 0 = I can confidently say I am out to \"None\" of these people. (LOWEST OUTNESS)\n",
    "# 5 -> 1 = I'm being wishy-washy about how out I am to these people.\n",
    "# 3 -> 2 = Some\n",
    "# 2 -> 3 = Most\n",
    "# 1 -> 4 = All (HIGHEST OUTNESS)\n",
    "# ('w1q123a_ei', [5], 5), ('w1q123b_ei', [5], 5), ('w1q123c_ei', [5], 5), ('w1q123d_ei', [5], 5)]\n",
    "\n",
    "meyer[['w1q123a_ei_r', 'w1q123b_ei_r', 'w1q123c_ei_r', 'w1q123d_ei_r']] = meyer[[\n",
    "  'w1q123a_ei', 'w1q123b_ei', 'w1q123c_ei', 'w1q123d_ei']]\n",
    "\n",
    "recode_123s = {4: 0, 5: 1, 3: 2, 2: 3, 1: 4}\n",
    "\n",
    "old_cols = ['w1q123a_ei', 'w1q123b_ei', 'w1q123c_ei', 'w1q123d_ei']\n",
    "new_cols = ['w1q123a_ei_r', 'w1q123b_ei_r', 'w1q123c_ei_r', 'w1q123d_ei_r']\n",
    "\n",
    "for old, new in list(zip(old_cols, new_cols)):\n",
    "  meyer[new] = meyer[old].map(recode_123s)\n",
    "\n",
    "check = meyer[['w1q123a_ei', 'w1q123a_ei_r', 'w1q123b_ei', 'w1q123b_ei_r', \n",
    "  'w1q123c_ei', 'w1q123c_ei_r', 'w1q123d_ei', 'w1q123d_ei_r']]\n",
    "# Yeah checks out!\n",
    "\n",
    "meyer.drop(columns = old_cols, inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "\n",
    "# I don't remember if I actually *executed* all that code, so let me test it again. I reran \n",
    "# the code to define problem_children just to be safe, and now I'm going to modify the loop.\n",
    "column_jail = []\n",
    "columns_done = []\n",
    "z = 0 # I'm a little hazy on `continue` so this is just a check\n",
    "for c, v in problem_children.items():\n",
    "\n",
    "  # if that column name isn't in the list, find out why\n",
    "  if c not in list(meyer.columns):\n",
    "    if c=='w1q02_ei':\n",
    "      columns_done.append(c)\n",
    "      z+=1\n",
    "      continue\n",
    "  \n",
    "    elif c in age_cols:\n",
    "      columns_done.append(c)\n",
    "      z+=1\n",
    "      continue\n",
    "    \n",
    "    # if neither of those work, try renaming it\n",
    "    c_r = ''.join([c, '_r'])\n",
    "    if c_r not in list(meyer.columns):\n",
    "      print(c)\n",
    "      columns_done.append(c)\n",
    "      z+=1\n",
    "      continue\n",
    "    elif c_r in list(meyer.columns): \n",
    "      c = c_r # rename it and continue thru the loop\n",
    "\n",
    "  # if that column name is in the list, or if c_r is, do this\n",
    "  a = list(meyer[c].unique())\n",
    "  for i in v:\n",
    "    if i in a:\n",
    "      column_jail.append((c, v, i))\n",
    "      break\n",
    "  columns_done.append(c)\n",
    "  z+=1\n",
    "z==len(problem_children.keys())\n",
    "\n",
    "# huh = [c for c in list((problem_children.keys())) if c not in columns_done]\n",
    "# fixed it ^\n",
    "\n",
    "column_jail\n",
    "# oh it's just [('w1q114_ei', [6, 7, 8, 9, 97], 6)]\n",
    "meyer['w1q114_ei'].value_counts(dropna = False)\n",
    "# w1q114_ei\n",
    "# 0.0     1146\n",
    "# 1.0      251\n",
    "# 3.0       32\n",
    "# 2.0       30\n",
    "# 6.0       17\n",
    "# 4.0       14\n",
    "# 5.0        9\n",
    "# 11.0       6\n",
    "# 16.0       1\n",
    "# 21.0       1\n",
    "# Name: count, dtype: int64\n",
    "\n",
    "# it's totally fine, I fixed the thing I wanted to fix with 6+\n",
    "# I should probably indicate that I changed stuff though.\n",
    "\n",
    "meyer.shape # (1507, 233)\n",
    "meyer[['w1q114_ei_r']] = meyer[['w1q114_ei']]\n",
    "meyer.shape\n",
    "meyer.drop(columns = ['w1q114_ei'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "\n",
    "# ok!  Woohoo!  Let's save another copy.\n",
    "meyer.to_csv('2024-06-06_non-NA-NAs-fixed.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 3 in full:\n",
    "2024-06-06_y-variable.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thursday, June 6, 2024\n",
    "\n",
    "# I should probably also choose a y variable.  [several hours latere]  Let's go with\n",
    "# Kessler6!  It's a general mental health scale.  According to the documentation, there were\n",
    "# 1491 complete cases of this, and honestly I'd be fine just dropping the rest.  Let me see\n",
    "# how many missing values there were per row in the imputed versions.  If it's only 1, I \n",
    "# might let it slide.\n",
    "\n",
    "\n",
    "# Reimport the data to get a look at the unimputed columns\n",
    "og_meyer = pd.read_csv(\n",
    "  './potential_datasets/2024_05_23_download_ICPSR_Meyer_2023_generations_data_attempt_2/ICPSR_37166/DS0007/37166-0007-Data.tsv',\n",
    "  sep = '\\t', low_memory=False, na_values = ' ') # Thanks to ibrahim rupawala for highlighting the na_values argument\n",
    "\n",
    "# Pull some stuff out\n",
    "og_meyer_kessler = og_meyer[['STUDYID', 'W1Q77A', 'W1Q77B', 'W1Q77C', 'W1Q77D', 'W1Q77E', 'W1Q77F', 'W1KESSLER6', 'W1KESSLER6_I']]\n",
    "og_meyer_kessler = og_meyer_kessler[og_meyer_kessler['W1KESSLER6'].isna()]\n",
    "\n",
    "# Find the rows that have multiple NAs\n",
    "kes_cols = ['W1Q77A', 'W1Q77B', 'W1Q77C', 'W1Q77D', 'W1Q77E', 'W1Q77F']\n",
    "drop_rows = []\n",
    "for i in og_meyer_kessler.index:\n",
    "  if og_meyer_kessler.loc[i, kes_cols].isna().sum() > 1:\n",
    "    drop_rows.append(og_meyer_kessler.loc[i, 'STUDYID'])\n",
    "\n",
    "# get rid of those rows\n",
    "meyer.shape # (1507, 233)\n",
    "len(drop_rows) # 13\n",
    "# meyer_d = meyer.loc[(meyer['studyid'] not in drop_rows), :]  # goal is (1494, 233)\n",
    "# not working ^\n",
    "\n",
    "drop_index = []\n",
    "for i in drop_rows:\n",
    "  drop_index.append(meyer[meyer['studyid']==i].index[0])\n",
    "check = meyer.loc[drop_index, ['studyid', 'w1kessler6_i']]\n",
    "\n",
    "meyer.drop(index = drop_index, inplace = True)\n",
    "\n",
    "meyer.shape\n",
    "\n",
    "meyer.to_csv('2024-06-06_bad-y-rows-dropped.csv', index = False)\n",
    "\n",
    "# thanks to pandas documentation for this syntax\n",
    "meyer.reset_index(drop = True, inplace = True)\n",
    "meyer.to_csv('2024-06-06_bad-y-rows-dropped_reindexed.csv', index = False)\n",
    "# I checked in the terminal and this version and the previous csv are exactly the same.\n",
    "\n",
    "meyer['w1kessler6_i'].describe()\n",
    "\n",
    "\n",
    "# oh no is it skewed\n",
    "# Plot a histogram of it\n",
    "plt.figure(figsize = (16, 9));\n",
    "plt.hist(meyer['w1kessler6_i'], bins = 'auto', color = 'purple');\n",
    "plt.suptitle(f'Distribution of {Y}', size = 24)\n",
    "# plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "plt.xlabel('w1kessler6_i', size = 20);\n",
    "plt.ylabel('Frequency', size = 20);\n",
    "plt.xticks(size = 16, rotation = 60);\n",
    "plt.yticks(size = 16)\n",
    "#plt.tight_layout()\n",
    "# plt.savefig(f'./{a}/{i}_histogram.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot a histogram of it\n",
    "plt.figure(figsize = (16, 9));\n",
    "plt.hist(meyer['w1kessler6_i']**(1/2), bins = 'auto', color = 'purple');\n",
    "plt.suptitle(f'Distribution of {Y}', size = 24)\n",
    "# plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "plt.xlabel('w1kessler6_i', size = 20);\n",
    "plt.ylabel('Frequency', size = 20);\n",
    "plt.xticks(size = 16, rotation = 60);\n",
    "plt.yticks(size = 16)\n",
    "#plt.tight_layout()\n",
    "# plt.savefig(f'./{a}/{i}_histogram.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot a histogram of it\n",
    "plt.figure(figsize = (16, 9));\n",
    "plt.hist(meyer['w1kessler6_i']**(1/3), bins = 'auto', color = 'purple');\n",
    "plt.suptitle(f'Distribution of {Y}', size = 24)\n",
    "# plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "plt.xlabel('w1kessler6_i', size = 20);\n",
    "plt.ylabel('Frequency', size = 20);\n",
    "plt.xticks(size = 16, rotation = 60);\n",
    "plt.yticks(size = 16)\n",
    "#plt.tight_layout()\n",
    "# plt.savefig(f'./{a}/{i}_histogram.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot a histogram of it\n",
    "\n",
    "# log transformation gets a div by 0 warning\n",
    "# plt.figure(figsize = (16, 9));\n",
    "# plt.hist(np.log(meyer['w1kessler6_i']), bins = 'auto', color = 'purple');\n",
    "# plt.suptitle(f'Distribution of {Y}', size = 24)\n",
    "# # plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "# plt.xlabel('w1kessler6_i', size = 20);\n",
    "# plt.ylabel('Frequency', size = 20);\n",
    "# plt.xticks(size = 16, rotation = 60);\n",
    "# plt.yticks(size = 16)\n",
    "# #plt.tight_layout()\n",
    "# # plt.savefig(f'./{a}/{i}_histogram.png')\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# QQplot of raw kessler\n",
    "sm.qqplot(meyer['w1kessler6_i'], line='45');\n",
    "plt.suptitle(f'QQ-Plot of kessler6_i', size = 20);\n",
    "plt.xticks(size = 14, rotation = 60);\n",
    "plt.yticks(size = 14);\n",
    "plt.tight_layout();\n",
    "# plt.savefig('./03_images/output/dropout_rate_qqplot.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# QQplot of sqrt kessler\n",
    "sm.qqplot((meyer['w1kessler6_i']**0.5), line='45');\n",
    "plt.suptitle(f'QQ-Plot of kessler6_i', size = 20);\n",
    "plt.xticks(size = 14, rotation = 60);\n",
    "plt.yticks(size = 14);\n",
    "plt.tight_layout();\n",
    "# plt.savefig('./03_images/output/dropout_rate_qqplot.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# QQplot of cbrt kessler\n",
    "sm.qqplot((meyer['w1kessler6_i']**(1/3)), line='45');\n",
    "plt.suptitle(f'QQ-Plot of kessler6_i', size = 20);\n",
    "plt.xticks(size = 14, rotation = 60);\n",
    "plt.yticks(size = 14);\n",
    "plt.tight_layout();\n",
    "# plt.savefig('./03_images/output/dropout_rate_qqplot.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Ok, I'll square root it.\n",
    "\n",
    "meyer['kessler6_sqrt'] = (meyer['w1kessler6_i']**0.5)\n",
    "meyer.shape\n",
    "\n",
    "# let's get a copy of that too\n",
    "meyer.to_csv('2024-06-06_sqrt-kessler.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 4 in full:\n",
    "2024-06-06_column_recoding_for_feat_eng_dict_combinations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite Scale Calculations\n",
    "\n",
    "# In this script, I need to go back to `feat_eng_dict`, that list I made in \n",
    "# `2024-05-23_thru_2024-05-30_exploring_Meyer_2023_dataset.py`, and use it to \n",
    "# combine some of these columns into composite columns and drop the rest.\n",
    "\n",
    "unaltered_col_names = [\n",
    "  c.replace('_r', '').replace('_ei', '') for c in list(meyer.columns)]\n",
    "  \n",
    "for k, v in feat_eng_dict.items():\n",
    "  a = v[1:]\n",
    "  for j in a:\n",
    "    if ((j not in unaltered_col_names) & (j not in list(meyer.columns))):\n",
    "      print(k, j)\n",
    "\n",
    "# redefine it here without those\n",
    "feat_eng_dict = {'pers_well_being': ['sum', 'w1q01'],\n",
    "  'neighb_welcoming': ['mean', 'w1q19a', 'w1q19b', 'w1q19c', 'w1q19d'],\n",
    "  'age_awakening': ['min','w1q45', 'w1q46', 'w1q47', 'w1q48'],\n",
    "  'age_out': ['min', 'w1q49', 'w1q50', 'w1q51'],\n",
    "  'health_insurance': ['binarize', 'w1q64_1', 'w1q64_2', 'w1q64_3', 'w1q64_4', \n",
    "    'w1q64_5', 'w1q64_6', 'w1q64_7', 'w1q64_8', 'w1q64_9', 'w1q64_10', \n",
    "    'w1q64_11', 'w1q64_12', 'w1q64_13', 'w1q64_t_num'], \n",
    "  'serious_health_cond': ['binarize', 'w1q74_5', 'w1q74_6', 'w1q74_10', \n",
    "    'w1q74_11', 'w1q74_14', 'w1q74_17', 'w1q74_18', 'w1q74_20'], \n",
    "  'disabled': ['binarize', 'w1q75', 'w1q76'],\n",
    "  'suicidal_ideation': ['sum', 'w1q101', 'w1q105', 'w1q109'], \n",
    "  'suicide_attempts': ['recode', 'w1q113', 'w1q114'],\n",
    "  'outness': ['sum', 'w1q123a', 'w1q123b', 'w1q123c', 'w1q123d', 'w1q124'],\n",
    "  'abusive_treatment': ['sum', 'w1q135a', 'w1q135b', \n",
    "    'w1q135c', 'w1q135d', 'w1q135e', 'w1q135f'],\n",
    "  'work_neg_outcomes': ['recode', 'w1q137', 'w1q138'], # account for age\n",
    "  'abus_treat_non_queer': ['binarize', 'w1q136_1', 'w1q136_5', 'w1q136_6', \n",
    "    'w1q136_8', 'w1q136_9', 'w1q136_10'],\n",
    "  'stress_past_year_gen': ['recode', 'w1q142a', 'w1q142h', 'w1q142i'],\n",
    "  'stress_past_year_work': ['recode', 'w1q142b', 'w1q142c', 'w1q142e'],\n",
    "  'stress_past_year_interpersonal': ['recode', 'w1q142d', 'w1q142f', 'w1q142g'],\n",
    "  'stress_past_year_crime': ['recode', 'w1q142j', 'w1q142k'],\n",
    "  'work_disc_non_queer': ['binarize', 'w1q139_1', 'w1q139_5', 'w1q139_6', \n",
    "    'w1q139_8', 'w1q139_9', 'w1q139_10'],\n",
    "  'housing_disc_non_queer': ['binarize', 'w1q141_1', 'w1q141_5', 'w1q141_6', \n",
    "    'w1q141_8', 'w1q141_9', 'w1q141_10'],\n",
    "  'stress_past_year_non_queer': ['binarize', 'w1q143_1', 'w1q143_5', 'w1q143_6', \n",
    "    'w1q143_8', 'w1q143_9', 'w1q143_10'],\n",
    "  'daily_discr_non_queer': ['binarize', 'w1q145_1', 'w1q145_5', 'w1q145_6'],\n",
    "  'childhd_bullying_non_queer': ['binarize', 'w1q163_1', 'w1q163_5', 'w1q163_6', \n",
    "    'w1q163_8', 'w1q163_9', 'w1q163_10'],\n",
    "  'abus_treat_sex_gender': ['binarize', 'w1q136_2', 'w1q136_3', 'w1q136_4'],\n",
    "  'work_disc_sex_gender': ['binarize', 'w1q139_2', 'w1q139_3', 'w1q139_4'],\n",
    "  'housing_disc_sex_gender': ['binarize', 'w1q141_2', 'w1q141_3', 'w1q141_4'],\n",
    "  'stress_past_year_sex_gender': ['binarize', 'w1q143_2', 'w1q143_3', 'w1q143_4'],\n",
    "  'daily_discr_sex_gender': ['binarize', 'w1q145_2', 'w1q145_3', 'w1q145_4', \n",
    "    'w1q145_8', 'w1q145_9', 'w1q145_10'],\n",
    "  'childhd_bullying_sex_gender': ['binarize', 'w1q163_2', 'w1q163_3', 'w1q163_4'],\n",
    "  'religiosity': ['recode', 'w1q179', 'w1q180', 'w1q181'], \n",
    "  'chronic_strain': ['sum', 'w1q146a', 'w1q146b', 'w1q146c', 'w1q146d', 'w1q146e', \n",
    "    'w1q146f', 'w1q146g', 'w1q146h', 'w1q146i', 'w1q146j', 'w1q146k', 'w1q146l']}\n",
    "\n",
    "# then go back up and check again\n",
    "# got them all!\n",
    "\n",
    "# Now I'm going to go through and find which ones have NOT been altered\n",
    "for k, v in feat_eng_dict.items():\n",
    "  a = v[1:]\n",
    "  for j in a:\n",
    "    if j in list(meyer.columns):\n",
    "      print(k, j)\n",
    "\n",
    "# oh dear just one\n",
    "for k, v in feat_eng_dict.items():\n",
    "  a = v[:1]\n",
    "  b = ['_'.join([x, 'ei']) for x in v[1:]]\n",
    "  c = a + b\n",
    "  feat_eng_dict[k] = c\n",
    "  \n",
    "# manually fix this one\n",
    "feat_eng_dict['health_insurance'] = ['binarize', 'w1q64_1_ei', \n",
    "  'w1q64_2_ei', 'w1q64_3_ei', 'w1q64_4_ei', 'w1q64_5_ei', 'w1q64_6_ei', \n",
    "  'w1q64_7_ei', 'w1q64_8_ei', 'w1q64_9_ei', 'w1q64_10_ei', \n",
    "  'w1q64_11_ei', 'w1q64_12_ei', 'w1q64_13_ei', 'w1q64_t_num']\n",
    "  \n",
    "# and these\n",
    "feat_eng_dict['suicide_attempts'] = ['recode', 'w1q113_ei', 'w1q114_ei_r']\n",
    "feat_eng_dict['outness'] = ['sum', 'w1q123a_ei_r', \n",
    "  'w1q123b_ei_r', 'w1q123c_ei_r', 'w1q123d_ei_r', 'w1q124_ei']\n",
    "  \n",
    "# check again\n",
    "for k, v in feat_eng_dict.items():\n",
    "  a = v[1:]\n",
    "  for j in a:\n",
    "    if j not in list(meyer.columns):\n",
    "      print(k, j)\n",
    "      \n",
    "      \n",
    "for k, v in feat_eng_dict.items():\n",
    "  print(k, v)\n",
    "  print('')\n",
    "\n",
    "feat_eng_dict['health_insurance'] = ['binarize', 'w1q64_2_ei', \n",
    "  'w1q64_3_ei', 'w1q64_4_ei', 'w1q64_5_ei', 'w1q64_6_ei', \n",
    "  'w1q64_7_ei', 'w1q64_8_ei', 'w1q64_9_ei', 'w1q64_10_ei', \n",
    "  'w1q64_11_ei', 'w1q64_12_ei', 'w1q64_13_ei', 'w1q64_t_num']\n",
    "\n",
    "# cut\n",
    "drop_items = ['pers_well_being', 'age_awakening', 'age_out', 'neighb_welcoming']\n",
    "drop_cols = ['w1q45_ei', 'w1q46_ei', 'w1q47_ei', 'w1q48_ei', 'w1q49_ei', 'w1q50_ei', \n",
    "  'w1q51_ei']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# mess with the columns before computing\n",
    "\n",
    "def recode(dictry, name, cols):\n",
    "  '''cols is a list of columns that need to be recoded.  \n",
    "  name is the name I want to give the composite column based on cols\n",
    "  dict is the STRING name of the dictionary (I need that for the func to modify it)\n",
    "    this function will spit out the appropriate syntax, but only for THIS df.'''\n",
    "  for i in cols[1:]:\n",
    "    print(f\"meyer[['{i}_r']] = meyer[['{i}']]\")\n",
    "  print('')\n",
    "  for i in cols[1:]:\n",
    "    print(f\"meyer['{i}'].value_counts(dropna = False).sort_index()\")\n",
    "    print(f\"meyer['{i}_r'].value_counts(dropna = False).sort_index()\")\n",
    "  print('')\n",
    "  print(f\"meyer.drop(columns = {cols[1:]}).shape\")\n",
    "  print(f\"meyer.drop(columns = {cols[1:]}, inplace = True)\")\n",
    "  print('')\n",
    "  cols_r = cols[:1] + [''.join([x, '_r']) for x in cols[1:]]\n",
    "  print(f\"{dictry}['{name}'] = {cols_r}\")\n",
    "# I then ran this a bunch of times in the console\n",
    "\n",
    "# reverse code these so that 1=bad neighborhood and 0=fine\n",
    "meyer[['w1q19a_ei_r']] = meyer[['w1q19a_ei']]-1\n",
    "meyer[['w1q19b_ei_r']] = meyer[['w1q19b_ei']]-1\n",
    "meyer[['w1q19c_ei_r']] = meyer[['w1q19c_ei']]-1\n",
    "meyer[['w1q19d_ei_r']] = meyer[['w1q19d_ei']]-1\n",
    "meyer.drop(columns = ['w1q19a_ei', 'w1q19b_ei', 'w1q19c_ei', 'w1q19d_ei']).shape\n",
    "meyer.drop(columns = ['w1q19a_ei', 'w1q19b_ei', 'w1q19c_ei', 'w1q19d_ei'], inplace = True)\n",
    "feat_eng_dict['bad_neighbhd'] = ['sum', 'w1q19a_ei_r', 'w1q19b_ei_r', 'w1q19c_ei_r', 'w1q19d_ei_r']\n",
    "\n",
    "# reverse code these so that 1=disabled, 0=non-disabled\n",
    "# currently it's 1=disabled, 2=non-disabled\n",
    "# (1-2)*(-1)==1, (2-2)*(-1)==0\n",
    "meyer[['w1q75_ei_r']] = abs(meyer[['w1q75_ei']]-2)\n",
    "meyer[['w1q76_ei_r']] = abs(meyer[['w1q76_ei']]-2)\n",
    "\n",
    "meyer['w1q75_ei'].value_counts(dropna = False, sort = True, ascending = True)\n",
    "meyer['w1q75_ei_r'].value_counts(dropna = False, sort = True, ascending = True)\n",
    "meyer['w1q76_ei'].value_counts(dropna = False, sort = True, ascending = True)\n",
    "meyer['w1q76_ei_r'].value_counts(dropna = False, sort = True, ascending = True)\n",
    "\n",
    "meyer.drop(columns = ['w1q75_ei', 'w1q76_ei']).shape\n",
    "meyer.drop(columns = ['w1q75_ei', 'w1q76_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['disabled'] = ['binarize', 'w1q75_ei_r', 'w1q76_ei_r']\n",
    "\n",
    "# recode these to eliminate the 0 (created during imputation), then 0-base\n",
    "meyer[['w1q101_ei_r']] = meyer[['w1q101_ei']]\n",
    "meyer[['w1q105_ei_r']] = meyer[['w1q105_ei']]\n",
    "meyer[['w1q109_ei_r']] = meyer[['w1q109_ei']]\n",
    "\n",
    "for i in ['sum', 'w1q101_ei', 'w1q105_ei', 'w1q109_ei']:\n",
    "  print(f\"cond = meyer['{i}']==0\")\n",
    "cond1 = meyer['w1q101_ei']==0\n",
    "cond2 = meyer['w1q105_ei']==0\n",
    "cond3 = meyer['w1q109_ei']==0\n",
    "\n",
    "meyer.loc[cond1, 'w1q101_ei_r'] = 1\n",
    "meyer.loc[cond2, 'w1q105_ei_r'] = 1\n",
    "meyer.loc[cond3, 'w1q109_ei_r'] = 1\n",
    "\n",
    "meyer['w1q101_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q101_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q105_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q105_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q109_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q109_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q101_ei', 'w1q105_ei', 'w1q109_ei']).shape\n",
    "meyer.drop(columns = ['w1q101_ei', 'w1q105_ei', 'w1q109_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['suicidal_ideation'] = ['sum', 'w1q101_ei_r', 'w1q105_ei_r', 'w1q109_ei_r']\n",
    "\n",
    "# ack I forgot to 0-base\n",
    "# recode these to eliminate the 0 (created during imputation), then 0-base\n",
    "meyer[['w1q101_ei_r']] = meyer[['w1q101_ei_r']]-1\n",
    "meyer[['w1q105_ei_r']] = meyer[['w1q105_ei_r']]-1\n",
    "meyer[['w1q109_ei_r']] = meyer[['w1q109_ei_r']]-1\n",
    "\n",
    "# same for these\n",
    "# actually, do I need both of these?\n",
    "\n",
    "cond1 = meyer['w1q113_ei']>1\n",
    "cond2 = meyer['w1q114_ei_r']==0\n",
    "\n",
    "check = meyer.loc[(cond1 & cond2), ['w1q113_ei', 'w1q114_ei_r']]\n",
    "# both times it's 3, 0\n",
    "# I'm guessing they didn't want to say how many\n",
    "# I'm going to impute 2 for them.\n",
    "meyer.loc[(cond1 & cond2), 'w1q114_ei_r'] = 2\n",
    "\n",
    "# And with that done, the first column is redundant!\n",
    "# This entry in the dictionary now only has one column in its\n",
    "# composite, and I actually think it would be better to combine\n",
    "# it with the previous one, for an overall 'suicidality' column\n",
    "\n",
    "meyer.shape\n",
    "meyer[['w1q113_ei_r']] = meyer[['w1q113_ei']]\n",
    "# meyer[['w1q114_ei_r_r']] = meyer[['w1q114_ei_r']]\n",
    "\n",
    "meyer['w1q113_ei'].value_counts(dropna = False).sort_index()\n",
    "# w1q113_ei\n",
    "# 0.0       9\n",
    "# 1.0    1122\n",
    "# 2.0     251\n",
    "# 3.0     112\n",
    "\n",
    "# meyer['w1q113_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q114_ei_r'].value_counts(dropna = False).sort_index()\n",
    "# w1q114_ei_r\n",
    "# 0.0     1131\n",
    "# 1.0      251\n",
    "# [etc.]   112   # I manually added them up\n",
    "\n",
    "# meyer['w1q114_ei_r_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q113_ei']).shape\n",
    "meyer.drop(columns = ['w1q113_ei'], inplace = True)\n",
    "\n",
    "# feat_eng_dict['suicide_attempts'] = ['recode', 'w1q113_ei_r', 'w1q114_ei_r_r']\n",
    "drop_items.append('suicidal_ideation')\n",
    "drop_items.append('suicide_attempts')\n",
    "feat_eng_dict['suicidality'] = ['sum', 'w1q101_ei_r', 'w1q105_ei_r', 'w1q109_ei_r', 'w1q114_ei_r']\n",
    "\n",
    "\n",
    "# I already got the 123s, but I need to reverse code 124\n",
    "# here's what it is now -> what I want it to be\n",
    "# MOST VISIBLE\n",
    "# 1 -> \n",
    "# 2 -> \n",
    "# 3 -> \n",
    "# 4 -> 1\n",
    "# 5 -> 0\n",
    "# LEAST VISIBLE\n",
    "# oh it's just 5 minus thing\n",
    "\n",
    "meyer[['w1q124_ei_r']] = 5-meyer[['w1q124_ei']]\n",
    "\n",
    "meyer['w1q123a_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q123b_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q123c_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q123d_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q124_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q124_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q124_ei']).shape\n",
    "meyer.drop(columns = ['w1q124_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['outness'] = ['mean', 'w1q123a_ei_r', 'w1q123b_ei_r', \n",
    "  'w1q123c_ei_r', 'w1q123d_ei_r', 'w1q124_ei_r']\n",
    "\n",
    "# these guys I just need to 0-base\n",
    "meyer[['w1q135a_ei_r']] = meyer[['w1q135a_ei']]-1\n",
    "meyer[['w1q135b_ei_r']] = meyer[['w1q135b_ei']]-1\n",
    "meyer[['w1q135c_ei_r']] = meyer[['w1q135c_ei']]-1\n",
    "meyer[['w1q135d_ei_r']] = meyer[['w1q135d_ei']]-1\n",
    "meyer[['w1q135e_ei_r']] = meyer[['w1q135e_ei']]-1\n",
    "meyer[['w1q135f_ei_r']] = meyer[['w1q135f_ei']]-1\n",
    "\n",
    "meyer['w1q135a_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135a_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135b_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135b_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135c_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135c_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135d_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135d_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135e_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135e_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135f_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135f_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q135a_ei', 'w1q135b_ei', 'w1q135c_ei', 'w1q135d_ei', 'w1q135e_ei', 'w1q135f_ei']).shape\n",
    "meyer.drop(columns = ['w1q135a_ei', 'w1q135b_ei', 'w1q135c_ei', 'w1q135d_ei', 'w1q135e_ei', 'w1q135f_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['abusive_treatment'] = ['sum', 'w1q135a_ei_r', 'w1q135b_ei_r', 'w1q135c_ei_r', 'w1q135d_ei_r', 'w1q135e_ei_r', 'w1q135f_ei_r']\n",
    "\n",
    "# same \n",
    "meyer[['w1q137_ei_r']] = meyer[['w1q137_ei']]-1\n",
    "meyer[['w1q138_ei_r']] = meyer[['w1q138_ei']]-1\n",
    "\n",
    "meyer['w1q137_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q137_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q138_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q138_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q137_ei', 'w1q138_ei']).shape\n",
    "meyer.drop(columns = ['w1q137_ei', 'w1q138_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['work_neg_outcomes'] = ['sum', 'w1q137_ei_r', 'w1q138_ei_r']\n",
    "\n",
    "# recode code these guys so 1=stress and 0=not\n",
    "meyer[['w1q142a_ei_r']] = abs(meyer[['w1q142a_ei']]-2)\n",
    "meyer[['w1q142h_ei_r']] = abs(meyer[['w1q142h_ei']]-2)\n",
    "meyer[['w1q142i_ei_r']] = abs(meyer[['w1q142i_ei']]-2)\n",
    "\n",
    "meyer['w1q142a_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142a_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142h_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142h_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142i_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142i_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q142a_ei', 'w1q142h_ei', 'w1q142i_ei']).shape\n",
    "meyer.drop(columns = ['w1q142a_ei', 'w1q142h_ei', 'w1q142i_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['stress_past_year_gen'] = ['sum', 'w1q142a_ei_r', 'w1q142h_ei_r', 'w1q142i_ei_r']\n",
    "\n",
    "# these too\n",
    "meyer[['w1q142b_ei_r']] = abs(meyer[['w1q142b_ei']]-2)\n",
    "meyer[['w1q142c_ei_r']] = abs(meyer[['w1q142c_ei']]-2)\n",
    "meyer[['w1q142e_ei_r']] = abs(meyer[['w1q142e_ei']]-2)\n",
    "\n",
    "meyer['w1q142b_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142b_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142c_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142c_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142e_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142e_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q142b_ei', 'w1q142c_ei', 'w1q142e_ei']).shape\n",
    "meyer.drop(columns = ['w1q142b_ei', 'w1q142c_ei', 'w1q142e_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['stress_past_year_work'] = ['sum', 'w1q142b_ei_r', 'w1q142c_ei_r', 'w1q142e_ei_r']\n",
    "\n",
    "# and these\n",
    "meyer[['w1q142d_ei_r']] = abs(meyer[['w1q142d_ei']]-2)\n",
    "meyer[['w1q142f_ei_r']] = abs(meyer[['w1q142f_ei']]-2)\n",
    "meyer[['w1q142g_ei_r']] = abs(meyer[['w1q142g_ei']]-2)\n",
    "\n",
    "meyer['w1q142d_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142d_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142f_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142f_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142g_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142g_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q142d_ei', 'w1q142f_ei', 'w1q142g_ei']).shape\n",
    "meyer.drop(columns = ['w1q142d_ei', 'w1q142f_ei', 'w1q142g_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['stress_past_year_interpersonal'] = ['sum', 'w1q142d_ei_r', 'w1q142f_ei_r', 'w1q142g_ei_r']\n",
    "\n",
    "# and these\n",
    "meyer[['w1q142j_ei_r']] = abs(meyer[['w1q142j_ei']]-2)\n",
    "meyer[['w1q142k_ei_r']] = abs(meyer[['w1q142k_ei']]-2)\n",
    "\n",
    "meyer['w1q142j_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142j_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142k_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142k_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q142j_ei', 'w1q142k_ei']).shape\n",
    "meyer.drop(columns = ['w1q142j_ei', 'w1q142k_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['stress_past_year_crime'] = ['sum', 'w1q142j_ei_r', 'w1q142k_ei_r']\n",
    "\n",
    "# religiosity \n",
    "\n",
    "# here's what 179 is now; 1 and 2 are massive in 180\n",
    "# 1 Protestant (for example, Baptist, Methodist) 295 19.4 %\n",
    "# 2 Roman Catholic 133 8.8 %\n",
    "# 3 Mormon (Church of Jesus Christ of Latter-day Saints or LDS) 10 0.7 %\n",
    "# 4 Orthodox (Greek, Russian, or another Orthodox church) 6 0.4 %\n",
    "\n",
    "# 5 Jewish 38 2.5 %\n",
    "# 6 Muslim 3 0.2 %\n",
    "# 7 Buddhist 30 2.0 %\n",
    "# 8 Hindu 1 0.1 %\n",
    "# 11 Spiritual 262 17.3 %\n",
    "# 12 Something else 95 6.3 %\n",
    "\n",
    "# 9 Atheist (do not believe in God) 192 12.6 %\n",
    "# 10 Agnostic (not sure if there is a God) 156 10.3 %\n",
    "# 13 Nothing in particular 273 18.0 %\n",
    "\n",
    "# collapse 3-8 into \"other organized\"\n",
    "# .... actually collapse more, bcz this needs to be OHE'd\n",
    "\n",
    "# 1-4 christian-influenced religious     1\n",
    "# 5-8, 11-12  non-christian religious    5\n",
    "# 9-10, 13 not religious                 make this 0, because it's literally none\n",
    "\n",
    "relig_recode = {1: 1, 2: 1, 3: 1, 4: 1, # vaguely christian\n",
    "  5: 5, 6: 5, 7: 5, 8: 5, 11: 5, 12: 5, # religious but not christian\n",
    "  9: 0, 10: 0, 13: 0} # not religious\n",
    "\n",
    "# 179 and 180 are the \"are you religious / were you raised religious\" Qs\n",
    "meyer['w1q179_ei_r'] = meyer['w1q179_ei'].map(relig_recode)\n",
    "meyer['w1q180_ei_r'] = meyer['w1q180_ei'].map(relig_recode)\n",
    "\n",
    "meyer['w1q179_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q179_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q180_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q180_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q179_ei', 'w1q180_ei']).shape\n",
    "meyer.drop(columns = ['w1q179_ei', 'w1q180_ei'], inplace = True)\n",
    "\n",
    "# I actually cannot think of a meaningful way to combine these.I think the \n",
    "# thing to do is just leave them all in alone, then see what pops as meaningful.\n",
    "\n",
    "# feat_eng_dict['religiosity'] = ['recode', 'w1q179_ei_r', 'w1q180_ei_r', 'w1q181_ei_r']\n",
    "drop_items.append('religiosity')\n",
    "\n",
    "\n",
    "# 181 is about how often you attend religious services\n",
    "meyer[['w1q181_ei_r']] = 6-(meyer[['w1q181_ei']])\n",
    "\n",
    "meyer['w1q181_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q181_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q181_ei']).shape\n",
    "meyer.drop(columns = ['w1q181_ei'], inplace = True)\n",
    "\n",
    "# woohoo!  save it to a csv\n",
    "meyer.to_csv('2024-06-06_recodes-for-feat-eng-dict-done_not-ohe-yet.csv', index = False)\n",
    "\n",
    "# just ohe them now while I'm thinking about it\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ohe = OneHotEncoder(drop = None, # I want to manually drop a specific one\n",
    "  handle_unknown = 'ignore', sparse_output = False) \n",
    "\n",
    "ctx = ColumnTransformer(transformers=[('one_hot', ohe, ['w1q179_ei_r', 'w1q180_ei_r'])],\n",
    "    remainder = 'passthrough', verbose_feature_names_out=False)\n",
    "\n",
    "meyer_ohe = pd.DataFrame(data = ctx.fit_transform(meyer), \n",
    "  columns = ctx.get_feature_names_out())\n",
    "meyer_ohe.shape\n",
    "\n",
    "# drop the non-religious ones\n",
    "meyer_ohe.drop(columns = ['w1q179_ei_r_0', 'w1q180_ei_r_0']).shape\n",
    "meyer_ohe.drop(columns = ['w1q179_ei_r_0', 'w1q180_ei_r_0'], inplace = True)\n",
    "\n",
    "relig_rename = {'w1q179_ei_r_1': 'w1q179_ei_r_relig_christ', \n",
    "  'w1q179_ei_r_5': 'w1q179_ei_r_relig_other', \n",
    "  'w1q180_ei_r_1': 'w1q180_ei_r_relig_christ', \n",
    "  'w1q180_ei_r_5': 'w1q180_ei_r_relig_other'}\n",
    "\n",
    "meyer_ohe.rename(columns = relig_rename, inplace = True)\n",
    "\n",
    "# put it back in the right name\n",
    "meyer = meyer_ohe.copy(deep = True)\n",
    "\n",
    "# drop the columns I set aside before\n",
    "meyer.drop(columns = drop_cols, inplace = True)\n",
    "\n",
    "# save to csv\n",
    "meyer.to_csv('2024-06-06_recodes-for-feat-eng-dict-done_religious-ohe-done.csv', index = False)\n",
    "\n",
    "# finish updating the feat_eng_list\n",
    "\n",
    "# safety first\n",
    "feat_eng_backup = feat_eng_dict.copy()\n",
    "\n",
    "for i in drop_items:\n",
    "  del feat_eng_dict[i]\n",
    "# thanks to stack overflow for the knowledge that that's how to del dict entries\n",
    "# https://stackoverflow.com/questions/5844672/delete-an-element-from-a-dictionary\n",
    "\n",
    "# oh one more thing\n",
    "# Reorder the columns\n",
    "ordered_cols = sorted(list(meyer.columns))\n",
    "ordered_cols.remove('studyid')\n",
    "ordered_cols = ['studyid'] + ordered_cols\n",
    "len(ordered_cols) # 228\n",
    "len(list(meyer.columns)) # 228\n",
    "meyer.shape # (1494, 228)\n",
    "\n",
    "meyer = meyer[ordered_cols]\n",
    "meyer.shape\n",
    "\n",
    "# save to csv\n",
    "meyer.to_csv('2024-06-06_recodes-for-feat-eng-dict-done_religious-ohe-done_reordered.csv', index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 5 in full:\n",
    "2024-06-06_updated_feat_eng_dict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thursday, June 6, 2024 - End of day\n",
    "# updated version of my feat_eng_dict\n",
    "\n",
    "feat_eng_dict = {\n",
    "  'health_insurance': [\n",
    "    'binarize', 'w1q64_2_ei', 'w1q64_3_ei', 'w1q64_4_ei', 'w1q64_5_ei', \n",
    "    'w1q64_6_ei', 'w1q64_7_ei', 'w1q64_8_ei', 'w1q64_9_ei', 'w1q64_10_ei', \n",
    "    'w1q64_11_ei', 'w1q64_12_ei', 'w1q64_13_ei', 'w1q64_t_num'], \n",
    "  'serious_health_cond': [\n",
    "    'binarize', 'w1q74_5_ei', 'w1q74_6_ei', 'w1q74_10_ei', 'w1q74_11_ei', \n",
    "    'w1q74_14_ei', 'w1q74_17_ei', 'w1q74_18_ei', 'w1q74_20_ei'], \n",
    "  'disabled': [\n",
    "    'binarize', 'w1q75_ei_r', 'w1q76_ei_r'], \n",
    "  'outness': [\n",
    "    'mean', 'w1q123a_ei_r', 'w1q123b_ei_r', 'w1q123c_ei_r', 'w1q123d_ei_r',\n",
    "    'w1q124_ei_r'], \n",
    "  'abusive_treatment': [\n",
    "    'sum', 'w1q135a_ei_r', 'w1q135b_ei_r', 'w1q135c_ei_r', 'w1q135d_ei_r', \n",
    "    'w1q135e_ei_r', 'w1q135f_ei_r'], \n",
    "  'work_neg_outcomes': [\n",
    "    'sum', 'w1q137_ei_r', 'w1q138_ei_r'], \n",
    "  'abus_treat_non_queer': [\n",
    "    'binarize', 'w1q136_1_ei', 'w1q136_5_ei', 'w1q136_6_ei', 'w1q136_8_ei', \n",
    "    'w1q136_9_ei', 'w1q136_10_ei'], \n",
    "  'stress_past_year_gen': [\n",
    "    'sum', 'w1q142a_ei_r', 'w1q142h_ei_r', 'w1q142i_ei_r'], \n",
    "  'stress_past_year_work': [\n",
    "    'sum', 'w1q142b_ei_r', 'w1q142c_ei_r', 'w1q142e_ei_r'], \n",
    "  'stress_past_year_interpersonal': [\n",
    "    'sum', 'w1q142d_ei_r', 'w1q142f_ei_r', 'w1q142g_ei_r'], \n",
    "  'stress_past_year_crime': [\n",
    "    'sum', 'w1q142j_ei_r', 'w1q142k_ei_r'], \n",
    "  'work_disc_non_queer': [\n",
    "    'binarize', 'w1q139_1_ei', 'w1q139_5_ei', 'w1q139_6_ei', 'w1q139_8_ei', \n",
    "    'w1q139_9_ei', 'w1q139_10_ei'], \n",
    "  'housing_disc_non_queer': [\n",
    "    'binarize', 'w1q141_1_ei', 'w1q141_5_ei', 'w1q141_6_ei', 'w1q141_8_ei', \n",
    "    'w1q141_9_ei', 'w1q141_10_ei'], \n",
    "  'stress_past_year_non_queer': [\n",
    "    'binarize', 'w1q143_1_ei', 'w1q143_5_ei', 'w1q143_6_ei', 'w1q143_8_ei', \n",
    "    'w1q143_9_ei', 'w1q143_10_ei'], \n",
    "  'daily_discr_non_queer': [\n",
    "    'binarize', 'w1q145_1_ei', 'w1q145_5_ei', 'w1q145_6_ei'], \n",
    "  'childhd_bullying_non_queer': [\n",
    "    'binarize', 'w1q163_1_ei', 'w1q163_5_ei', 'w1q163_6_ei', 'w1q163_8_ei', \n",
    "    'w1q163_9_ei', 'w1q163_10_ei'], \n",
    "  'abus_treat_sex_gender': [\n",
    "    'binarize', 'w1q136_2_ei', 'w1q136_3_ei', 'w1q136_4_ei'], \n",
    "  'work_disc_sex_gender': [\n",
    "    'binarize', 'w1q139_2_ei', 'w1q139_3_ei', 'w1q139_4_ei'], \n",
    "  'housing_disc_sex_gender': [\n",
    "    'binarize', 'w1q141_2_ei', 'w1q141_3_ei', 'w1q141_4_ei'], \n",
    "  'stress_past_year_sex_gender': [\n",
    "    'binarize', 'w1q143_2_ei', 'w1q143_3_ei', 'w1q143_4_ei'], \n",
    "  'daily_discr_sex_gender': [\n",
    "    'binarize', 'w1q145_2_ei', 'w1q145_3_ei', 'w1q145_4_ei', 'w1q145_8_ei', \n",
    "    'w1q145_9_ei', 'w1q145_10_ei'], \n",
    "  'childhd_bullying_sex_gender': [\n",
    "    'binarize', 'w1q163_2_ei', 'w1q163_3_ei', 'w1q163_4_ei'], \n",
    "  'chronic_strain': [\n",
    "    'sum', 'w1q146a_ei', 'w1q146b_ei', 'w1q146c_ei', 'w1q146d_ei', 'w1q146e_ei', \n",
    "    'w1q146f_ei', 'w1q146g_ei', 'w1q146h_ei', 'w1q146i_ei', 'w1q146j_ei', \n",
    "    'w1q146k_ei', 'w1q146l_ei'], \n",
    "  'bad_neighbhd': [\n",
    "    'sum', 'w1q19a_ei_r', 'w1q19b_ei_r', 'w1q19c_ei_r', 'w1q19d_ei_r'], \n",
    "  'suicidality': ['sum', 'w1q101_ei_r', 'w1q105_ei_r', 'w1q109_ei_r', 'w1q114_ei_r']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Friday, June 7, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "I met with Hank and Alanna today, and they said it looked to them like I was in a good spot.  That's very encouraging!\n",
    "\n",
    "I created a new set-up file today, with an updated CSV and package imports.  I plan to do the same tomorrow.\n",
    "\n",
    "Today I finally got to run my `feat_eng_dict`!  I made some changes to it, and just summed up a lot of the things I was originally going to binarize.  I wanted to get a good look at them to see if there were differences based on the extent to which whatever the thing is has happened to someone, and in a handful of cases there were, but for the most part I think I would have been better off binarizing them.  I will try to do that in the future.  **Throughout the course of this evening, I have been thinking that I have to treat all of these columns the same way.  At this moment, I no longer think that's true.**  I think it's probably fine to say, \"I looked at these guys, and some of them had magnitude effects and some of them didn't.  I binarized the ones that didn't, and kept the others.\"\n",
    "\n",
    "I edited my autoplots function (`autoplots_v2.py`) to include qqplots.  This makes it take even longer to run and the output folder even more comically large, but it is handy.\n",
    "\n",
    "I then conducted some exploratory data analysis, mostly through the production of graphs.  These graphs can be found in the images folder.  In addition to the frequency distributions of each variable and their relationship (shown through scatterplots) with the target variable, I also created qqplots for each variable, and for 3 transformed versions of each variable.  The purpose of this was to assess each variable's normality, and determine whether any of them would benefit from a linear transformation.  I ultimately decided that this was not likely to useful for any variable other than the target, which is the square root of the Kessler scores.\n",
    "\n",
    "Look at these plots also revealed a number of variables that still need to be one-hot encoded, 0-based, or otherwise modified.  I would also like to rename the columns to be more intuitive, so that I don't have to contiually refer to the documentation.  I opted not to do all that before running my first model, but it would be good to do it before including any of those variables in the model.  I started several lists to do these cleaning tasks, but did not finish any of them.  These lists can be found in `2024-06-07-eda.py`, `2024-06-07_NEW_feat_eng_dict.py`, and `2024-06-07_columns-i-want-to-screw-with-some-more-and-how.py`.  When I have time to address these cleaning issues, I intend to rename and modify these scripts.\n",
    "\n",
    "Finally, I created my first model!  I flipped through the scatterplots and just eyeballed some predictors that seemed reasonable, and put them all into a linear regression.  I conducted a train-test split for a little bit of a check on the model, to get a sense of how overfit it might be, but I kept the test size small because this model is mostly for inference.  I am not concerned about its applicability to new data in the future, because newer data is likely to be legitimately different than this data from 2016.  I do, however, want to keep an eye on overfitting, especially as I add more predictors to my model.  Dimensionality has been a concern throughout this entire process, and I am wary of allowing my model to coil itself too tightly around the data.  Although I am not concerned about this model generalizing perfectly to future data, I also do not want it to be so bespoke to the current data that it becomes unsuitable for interpretation.  In other words, if it becomes so overfit that it cannot even generalize to testing data from the same dataset, then its coefficients are unlikely to be meaningful for any level of analysis.\n",
    "\n",
    "I did some cross validation because it seemed like a good idea to demonstrate that I know how to do that, although without gridsearching over anything, I'm not sure it's really necessary.\n",
    "\n",
    "I used several metrics to evaluate my model, and I think it's a decent model for the area.  $R^2$ was around 0.6 for both training and testing, and all of the loss functions were <1.  (This makes sense, because the y-variable itself is a square root.  Squaring a float <1 will make it smaller, not larger.)  This implies that my model is usually within a range of +/-1 with its predictions of the square root of Kessler scores (range: 0-4.9), and within +/-0.5 for the re-squared values (range: 0-24).\n",
    "\n",
    "I honestly can't believe how good this thing is for being so quickly slapped together.\n",
    "\n",
    "The most up to date CSV is **2024-06-07_h21-m19-s52_end-of-day.csv**\n",
    "\n",
    "\n",
    "CSVs created (in the main project folder):\n",
    "2024-06-07_column-combination-done_drops-done_reordered.csv\n",
    "2024-06-07_column-combination-done_no-drops_not-reordered.csv\n",
    "2024-06-07_h17-m36-s35_column-combination-done_no-drops_not-reordered_FIXED.csv\n",
    "2024-06-07_h17-m37-s11_column-combination-done_drops-done_reordered_FIXED.csv\n",
    "2024-06-07_h21-m19-s52_end-of-day.csv\n",
    "\n",
    "Other files created:\n",
    "- 04_scratch_work/01_notes/2024-06-06_value-counts-for-problem-children.txt (This one was actually *created* yesterday, but I forgot to save it somehow and it reappeared, unnamed, when I opened Rstudio this morning.)\n",
    "- 2 folders worth of plots in `dsb318-capstone/03_images/02_all_graphs/`\n",
    "\n",
    "`.py` files created: \n",
    "1. 2024-06-07_set-up.py\n",
    "2. 2024-06-07_updated_feat_eng_dict.py\n",
    "3. 2024-06-07_column-combination-at-last.py\n",
    "4. 2024-06-07-eda.py\n",
    "5. 2024-06-07_NEW_feat_eng_dict.py\n",
    "6. 2024-06-07_columns-i-want-to-screw-with-some-more-and-how.py\n",
    "7. 2024-06-07_modeling-part-1.py\n",
    "8. autoplots_v2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 1 in full:\n",
    "1. 2024-06-07_set-up.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capstone Setup\n",
    "\n",
    "# Module Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, date, time\n",
    "from string import capwords\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Working Directory\n",
    "os.getcwd()\n",
    "#os.chdir('C:/Users/emily/Git_Stuff/General_Assembly/04_Projects/project-capstone')\n",
    "os.listdir()\n",
    "\n",
    "# Set these right off the bat\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "# Import Data\n",
    "\n",
    "# THIS IS THE ORIGINAL\n",
    "# meyer = pd.read_csv(\n",
    "#   './potential_datasets/2024_05_23_download_ICPSR_Meyer_2023_generations_data_attempt_2/ICPSR_37166/DS0007/37166-0007-Data.tsv', \n",
    "#   sep = '\\t', low_memory=False, na_values = ' ') # Thanks to ibrahim rupawala for highlighting the na_values argument\n",
    "#   # https://stackoverflow.com/questions/13445241/replacing-blank-values-white-space-with-nan-in-pandas/47105408#47105408\n",
    "\n",
    "# THIS IS THE MOST UP TO DATE VERSION FOR WORKING ON\n",
    "meyer = pd.read_csv('2024-06-06_recodes-for-feat-eng-dict-done_religious-ohe-done_reordered.csv')\n",
    "meyer.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 2 in full:\n",
    "2. 2024-06-07_updated_feat_eng_dict.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thursday, June 6, 2024 - End of day\n",
    "# updated version of my feat_eng_dict\n",
    "\n",
    "feat_eng_dict = {\n",
    "  'health_insurance': [\n",
    "    'binarize', 'w1q64_2_ei', 'w1q64_3_ei', 'w1q64_4_ei', 'w1q64_5_ei', \n",
    "    'w1q64_6_ei', 'w1q64_7_ei', 'w1q64_8_ei', 'w1q64_9_ei', 'w1q64_10_ei', \n",
    "    'w1q64_11_ei', 'w1q64_12_ei', 'w1q64_13_ei', 'w1q64_t_num'], \n",
    "  'serious_health_cond': [\n",
    "    'binarize', 'w1q74_5_ei', 'w1q74_6_ei', 'w1q74_10_ei', 'w1q74_11_ei', \n",
    "    'w1q74_14_ei', 'w1q74_17_ei', 'w1q74_18_ei', 'w1q74_20_ei'], \n",
    "  'disabled': [\n",
    "    'binarize', 'w1q75_ei_r', 'w1q76_ei_r'], \n",
    "  'outness': [\n",
    "    'mean', 'w1q123a_ei_r', 'w1q123b_ei_r', 'w1q123c_ei_r', 'w1q123d_ei_r',\n",
    "    'w1q124_ei_r'], \n",
    "  'abusive_treatment': [\n",
    "    'sum', 'w1q135a_ei_r', 'w1q135b_ei_r', 'w1q135c_ei_r', 'w1q135d_ei_r', \n",
    "    'w1q135e_ei_r', 'w1q135f_ei_r'], \n",
    "  'work_neg_outcomes': [\n",
    "    'sum', 'w1q137_ei_r', 'w1q138_ei_r'], \n",
    "  'abus_treat_non_queer': [\n",
    "    'sum', 'w1q136_1_ei', 'w1q136_5_ei', 'w1q136_6_ei', 'w1q136_8_ei', \n",
    "    'w1q136_9_ei', 'w1q136_10_ei'], \n",
    "  'stress_past_year_gen': [\n",
    "    'sum', 'w1q142a_ei_r', 'w1q142h_ei_r', 'w1q142i_ei_r'], \n",
    "  'stress_past_year_work': [\n",
    "    'sum', 'w1q142b_ei_r', 'w1q142c_ei_r', 'w1q142e_ei_r'], \n",
    "  'stress_past_year_interpersonal': [\n",
    "    'sum', 'w1q142d_ei_r', 'w1q142f_ei_r', 'w1q142g_ei_r'], \n",
    "  'stress_past_year_crime': [\n",
    "    'sum', 'w1q142j_ei_r', 'w1q142k_ei_r'], \n",
    "  'work_disc_non_queer': [\n",
    "    'sum', 'w1q139_1_ei', 'w1q139_5_ei', 'w1q139_6_ei', 'w1q139_8_ei', \n",
    "    'w1q139_9_ei', 'w1q139_10_ei'], \n",
    "  'housing_disc_non_queer': [\n",
    "    'sum', 'w1q141_1_ei', 'w1q141_5_ei', 'w1q141_6_ei', 'w1q141_8_ei', \n",
    "    'w1q141_9_ei', 'w1q141_10_ei'], \n",
    "  'stress_past_year_non_queer': [\n",
    "    'sum', 'w1q143_1_ei', 'w1q143_5_ei', 'w1q143_6_ei', 'w1q143_8_ei', \n",
    "    'w1q143_9_ei', 'w1q143_10_ei'], \n",
    "  'daily_discr_non_queer': [\n",
    "    'sum', 'w1q145_1_ei', 'w1q145_5_ei', 'w1q145_6_ei'], \n",
    "  'childhd_bullying_non_queer': [\n",
    "    'sum', 'w1q163_1_ei', 'w1q163_5_ei', 'w1q163_6_ei', 'w1q163_8_ei', \n",
    "    'w1q163_9_ei', 'w1q163_10_ei'], \n",
    "  'abus_treat_sex_gender': [\n",
    "    'sum', 'w1q136_2_ei', 'w1q136_3_ei', 'w1q136_4_ei'], \n",
    "  'work_disc_sex_gender': [\n",
    "    'sum', 'w1q139_2_ei', 'w1q139_3_ei', 'w1q139_4_ei'], \n",
    "  'housing_disc_sex_gender': [\n",
    "    'sum', 'w1q141_2_ei', 'w1q141_3_ei', 'w1q141_4_ei'], \n",
    "  'stress_past_year_sex_gender': [\n",
    "    'sum', 'w1q143_2_ei', 'w1q143_3_ei', 'w1q143_4_ei'], \n",
    "  'daily_discr_sex_gender': [\n",
    "    'sum', 'w1q145_2_ei', 'w1q145_3_ei', 'w1q145_4_ei', 'w1q145_8_ei', \n",
    "    'w1q145_9_ei', 'w1q145_10_ei'], \n",
    "  'childhd_bullying_sex_gender': [\n",
    "    'sum', 'w1q163_2_ei', 'w1q163_3_ei', 'w1q163_4_ei'], \n",
    "  'chronic_strain': [\n",
    "    'sum', 'w1q146a_ei', 'w1q146b_ei', 'w1q146c_ei', 'w1q146d_ei', 'w1q146e_ei', \n",
    "    'w1q146f_ei', 'w1q146g_ei', 'w1q146h_ei', 'w1q146i_ei', 'w1q146j_ei', \n",
    "    'w1q146k_ei', 'w1q146l_ei'], \n",
    "  'bad_neighbhd': [\n",
    "    'sum', 'w1q19a_ei_r', 'w1q19b_ei_r', 'w1q19c_ei_r', 'w1q19d_ei_r'], \n",
    "  'suicidality': ['sum', 'w1q101_ei_r', 'w1q105_ei_r', 'w1q109_ei_r', 'w1q114_ei_r']}\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 3 in full:\n",
    "3. 2024-06-07_column-combination-at-last.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friday, June 7, 2024\n",
    "# Feature Engineering!  Whoo!\n",
    "\n",
    "# In this script, I will finally execute my feat_eng_dict to create\n",
    "# composite score columns and drop the individual parts.\n",
    "\n",
    "# I want to look at the values in each variable and triple check \n",
    "# that they're ready to go and that the method is appropriate.\n",
    "for k, v in feat_eng_dict.items():\n",
    "  print('='*20)\n",
    "  print(f'{k}: {v[0]}')\n",
    "  print('-'*20)\n",
    "  for i in v[1:]:\n",
    "    if i not in list(meyer.columns):\n",
    "      print(f'{i} not in columns')\n",
    "    else:\n",
    "      # print(f'Dtype: {meyer[[i]].dytpe}')\n",
    "      meyer[i].value_counts(dropna = False).sort_index()\n",
    "    print('-'*20)\n",
    "  print('='*20, '\\n'*2)\n",
    "# All good!\n",
    "\n",
    "# Now I want to see a list of all the methods of combination.\n",
    "methods = []\n",
    "for v in feat_eng_dict.values():\n",
    "  if v[0] not in methods:\n",
    "    methods.append(v[0])\n",
    "methods\n",
    "\n",
    "# I still have binarize, mean, and sum, but I think I already\n",
    "# binarized the individual columns that will go into those\n",
    "# binarized composite scores, meaning that taking the mean\n",
    "# would accomplish the same thing.\n",
    "for k, v in feat_eng_dict.items():\n",
    "  if v[0]!='binarize':\n",
    "    continue\n",
    "  print('='*20)\n",
    "  print(f'{k}: {v[0]}')\n",
    "  print('-'*20)\n",
    "  for i in v[1:]:\n",
    "    if i not in list(meyer.columns):\n",
    "      print(f'{i} not in columns')\n",
    "    else:\n",
    "      # print(f'Dtype: {meyer[[i]].dytpe}')\n",
    "      meyer[i].value_counts(dropna = False).sort_index()\n",
    "    print('-'*20)\n",
    "  print('='*20, '\\n'*2)\n",
    "# Huzzah!  Yes!  I'm going to change those to means.\n",
    "\n",
    "\n",
    "# Ok let's run this bad boy!!\n",
    "# (I'm so freaking excited I'm so proud of myself for \n",
    "# coming up with this and I'm so clever)\n",
    "\n",
    "meyer.shape # (1494, 228)\n",
    "cols_added = 0\n",
    "cols_done = []\n",
    "for k, v in feat_eng_dict.items():\n",
    "  # print('')\n",
    "  # print(k)\n",
    "  meyer[k] = meyer[v[1]]\n",
    "  cols_done.append(v[1])\n",
    "  cols_added += 1\n",
    "  for i in v[2:]:\n",
    "    # print('doing the sum')\n",
    "    meyer[k] += meyer[i]\n",
    "    cols_done.append(i)\n",
    "  if v[0]=='mean':\n",
    "    # print('doing the mean')\n",
    "    meyer[k] = meyer[k]/len(v[1:])\n",
    "  elif v[0]=='binarize': \n",
    "    # print('binarizing')\n",
    "    meyer[k] = np.where(meyer[k]>1, 1, meyer[k])\n",
    "  \n",
    "cols_added + 228 # 253\n",
    "meyer.shape # (1494, 253)\n",
    "len(cols_done) # 121\n",
    "\n",
    "# These aren't showing as correct\n",
    "meyer['check_disabled'] = (meyer['w1q75_ei_r'] + meyer['w1q76_ei_r'])\n",
    "meyer.loc[(meyer['check_disabled']==2), 'check_disabled'] = 1 # = np.round((meyer['check_disabled']/2), 0)\n",
    "sum((meyer['check_disabled']-meyer['disabled'])!=0)\n",
    "sum(meyer['check_disabled']!=meyer['disabled'])\n",
    "\n",
    "# This works fine\n",
    "meyer['check_disabled'] += meyer['studyid']\n",
    "\n",
    "# let me see\n",
    "test = meyer[['disabled', 'check_disabled']]\n",
    "meyer['disabled'].describe()\n",
    "\n",
    "# Ooooh cool cool cool, I just hadn't re-run the dictionary since \n",
    "# changing the 'binarize's to 'mean's, so the if statement wasn't \n",
    "# always getting triggered.  It's fine.  Let me check others.\n",
    "\n",
    "# Also though, what was I thinking?  The mean of 0s and 1s isn't a \n",
    "# binary, it's a float.  So I need to change those back, edit the \n",
    "# code, and rerun it anyway.  \n",
    "# I'm going to go back up and do that in place.\n",
    "# Let me see how many bogus columns I've added though.\n",
    "meyer.shape\n",
    "# meyer.drop(columns = ['check_suicidality', ])\n",
    "# Eh actually I'll just reimport the CSV.\n",
    "\n",
    "# that all works upon re-running. \n",
    "\n",
    "# just a sum\n",
    "meyer['check_suicidality'] = (meyer['w1q101_ei_r'] + meyer['w1q105_ei_r'] + meyer['w1q109_ei_r'] + meyer['w1q114_ei_r'])\n",
    "meyer['check_suicidality']==meyer['suicidality']\n",
    "(meyer['check_suicidality']-meyer['suicidality'])==0\n",
    "# visually it looks like all Trues.  Let me see\n",
    "\n",
    "sum(meyer['check_suicidality']!=meyer['suicidality'])\n",
    "sum((meyer['check_suicidality']-meyer['suicidality'])!=0)\n",
    "# both 0s!  huzzah!  \n",
    "# this worked before, but upon rerunning, now it doesn't!\n",
    "check = meyer.loc[(meyer['check_suicidality']!=meyer['suicidality']), ['w1q101_ei_r', 'w1q105_ei_r', 'w1q109_ei_r', 'w1q114_ei_r', 'suicidality', 'check_suicidality']]\n",
    "check_index = list(check.index)\n",
    "check_2 = meyer.loc[check_index, ['w1q101_ei_r', 'w1q105_ei_r', 'w1q109_ei_r', 'w1q114_ei_r', 'suicidality', 'check_suicidality']]\n",
    "# ok now it works sometimes?????\n",
    "\n",
    "# actually a mean\n",
    "meyer['check_outness'] = (meyer['w1q123a_ei_r'] + meyer[\n",
    "  'w1q123b_ei_r'] + meyer['w1q123c_ei_r'] + meyer[\n",
    "    'w1q123d_ei_r'] + meyer['w1q124_ei_r'])/len([\n",
    "      'mean', 'w1q123a_ei_r', 'w1q123b_ei_r', 'w1q123c_ei_r', \n",
    "      'w1q123d_ei_r', 'w1q124_ei_r'][1:])\n",
    "meyer['check_outness']==meyer['outness']\n",
    "(meyer['check_outness']-meyer['outness'])==0\n",
    "# visually it looks like all Trues.  Let me see\n",
    "\n",
    "sum(meyer['check_outness']!=meyer['outness'])\n",
    "sum((meyer['check_outness']-meyer['outness'])!=0)\n",
    "\n",
    "6!=4\n",
    "# ok seems fine!\n",
    " \n",
    " \n",
    "# save a backup\n",
    "meyer.to_csv(f'{my_date()}_column-combination-done_no-drops_not-reordered_FIXED.csv', index = False)\n",
    "\n",
    "# drop the component columns\n",
    "cols_done.append('check_outness')\n",
    "cols_done.append('check_suicidality')\n",
    "cols_done.append('check_disabled')\n",
    "228-121+25\n",
    "meyer.drop(columns = cols_done).shape # goal is 132\n",
    "meyer.drop(columns = cols_done, inplace = True)\n",
    "\n",
    "ordered_cols = sorted(list(meyer.columns))\n",
    "ordered_cols.remove('studyid')\n",
    "ordered_cols.remove('kessler6_sqrt')\n",
    "ordered_cols.remove('w1kessler6_i')\n",
    "ordered_cols = ['studyid', 'w1kessler6_i', 'kessler6_sqrt'] + ordered_cols\n",
    "ordered_cols\n",
    "\n",
    "len(ordered_cols) # 132\n",
    "meyer.shape # (1494, 132)\n",
    "\n",
    "meyer = meyer[ordered_cols]\n",
    "meyer.shape\n",
    "\n",
    "# save again\n",
    "meyer.to_csv(f'{my_date()}_column-combination-done_drops-done_reordered_FIXED.csv', index = False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 4 in full:\n",
    "4. 2024-06-07-eda.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friday, June 7, 2024\n",
    "# EDA\n",
    "\n",
    "# As usual, we're starting with my beloved autoplots - now with new arguments!\n",
    "\n",
    "autoplots(meyer, 'w1kessler6_i', qqplots=True, transform=True, line=False, verbose=True)\n",
    "autoplots(meyer, 'kessler6_sqrt', qqplots=True, transform=True, line=False, verbose=True)\n",
    "\n",
    "# There are a few more that I need to OHE, 0-base, or otherwise tinker with, but I'm going\n",
    "# to try to resist the urge to do that until I have SOME kind of model fit.\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "meyer.info()\n",
    "\n",
    "# I hate spending the time on this, but I need the column names to have meaning.\n",
    "for i in list(meyer.columns):\n",
    "  print(f\"'{i}': '{i}', \")\n",
    "\n",
    "# I'm trying to both at once and it's not working\n",
    "# This list is for whether to screw with them some more.\n",
    "for i in list(meyer.columns):\n",
    "  print(f\"'{i}': _, \")\n",
    "\n",
    "meyer['w1kessler6_i'].value_counts(dropna = False).sort_index()\n",
    "col_bi.append('') # abus_treat_non_queer?\n",
    "\n",
    "col_names = {\n",
    "  'studyid': 'studyid', \n",
    "  'w1kessler6_i': 'kessler6_orig', \n",
    "  'kessler6_sqrt': 'kessler6_sqrt', \n",
    "  \n",
    "  'abus_treat_non_queer': 'abus_treat_non_queer', \n",
    "  'abus_treat_sex_gender': 'abus_treat_sex_gender', \n",
    "  'abusive_treatment': 'abusive_treatment', \n",
    "  'bad_neighbhd': 'bad_neighbhd', \n",
    "  'childhd_bullying_non_queer': 'childhd_bullying_non_queer', \n",
    "  'childhd_bullying_sex_gender': 'childhd_bullying_sex_gender', \n",
    "  'chronic_strain': 'chronic_strain', \n",
    "  'cohort': 'cohort', \n",
    "  'daily_discr_non_queer': 'daily_discr_non_queer', \n",
    "  'daily_discr_sex_gender': 'daily_discr_sex_gender', \n",
    "  'disabled': 'disabled', \n",
    "  'gcendiv': 'gcendiv', \n",
    "  'gcenreg': 'gcenreg', \n",
    "  'geduc1': 'geduc1', \n",
    "  'geduc2': 'geduc2', \n",
    "  'geducation': 'geducation', \n",
    "  'gmilesaway2_ei_r': 'gmilesaway2_ei_r', \n",
    "  'gurban_i': 'gurban_i', \n",
    "  'health_insurance': 'health_insurance', \n",
    "  'housing_disc_non_queer': 'housing_disc_non_queer', \n",
    "  'housing_disc_sex_gender': 'housing_disc_sex_gender', \n",
    "  'outness': 'outness', \n",
    "  'screen_race': 'screen_race', \n",
    "  'serious_health_cond': 'serious_health_cond', \n",
    "  'stress_past_year_crime': 'stress_past_year_crime', \n",
    "  'stress_past_year_gen': 'stress_past_year_gen', \n",
    "  'stress_past_year_interpersonal': 'stress_past_year_interpersonal', \n",
    "  'stress_past_year_non_queer': 'stress_past_year_non_queer', \n",
    "  'stress_past_year_sex_gender': 'stress_past_year_sex_gender', \n",
    "  'stress_past_year_work': 'stress_past_year_work', \n",
    "  'suicidality': 'suicidality', \n",
    "  'w1ace_emo_i': 'w1ace_emo_i', \n",
    "  'w1ace_i': 'w1ace_i', \n",
    "  'w1ace_inc_i': 'w1ace_inc_i', \n",
    "  'w1ace_ipv_i': 'w1ace_ipv_i', \n",
    "  'w1ace_men_i': 'w1ace_men_i', \n",
    "  'w1ace_phy_i': 'w1ace_phy_i', \n",
    "  'w1ace_sep_i': 'w1ace_sep_i', \n",
    "  'w1ace_sex_i': 'w1ace_sex_i', \n",
    "  'w1ace_sub_i': 'w1ace_sub_i', \n",
    "  'w1age': 'w1age', \n",
    "  'w1auditc_i': 'w1auditc_i', \n",
    "  'w1childgnc_i': 'w1childgnc_i', \n",
    "  'w1connectedness_i': 'w1connectedness_i', \n",
    "  'w1conversion': 'w1conversion', \n",
    "  'w1conversionhc': 'w1conversionhc', \n",
    "  'w1conversionrel': 'w1conversionrel', \n",
    "  'w1dudit_i': 'w1dudit_i', \n",
    "  'w1everyday_i': 'w1everyday_i', \n",
    "  'w1feltstigma_i': 'w1feltstigma_i', \n",
    "  'w1gender': 'w1gender', \n",
    "  'w1hcthreat_i': 'w1hcthreat_i', \n",
    "  'w1hinc_i': 'w1hinc_i', \n",
    "  'w1idcentral_i': 'w1idcentral_i', \n",
    "  'w1internalized_i': 'w1internalized_i', \n",
    "  'w1lifesat_i': 'w1lifesat_i', \n",
    "  'w1meim_i': 'w1meim_i', \n",
    "  'w1pinc_i': 'w1pinc_i', \n",
    "  'w1poverty_i_ei': 'w1poverty_i_ei', \n",
    "  'w1povertycat_i_ei': 'w1povertycat_i_ei', \n",
    "  'w1q01_ei': 'w1q01_ei', \n",
    "  'w1q03_ei': 'w1q03_ei', \n",
    "  'w1q119_ei': 'w1q119_ei', \n",
    "  'w1q136_7_ei': 'w1q136_7_ei', \n",
    "  'w1q139_7_ei': 'w1q139_7_ei', \n",
    "  'w1q140_ei': 'w1q140_ei', \n",
    "  'w1q141_7_ei': 'w1q141_7_ei', \n",
    "  'w1q143_7_ei': 'w1q143_7_ei', \n",
    "  'w1q145_7_ei': 'w1q145_7_ei', \n",
    "  'w1q162_ei': 'w1q162_ei', \n",
    "  'w1q163_7_ei': 'w1q163_7_ei', \n",
    "  'w1q166_ei': 'w1q166_ei', \n",
    "  'w1q167_ei': 'w1q167_ei', \n",
    "  'w1q168_ei': 'w1q168_ei', \n",
    "  'w1q169_ei': 'w1q169_ei', \n",
    "  'w1q171_1_ei': 'w1q171_1_ei', \n",
    "  'w1q171_2_ei': 'w1q171_2_ei', \n",
    "  'w1q171_3_ei': 'w1q171_3_ei', \n",
    "  'w1q171_4_ei': 'w1q171_4_ei', \n",
    "  'w1q171_5_ei': 'w1q171_5_ei', \n",
    "  'w1q171_6_ei': 'w1q171_6_ei', \n",
    "  'w1q171_7_ei': 'w1q171_7_ei', \n",
    "  'w1q171_8_ei': 'w1q171_8_ei', \n",
    "  'w1q171_9_ei': 'w1q171_9_ei', \n",
    "  'w1q175_ei': 'w1q175_ei', \n",
    "  'w1q179_ei_r_relig_christ': 'w1q179_ei_r_relig_christ', \n",
    "  'w1q179_ei_r_relig_other': 'w1q179_ei_r_relig_other', \n",
    "  'w1q180_ei_r_relig_christ': 'w1q180_ei_r_relig_christ', \n",
    "  'w1q180_ei_r_relig_other': 'w1q180_ei_r_relig_other', \n",
    "  'w1q181_ei_r': 'w1q181_ei_r', \n",
    "  'w1q30_1_ei': 'w1q30_1_ei', \n",
    "  'w1q30_2_ei': 'w1q30_2_ei', \n",
    "  'w1q30_3_ei': 'w1q30_3_ei', \n",
    "  'w1q30_4_ei': 'w1q30_4_ei', \n",
    "  'w1q30_5_ei': 'w1q30_5_ei', \n",
    "  'w1q32_ei': 'w1q32_ei', \n",
    "  'w1q33_ei': 'w1q33_ei', \n",
    "  'w1q34_ei': 'w1q34_ei', \n",
    "  'w1q35_ei': 'w1q35_ei', \n",
    "  'w1q36_ei': 'w1q36_ei', \n",
    "  'w1q37_ei': 'w1q37_ei', \n",
    "  'w1q38_ei': 'w1q38_ei', \n",
    "  'w1q52_ei': 'w1q52_ei', \n",
    "  'w1q64_1_ei': 'w1q64_1_ei', \n",
    "  'w1q65_ei': 'w1q65_ei', \n",
    "  'w1q69_ei': 'w1q69_ei', \n",
    "  'w1q72_ei': 'w1q72_ei', \n",
    "  'w1q74_21_ei': 'w1q74_21_ei', \n",
    "  'w1q74_22_ei': 'w1q74_22_ei', \n",
    "  'w1q74_23_ei': 'w1q74_23_ei', \n",
    "  'w1q78_ei': 'w1q78_ei', \n",
    "  'w1q79_ei': 'w1q79_ei', \n",
    "  'w1q89_ei': 'w1q89_ei', \n",
    "  'w1race': 'w1race', \n",
    "  'w1sex': 'w1sex', \n",
    "  'w1sex_gender': 'w1sex_gender', \n",
    "  'w1sexminid': 'w1sexminid', \n",
    "  'w1sexualid': 'w1sexualid', \n",
    "  'w1socialwb_i': 'w1socialwb_i', \n",
    "  'w1socsupport_fam_i': 'w1socsupport_fam_i', \n",
    "  'w1socsupport_fr_i': 'w1socsupport_fr_i', \n",
    "  'w1socsupport_i': 'w1socsupport_i', \n",
    "  'w1socsupport_so_i': 'w1socsupport_so_i', \n",
    "  'w1survey_yr': 'w1survey_yr', \n",
    "  'w1weight_full': 'w1weight_full', \n",
    "  'waveparticipated': 'waveparticipated', \n",
    "  'work_disc_non_queer': 'work_disc_non_queer', \n",
    "  'work_disc_sex_gender': 'work_disc_sex_gender', \n",
    "  'work_neg_outcomes': 'work_neg_outcomes'\n",
    "}\n",
    "\n",
    "\n",
    "meyer_corr = meyer.corr()\n",
    "\n",
    "meyer.shape\n",
    "meyer.to_csv(f'{my_date()}_end-of-day.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 5 in full:\n",
    "5. 2024-06-07_NEW_feat_eng_dict.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thursday, June 7, 2024\n",
    "# NEW feat_eng_dict\n",
    "\n",
    "# I thought I was so smart before, just getting a sum from these guys.\n",
    "# Fool.  It dilutes the power of the \"yes\"s! There aren't enough \"yes\"s \n",
    "# in any one level of frequency to compete with the \"no\"s.\n",
    "\n",
    "# I'm going to create this list with the first one I found and\n",
    "# then append to it in the console.\n",
    "columns_to_binarize = ['abus_treat_non_queer']\n",
    "# make a shorter name, dummy\n",
    "col_bi = columns_to_binarize.copy()\n",
    "\n",
    "# these are some visually very promising ones\n",
    "good_ones = []\n",
    "\n",
    "feat_eng_dict_NEW = {\n",
    "  'health_insurance': [\n",
    "    'binarize', 'w1q64_2_ei', 'w1q64_3_ei', 'w1q64_4_ei', 'w1q64_5_ei', \n",
    "    'w1q64_6_ei', 'w1q64_7_ei', 'w1q64_8_ei', 'w1q64_9_ei', 'w1q64_10_ei', \n",
    "    'w1q64_11_ei', 'w1q64_12_ei', 'w1q64_13_ei', 'w1q64_t_num'], \n",
    "  'serious_health_cond': [\n",
    "    'binarize', 'w1q74_5_ei', 'w1q74_6_ei', 'w1q74_10_ei', 'w1q74_11_ei', \n",
    "    'w1q74_14_ei', 'w1q74_17_ei', 'w1q74_18_ei', 'w1q74_20_ei'], \n",
    "  'disabled': [\n",
    "    'binarize', 'w1q75_ei_r', 'w1q76_ei_r'], \n",
    "  'outness': [\n",
    "    'mean', 'w1q123a_ei_r', 'w1q123b_ei_r', 'w1q123c_ei_r', 'w1q123d_ei_r',\n",
    "    'w1q124_ei_r'], \n",
    "  'abusive_treatment': [\n",
    "    'sum', 'w1q135a_ei_r', 'w1q135b_ei_r', 'w1q135c_ei_r', 'w1q135d_ei_r', \n",
    "    'w1q135e_ei_r', 'w1q135f_ei_r'], \n",
    "  'work_neg_outcomes': [\n",
    "    'sum', 'w1q137_ei_r', 'w1q138_ei_r'], \n",
    "  'abus_treat_non_queer': [\n",
    "    'sum', 'w1q136_1_ei', 'w1q136_5_ei', 'w1q136_6_ei', 'w1q136_8_ei', \n",
    "    'w1q136_9_ei', 'w1q136_10_ei'], \n",
    "  'stress_past_year_gen': [\n",
    "    'sum', 'w1q142a_ei_r', 'w1q142h_ei_r', 'w1q142i_ei_r'], \n",
    "  'stress_past_year_work': [\n",
    "    'sum', 'w1q142b_ei_r', 'w1q142c_ei_r', 'w1q142e_ei_r'], \n",
    "  'stress_past_year_interpersonal': [\n",
    "    'sum', 'w1q142d_ei_r', 'w1q142f_ei_r', 'w1q142g_ei_r'], \n",
    "  'stress_past_year_crime': [\n",
    "    'sum', 'w1q142j_ei_r', 'w1q142k_ei_r'], \n",
    "  'work_disc_non_queer': [\n",
    "    'sum', 'w1q139_1_ei', 'w1q139_5_ei', 'w1q139_6_ei', 'w1q139_8_ei', \n",
    "    'w1q139_9_ei', 'w1q139_10_ei'], \n",
    "  'housing_disc_non_queer': [\n",
    "    'sum', 'w1q141_1_ei', 'w1q141_5_ei', 'w1q141_6_ei', 'w1q141_8_ei', \n",
    "    'w1q141_9_ei', 'w1q141_10_ei'], \n",
    "  'stress_past_year_non_queer': [\n",
    "    'sum', 'w1q143_1_ei', 'w1q143_5_ei', 'w1q143_6_ei', 'w1q143_8_ei', \n",
    "    'w1q143_9_ei', 'w1q143_10_ei'], \n",
    "  'daily_discr_non_queer': [\n",
    "    'sum', 'w1q145_1_ei', 'w1q145_5_ei', 'w1q145_6_ei'], \n",
    "  'childhd_bullying_non_queer': [\n",
    "    'sum', 'w1q163_1_ei', 'w1q163_5_ei', 'w1q163_6_ei', 'w1q163_8_ei', \n",
    "    'w1q163_9_ei', 'w1q163_10_ei'], \n",
    "  'abus_treat_sex_gender': [\n",
    "    'sum', 'w1q136_2_ei', 'w1q136_3_ei', 'w1q136_4_ei'], \n",
    "  'work_disc_sex_gender': [\n",
    "    'sum', 'w1q139_2_ei', 'w1q139_3_ei', 'w1q139_4_ei'], \n",
    "  'housing_disc_sex_gender': [\n",
    "    'sum', 'w1q141_2_ei', 'w1q141_3_ei', 'w1q141_4_ei'], \n",
    "  'stress_past_year_sex_gender': [\n",
    "    'sum', 'w1q143_2_ei', 'w1q143_3_ei', 'w1q143_4_ei'], \n",
    "  'daily_discr_sex_gender': [\n",
    "    'sum', 'w1q145_2_ei', 'w1q145_3_ei', 'w1q145_4_ei', 'w1q145_8_ei', \n",
    "    'w1q145_9_ei', 'w1q145_10_ei'], \n",
    "  'childhd_bullying_sex_gender': [\n",
    "    'sum', 'w1q163_2_ei', 'w1q163_3_ei', 'w1q163_4_ei'], \n",
    "  'chronic_strain': [\n",
    "    'sum', 'w1q146a_ei', 'w1q146b_ei', 'w1q146c_ei', 'w1q146d_ei', 'w1q146e_ei', \n",
    "    'w1q146f_ei', 'w1q146g_ei', 'w1q146h_ei', 'w1q146i_ei', 'w1q146j_ei', \n",
    "    'w1q146k_ei', 'w1q146l_ei'], \n",
    "  'bad_neighbhd': [\n",
    "    'sum', 'w1q19a_ei_r', 'w1q19b_ei_r', 'w1q19c_ei_r', 'w1q19d_ei_r'], \n",
    "  'suicidality': ['sum', 'w1q101_ei_r', 'w1q105_ei_r', 'w1q109_ei_r', 'w1q114_ei_r']}\n",
    " \n",
    "len(feat_eng_dict['_'][1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 6 in full:\n",
    "6. 2024-06-07_columns-i-want-to-screw-with-some-more-and-how.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_screw = {\n",
    "  # 'studyid': _, \n",
    "  # 'w1kessler6_i': _, \n",
    "  # 'kessler6_sqrt': _, \n",
    "  \n",
    "  'abus_treat_non_queer': len(feat_eng_dict[k][1:]), \n",
    "  'abus_treat_sex_gender': len(feat_eng_dict[k][1:]), \n",
    "  # 'abusive_treatment': _, \n",
    "  # 'bad_neighbhd': _, \n",
    "  'childhd_bullying_non_queer': len(feat_eng_dict['childhd_bullying_non_queer'][1:]), \n",
    "  'childhd_bullying_sex_gender': len(feat_eng_dict['childhd_bullying_sex_gender'][1:]), \n",
    "  # 'chronic_strain': _, \n",
    "  'cohort': 'ohe', \n",
    "  'daily_discr_non_queer': len(feat_eng_dict[k][1:]), \n",
    "  'daily_discr_sex_gender': len(feat_eng_dict[k][1:]), \n",
    "  # 'disabled': _, \n",
    "  'gcendiv': d, \n",
    "  'gcenreg': o, \n",
    "  # 'geduc1': _, \n",
    "  'geduc2': d, \n",
    "  # 'geducation': _, \n",
    "  # 'gmilesaway2_ei_r': _, \n",
    "  # 'gurban_i': _, \n",
    "  'health_insurance': 'combine with q64', \n",
    "  'housing_disc_non_queer': len(feat_eng_dict[k][1:]), \n",
    "  'housing_disc_sex_gender': len(feat_eng_dict[k][1:]), \n",
    "  # 'outness': _, \n",
    "  'screen_race': 'check with other race stuff', \n",
    "  'serious_health_cond': _, \n",
    "  'stress_past_year_crime': _, \n",
    "  'stress_past_year_gen': _, \n",
    "  'stress_past_year_interpersonal': _, \n",
    "  'stress_past_year_non_queer': _, \n",
    "  'stress_past_year_sex_gender': _, \n",
    "  'stress_past_year_work': _, \n",
    "  'suicidality': _, \n",
    "  'w1ace_emo_i': _, \n",
    "  'w1ace_i': _, \n",
    "  'w1ace_inc_i': _, \n",
    "  'w1ace_ipv_i': _, \n",
    "  'w1ace_men_i': _, \n",
    "  'w1ace_phy_i': _, \n",
    "  'w1ace_sep_i': _, \n",
    "  'w1ace_sex_i': _, \n",
    "  'w1ace_sub_i': _, \n",
    "  'w1age': _, \n",
    "  'w1auditc_i': _, \n",
    "  'w1childgnc_i': _, \n",
    "  'w1connectedness_i': _, \n",
    "  'w1conversion': _, \n",
    "  'w1conversionhc': _, \n",
    "  'w1conversionrel': _, \n",
    "  'w1dudit_i': _, \n",
    "  'w1everyday_i': _, \n",
    "  'w1feltstigma_i': _, \n",
    "  'w1gender': _, \n",
    "  'w1hcthreat_i': _, \n",
    "  'w1hinc_i': _, \n",
    "  'w1idcentral_i': _, \n",
    "  'w1internalized_i': _, \n",
    "  'w1lifesat_i': _, \n",
    "  'w1meim_i': _, \n",
    "  'w1pinc_i': _, \n",
    "  'w1poverty_i_ei': _, \n",
    "  'w1povertycat_i_ei': _, \n",
    "  'w1q01_ei': _, \n",
    "  'w1q03_ei': _, \n",
    "  'w1q119_ei': _, \n",
    "  'w1q136_7_ei': _, \n",
    "  'w1q139_7_ei': _, \n",
    "  'w1q140_ei': _, \n",
    "  'w1q141_7_ei': _, \n",
    "  'w1q143_7_ei': _, \n",
    "  'w1q145_7_ei': _, \n",
    "  'w1q162_ei': _, \n",
    "  'w1q163_7_ei': _, \n",
    "  'w1q166_ei': _, \n",
    "  'w1q167_ei': _, \n",
    "  'w1q168_ei': _, \n",
    "  'w1q169_ei': _, \n",
    "  'w1q171_1_ei': _, \n",
    "  'w1q171_2_ei': _, \n",
    "  'w1q171_3_ei': _, \n",
    "  'w1q171_4_ei': _, \n",
    "  'w1q171_5_ei': _, \n",
    "  'w1q171_6_ei': _, \n",
    "  'w1q171_7_ei': _, \n",
    "  'w1q171_8_ei': _, \n",
    "  'w1q171_9_ei': _, \n",
    "  'w1q175_ei': _, \n",
    "  'w1q179_ei_r_relig_christ': _, \n",
    "  'w1q179_ei_r_relig_other': _, \n",
    "  'w1q180_ei_r_relig_christ': _, \n",
    "  'w1q180_ei_r_relig_other': _, \n",
    "  'w1q181_ei_r': _, \n",
    "  'w1q30_1_ei': _, \n",
    "  'w1q30_2_ei': _, \n",
    "  'w1q30_3_ei': _, \n",
    "  'w1q30_4_ei': _, \n",
    "  'w1q30_5_ei': _, \n",
    "  'w1q32_ei': _, \n",
    "  'w1q33_ei': _, \n",
    "  'w1q34_ei': _, \n",
    "  'w1q35_ei': _, \n",
    "  'w1q36_ei': _, \n",
    "  'w1q37_ei': _, \n",
    "  'w1q38_ei': _, \n",
    "  'w1q52_ei': _, \n",
    "  'w1q64_1_ei': _, \n",
    "  'w1q65_ei': _, \n",
    "  'w1q69_ei': _, \n",
    "  'w1q72_ei': _, \n",
    "  'w1q74_21_ei': _, \n",
    "  'w1q74_22_ei': _, \n",
    "  'w1q74_23_ei': _, \n",
    "  'w1q78_ei': _, \n",
    "  'w1q79_ei': _, \n",
    "  'w1q89_ei': _, \n",
    "  'w1race': _, \n",
    "  'w1sex': _, \n",
    "  'w1sex_gender': _, \n",
    "  'w1sexminid': _, \n",
    "  'w1sexualid': _, \n",
    "  'w1socialwb_i': _, \n",
    "  'w1socsupport_fam_i': _, \n",
    "  'w1socsupport_fr_i': _, \n",
    "  'w1socsupport_i': _, \n",
    "  'w1socsupport_so_i': _, \n",
    "  'w1survey_yr': _, \n",
    "  'w1weight_full': _, \n",
    "  'waveparticipated': _, \n",
    "  'work_disc_non_queer': _, \n",
    "  'work_disc_sex_gender': _, \n",
    "  'work_neg_outcomes': _,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 7 in full:\n",
    "7. 2024-06-07_modeling-part-1.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friday, June 7, 2024\n",
    "# Modeling!\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# I'm just going to eyeball some features from the scatterplots\n",
    "good_ones = ['chronic_strain', 'suicidality', 'w1age', 'w1auditc_i', \n",
    "    'w1connectedness_i', 'w1conversion', 'w1dudit_i', 'w1everyday_i', \n",
    "  'w1feltstigma_i', 'w1idcentral_i', 'w1internalized_i', 'w1lifesat_i', \n",
    "  'w1meim_i', 'w1pinc_i', 'w1poverty_i_ei', 'w1q03_ei', 'w1q33_ei', \n",
    "  'w1q52_ei', 'w1q72_ei', 'w1q74_22_ei', 'w1q74_21_ei', 'w1q140_ei', \n",
    "  'w1q166_ei', 'w1q167_ei', 'w1q171_8_ei', 'w1q181_ei_r', 'w1socialwb_i', \n",
    "  'w1socsupport_i']\n",
    "\n",
    "# Make X and y\n",
    "X = meyer[good_ones]\n",
    "y = meyer['kessler6_sqrt']\n",
    "\n",
    "# Very small testing set because this is an inferential model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "  test_size = 0.1, random_state = 6)\n",
    "  \n",
    "for i in [X_train, X_test, y_train, y_test]:\n",
    "  print(i.shape)\n",
    "\n",
    "# Instantiate the model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Cross validation just for funsies\n",
    "cross_val_score(lr, X_train, y_train)\n",
    "\n",
    "# Fit the model\n",
    "model_1 = lr.fit(X_train, y_train)\n",
    "\n",
    "# Make some predictions\n",
    "model_1_train_preds = model_1.predict(X_train)\n",
    "model_1_test_preds = model_1.predict(X_test)\n",
    "\n",
    "# Score the model\n",
    "print('Training Set')\n",
    "model_1.score(X_train, y_train)\n",
    "mean_squared_error(y_train, model_1_train_preds)\n",
    "mean_squared_error(y_train, model_1_train_preds, squared = False)\n",
    "mean_absolute_error(y_train, model_1_train_preds)\n",
    "print('='*20)\n",
    "print('Testing Set')\n",
    "model_1.score(X_test, y_test)\n",
    "mean_squared_error(y_test, model_1_test_preds)\n",
    "mean_squared_error(y_test, model_1_test_preds, squared = False)\n",
    "mean_absolute_error(y_test, model_1_test_preds)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 8 in full:\n",
    "8. autoplots_v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, date, time\n",
    "from string import capwords\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Define my_date()\n",
    "def my_date():\n",
    "  return datetime.now().strftime('%Y-%m-%d_h%H-m%M-s%S')\n",
    "my_date()\n",
    "\n",
    "# Define autoplots()\n",
    "def autoplots(d, y, qqplots = True, transform = False, line = False, verbose = True):\n",
    "  '''a function to make a ton of graphs.\n",
    "  Each plot is based on a subset of d where all variables in the\n",
    "  plot have no null values.  The size of this subset (n) is \n",
    "  displayed in the subtitle of the plot, and can be used \n",
    "  similarly to d.isnull().sum(), if desired.\n",
    "  \n",
    "  args:\n",
    "    d: dataframe, the dataframe of the information.\n",
    "    \n",
    "    y: string, the name of the column within the dataframe that is the target.\n",
    "    \n",
    "    qqplots: bool, whether to generate qqplots for each variable. Default = True.\n",
    "    \n",
    "    transform: bool, whether to generate qqplots of transformed versions \n",
    "      of each variable. Only used when `qqplots = True`, otherwise ignored. \n",
    "      Default = False. Included transformations are (x+z)**(1/2), (x+z)**(1/3), \n",
    "      and log(x+z+1), where z=abs(x.min()) if x.min()<0, else z=0.\n",
    "    \n",
    "    line: bool, whether to plot line graphs. Default = False.\n",
    "    \n",
    "    verbose: bool, whether to print updates while running. Default = True.\n",
    "  \n",
    "  return:\n",
    "    a ton of plots in a FOLDER\n",
    "  \n",
    "  raise:\n",
    "    pls no'''\n",
    "  \n",
    "  # Need these\n",
    "  import os\n",
    "  from string import capwords\n",
    "  import matplotlib.pyplot as plt\n",
    "  import seaborn as sns\n",
    "  import statsmodels.api as sm\n",
    "  import numpy as np\n",
    "  import pandas as pd\n",
    "  \n",
    "  # Make a folder \n",
    "  try: \n",
    "    os.mkdir('images')\n",
    "  except:\n",
    "    pass\n",
    "  a = f'images/plots_{my_date()}'\n",
    "  os.mkdir(a)\n",
    "  \n",
    "  # Define these once\n",
    "  n_grand = len(d[y])\n",
    "  if verbose==True: print(n_grand)\n",
    "  # in future versions, I'd like to raise a warning\n",
    "  # if len(d[y])!=len(d[d[y].notna()])    \n",
    "  # (i.e., if there are nulls in the target)\n",
    "  n_features = d.shape[1]\n",
    "  \n",
    "  # Give y a good(ish) name\n",
    "  ty = capwords(y.replace('_', ' '))\n",
    "  if verbose==True: print(ty)\n",
    "  \n",
    "  # Plot the distributions of all variables\n",
    "  for i in d.columns:\n",
    "    if verbose==True: print(i)\n",
    "    # Give it a good(ish) name\n",
    "    t = capwords(i.replace('_', ' '))\n",
    "    if verbose==True: print(t)\n",
    "    \n",
    "    # Extract the subset dataframe, drop NAs, get n\n",
    "    df = d[i]\n",
    "    if verbose==True: print(df.shape)\n",
    "    df.dropna(inplace = True)\n",
    "    n = len(df)\n",
    "    \n",
    "    # Plot a histogram of it\n",
    "    plt.figure(figsize = (16, 9));\n",
    "    plt.hist(df, bins = 'auto', color = 'purple');\n",
    "    plt.suptitle(f'Distribution of {t}', size = 24)\n",
    "    plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "    plt.xlabel(f'{t}', size = 20);\n",
    "    plt.ylabel('Frequency', size = 20);\n",
    "    plt.xticks(size = 16, rotation = 60);\n",
    "    plt.yticks(size = 16)\n",
    "    # plt.tight_layout()\n",
    "    plt.savefig(f'./{a}/{i}_histogram.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot a boxplot of it\n",
    "    plt.figure(figsize = (16, 9))\n",
    "    sns.boxplot(data = df, color = 'purple', orient = 'h')\n",
    "    plt.suptitle(f'Distribution of {t}', size = 24)\n",
    "    plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "    plt.xlabel(f'{t}', size = 20);\n",
    "    plt.xticks(size = 16, rotation = 60)\n",
    "    # plt.tight_layout()\n",
    "    plt.savefig(f'./{a}/{i}_boxplot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot (a) qqplot(s) of it, if indicated\n",
    "    if qqplots==True:\n",
    "      \n",
    "      # Raw data\n",
    "      plt.figure(figsize = (9, 9))\n",
    "      sm.qqplot(data = df, line='45', markerfacecolor = 'purple', \n",
    "        markeredgecolor = 'purple', alpha = 0.5)\n",
    "      plt.suptitle(f'QQ-Plot of {t}', size = 24)\n",
    "      plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "      plt.xticks(size = 16, rotation = 60);\n",
    "      plt.yticks(size = 16);\n",
    "      # plt.tight_layout();\n",
    "      plt.savefig(f'./{a}/{i}_qqplot.png')\n",
    "      plt.close()\n",
    "      \n",
    "      # Plot transformations, if indicated\n",
    "      if transform == True:\n",
    "        \n",
    "        # Calculate z\n",
    "        if df.min()<0:\n",
    "          z=abs(df.min())\n",
    "        else: \n",
    "          z=0\n",
    "        \n",
    "        # Square root\n",
    "        plt.figure(figsize = (9, 9))\n",
    "        sm.qqplot(data = ((df+z)**(1/2)), line='45', alpha = 0.5, \n",
    "          markerfacecolor = 'purple', markeredgecolor = 'purple')\n",
    "        plt.suptitle(f'QQ-Plot of Square Root of {t}-Plus-{z}', size = 24)\n",
    "        plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "        plt.xticks(size = 16, rotation = 60);\n",
    "        plt.yticks(size = 16);\n",
    "        # plt.tight_layout();\n",
    "        plt.savefig(f'./{a}/{i}_qqplot_2nd_root.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Cube root\n",
    "        plt.figure(figsize = (9, 9))\n",
    "        sm.qqplot(data = ((df+z)**(1/3)), line='45', alpha = 0.5, \n",
    "          markerfacecolor = 'purple', markeredgecolor = 'purple')\n",
    "        plt.suptitle(f'QQ-Plot of Cube Root of {t}-Plus-{z}', size = 24)\n",
    "        plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "        plt.xticks(size = 16, rotation = 60);\n",
    "        plt.yticks(size = 16);\n",
    "        # plt.tight_layout();\n",
    "        plt.savefig(f'./{a}/{i}_qqplot_3rd_root.png')\n",
    "        plt.close()        \n",
    "        \n",
    "        # Log... ish\n",
    "        plt.figure(figsize = (9, 9))\n",
    "        sm.qqplot(data = np.log(df+z+1), line='45', alpha = 0.5, \n",
    "          markerfacecolor = 'purple', markeredgecolor = 'purple')\n",
    "        plt.suptitle(f'QQ-Plot of Log of {t}-Plus-{z+1}', size = 24)\n",
    "        plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "        plt.xticks(size = 16, rotation = 60);\n",
    "        plt.yticks(size = 16);\n",
    "        # plt.tight_layout();\n",
    "        plt.savefig(f'./{a}/{i}_qqplot_log.png')\n",
    "        plt.close()       \n",
    "    \n",
    "  # Drop y from the list\n",
    "  X = [col for col in list(d.drop(columns = [y]).columns)]\n",
    "  \n",
    "  # Make plots of each x against y\n",
    "  for i in X:\n",
    "    # Give it a good(ish) name\n",
    "    t = capwords(i.replace('_', ' '))\n",
    "    if verbose==True: print(t)\n",
    "    \n",
    "    # Extract the subset dataframe, drop NAs, get n\n",
    "    df = d[[i, y]]\n",
    "    df.dropna(inplace = True)\n",
    "    n = len(df[y])\n",
    "    \n",
    "    # Plot a scatterplot of it against y\n",
    "    plt.figure(figsize = (16, 9))\n",
    "    plt.scatter(df[i], df[y], alpha = 0.5, color = 'purple')\n",
    "    plt.suptitle(f'Relationship between {t} and {ty}', size = 24)\n",
    "    plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "    plt.xlabel(f'{t}', size = 20);\n",
    "    plt.ylabel(f'{ty}', size = 20);\n",
    "    plt.xticks(size = 16, rotation = 60)\n",
    "    plt.yticks(size = 16)\n",
    "    # plt.tight_layout()\n",
    "    plt.savefig(f'./{a}/{i}-by-{y}_scatterplot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot a line plot of it against y\n",
    "    if line==True:\n",
    "      plt.figure(figsize = (16, 9))\n",
    "      plt.plot(i, y, data = df, color = 'purple')\n",
    "      plt.suptitle(f'Relationship between {t} and {ty}', size = 24)\n",
    "      plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "      plt.xlabel(f'{t}', size = 20);\n",
    "      plt.ylabel(f'{ty}', size = 20);\n",
    "      plt.xticks(size = 16, rotation = 60)\n",
    "      plt.yticks(size = 16)\n",
    "      # plt.tight_layout()\n",
    "      plt.savefig(f'./{a}/{i}-by-{y}_lineplot.png')\n",
    "      plt.close()\n",
    "    \n",
    "  # All together now\n",
    "  n = len(d[y])\n",
    "  \n",
    "  # Plot a line plot of everything against y\n",
    "  if line==True:\n",
    "    plt.figure(figsize = (16, 9))\n",
    "    for i in X:\n",
    "      if verbose==True: print(i)\n",
    "      plt.plot(i, y, data = d)\n",
    "    plt.suptitle(f'Relationship between Predictors and {ty}', size = 24)\n",
    "    plt.title(f'Based on {n_grand} Observations out of {n_grand}', size = 18)\n",
    "    plt.xlabel(f'{t}', size = 20);\n",
    "    plt.ylabel(f'{ty}', size = 20);\n",
    "    plt.xticks(size = 16, rotation = 60)\n",
    "    plt.yticks(size = 16)\n",
    "    plt.legend();\n",
    "    # plt.tight_layout()\n",
    "    plt.savefig(f'./{a}/all-by-{y}_lineplot.png')\n",
    "    plt.close()\n",
    "  \n",
    "  # Get some correlations\n",
    "  corr = round(d.corr(numeric_only = True), 2)\n",
    "  \n",
    "  # Plot a heatmap\n",
    "  mask = np.zeros_like(corr)\n",
    "  mask[np.triu_indices_from(mask)] = True\n",
    "  quarter_features = np.round((n_features/4), 0)\n",
    "  plt.figure(figsize = (quarter_features, quarter_features))\n",
    "  sns.heatmap(corr, square = True, \n",
    "    annot = True, cmap = 'coolwarm', mask = mask);\n",
    "  plt.suptitle(f'Relationships Between Variables', size = 24)\n",
    "  plt.title(f'Based on {n_grand} Observations out of {n_grand}', size = 18)\n",
    "  # plt.tight_layout()\n",
    "  plt.savefig(f'./{a}/all_heatmap.png')\n",
    "  plt.close()\n",
    "  \n",
    "  # Plot a heatmap column on y\n",
    "  if y in corr:\n",
    "    plt.figure(figsize = (16, 9))\n",
    "    sns.heatmap(np.asarray([corr[y].sort_values(ascending = False)]).T, \n",
    "      vmin = 0, vmax = 1, annot = True, cmap = 'coolwarm')\n",
    "    plt.suptitle(f'Relationship between Predictors and {ty}', size = 24)\n",
    "    plt.title(f'Based on {n_grand} Observations out of {n_grand}', size = 18)\n",
    "    plt.xlabel(f'{ty}', size = 20)\n",
    "    plt.yticklabels = True\n",
    "    # plt.tight_layout()\n",
    "    plt.savefig(f'./{a}/all-by-{y}_heatmap.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "\n",
    "\n",
    "CSVs created (in the main project folder):\n",
    "\n",
    "\n",
    "`.py` files created: \n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 1 in full:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 2 in full:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 3 in full:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 4 in full:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
