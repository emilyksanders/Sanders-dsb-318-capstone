{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuesday, May 21, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on 5/21/24 I googled around for a dataset and found one thru SAMHSA.  it includes everyone, queer and otherwise, but it looks promising.  it's really, really big though, so it's taking a lot of cleaning before I can even evaluate whether it's what I want or not.  maybe i should just define that it is what I want, and then figure out what that is as I clean it.  I definitely think I could make a project out of it, even if it's not the project of my dreams.  this dataset has nearly 3000 columns and it's running VERY slow in Rstudio.  after finding it, I spend several hours going thru the data dictionary to try and eliminate columns.  I got thru the \"self-administered substance use sections,\" and will pick up tomorrow on \"imputed substance use.\"\n",
    "\n",
    "files created:\n",
    "- everything in the potential datasets folder\n",
    "- 2024-05-21_set-up.py\n",
    "- evaluating-columns-in-samhsa-dataset.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capstone Setup\n",
    "\n",
    "# Module Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, date, time\n",
    "from string import capwords\n",
    "\n",
    "# Working Directory\n",
    "os.getcwd()\n",
    "os.chdir('C:/Users/emily/Git_Stuff/General_Assembly/04_Projects/project-capstone')\n",
    "os.listdir()\n",
    "\n",
    "# Import Data\n",
    "# (This will likely change as my dataset choice changes!)\n",
    "df = pd.read_csv(\n",
    "  './potential_datasets/2024-05-21_download_SAMHSA_NSDUH-2019-DS0001/NSDUH_2019_Tab.txt', \n",
    "  sep = '\\t', low_memory=False)\n",
    "\n",
    "# This thing has 2741 columns!  \n",
    "# I'm going to pair it down by printing out the column names, \n",
    "# copying them to a text file, and then\n",
    "# going through the codebook to see which ones I really need.\n",
    "\n",
    "#print(list(df.columns))\n",
    "# commented out bcz omg.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wednesday, May 22, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today I am continuing to clean that SAMHSA dataset.  I am feeling more and more strongly that I should just make a project out of this, whatever it is, because I am investing so much time into investigating it.  I can always find a more queer-centric dataset later and play with it in my portfolio.\n",
    "\n",
    "~I am going to have multiple scripts today,~ nope, had a power outage instead!\n",
    "\n",
    "files created:\n",
    "- 2024-05-22_continued_column_exploration_of_samhsa_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May 22, 2024\n",
    "\n",
    "# continued column cleaning in SAMHSA dataset\n",
    "# I'm really starting to think that when I get to the bottom of this one, I should just use it\n",
    "\n",
    "# Status: I have gone thru a lot of columns by hand, and noticed some patterns.\n",
    "# Specifically, for rightnow, I have noticed that all variables that only serve to show when another variable has been imputed\n",
    "# start with II.  Therefore, I'm going to use a list comprehension (or similar) to eliminate them all at once.\n",
    "\n",
    "# Here are the remaining column names.  I'm copying them in from evaluating-columns-in-samhsa-dataset.txt, and when I'm done,\n",
    "# I will copy them back.  I am deleting column names in that dataset as I go along.  If I ever need a full list\n",
    "# of the original column names again, I can always pull them again from the original dataset.\n",
    "\n",
    "col_names_1 = ['IRCGRFM', 'IICGRFM', 'II2CGRFM', 'IRSMKLSS30N', 'IISMKLSS30N', 'IRALCFM', 'IIALCFM', 'II2ALCFM', 'IRALCBNG30D', 'IIALCBNG30D', 'IRMJFM', 'IIMJFM', 'II2MJFM', 'IRCOCFM', 'IICOCFM', 'II2COCFM', 'IRCRKFM', 'IICRKFM', 'II2CRKFM', 'IRHERFM', 'IIHERFM', 'II2HERFM', 'IRHALLUC30N', 'IIHALLUC30N', 'IRINHAL30N', 'IIINHAL30N', 'IRMETHAM30N', 'IIMETHAM30N', 'IRPNRNM30FQ', 'IIPNRNM30FQ', 'IRTRQNM30FQ', 'IITRQNM30FQ', 'IRSTMNM30FQ', 'IISTMNM30FQ', 'IRSEDNM30FQ', 'IISEDNM30FQ', 'IRCIGAGE', 'IICIGAGE', 'IRCIGYFU', 'IICIGYFU', 'IRCDUAGE', 'IICDUAGE', 'IRCD2YFU', 'IICD2YFU', 'IRCGRAGE', 'IICGRAGE', 'IRCGRYFU', 'IICGRYFU', 'IRSMKLSSTRY', 'IISMKLSSTRY', 'IRSMKLSSYFU', 'IISMKLSSYFU', 'IRALCAGE', 'IIALCAGE', 'IRALCYFU', 'IIALCYFU', 'IRMJAGE', 'IIMJAGE', 'IRMJYFU', 'IIMJYFU', 'IRCOCAGE', 'IICOCAGE', 'IRCOCYFU', 'IICOCYFU', 'IRCRKAGE', 'IICRKAGE', 'IRCRKYFU', 'IICRKYFU', 'IRHERAGE', 'IIHERAGE', 'IRHERYFU', 'IIHERYFU', 'IRHALLUCAGE', 'IIHALLUCAGE', 'IRHALLUCYFU', 'IIHALLUCYFU', 'IRLSDAGE', 'IILSDAGE', 'IRLSDYFU', 'IILSDYFU', 'IRPCPAGE', 'IIPCPAGE', 'IRPCPYFU', 'IIPCPYFU', 'IRECSTMOAGE', 'IIECSTMOAGE', 'IRECSTMOYFU', 'IIECSTMOYFU', 'IRINHALAGE', 'IIINHALAGE', 'IRINHALYFU', 'IIINHALYFU', 'IRMETHAMAGE', 'IIMETHAMAGE', 'IRMETHAMYFU', 'IIMETHAMYFU', 'IRPNRNMINIT', 'IIPNRNMINIT', 'IRTRQNMINIT', 'IITRQNMINIT', 'IRSTMNMINIT', 'IISTMNMINIT', 'IRSEDNMINIT', 'IISEDNMINIT', 'IRPNRNMYFU', 'IIPNRNMYFU', 'IRPNRNMAGE', 'IIPNRNMAGE', 'IRTRQNMYFU', 'IITRQNMYFU', 'IRTRQNMAGE', 'IITRQNMAGE', 'IRSTMNMYFU', 'IISTMNMYFU', 'IRSTMNMAGE', 'IISTMNMAGE', 'IRSEDNMYFU', 'IISEDNMYFU', 'IRSEDNMAGE', 'IISEDNMAGE', 'CIGFLAG', 'CIGYR', 'CIGMON', 'CGRFLAG', 'CGRYR', 'CGRMON', 'PIPFLAG', 'PIPMON', 'SMKLSSFLAG', 'SMKLSSYR', 'SMKLSSMON', 'TOBFLAG', 'TOBYR', 'TOBMON', 'ALCFLAG', 'ALCYR', 'ALCMON', 'MRJFLAG', 'MRJYR', 'MRJMON', 'COCFLAG', 'COCYR', 'COCMON', 'CRKFLAG', 'CRKYR', 'CRKMON', 'HERFLAG', 'HERYR', 'HERMON', 'HALLUCFLAG', 'HALLUCYR', 'HALLUCMON', 'LSDFLAG', 'LSDYR', 'LSDMON', 'PCPFLAG', 'PCPYR', 'PCPMON', 'ECSTMOFLAG', 'ECSTMOYR', 'ECSTMOMON', 'DAMTFXFLAG', 'DAMTFXYR', 'DAMTFXMON', 'KETMINFLAG', 'KETMINYR', 'KETMINMON', 'SALVIAFLAG', 'SALVIAYR', 'SALVIAMON', 'INHALFLAG', 'INHALYR', 'INHALMON', 'METHAMFLAG', 'METHAMYR', 'METHAMMON', 'PNRANYFLAG', 'PNRANYYR', 'OXYCNANYYR', 'TRQANYFLAG', 'TRQANYYR', 'STMANYFLAG', 'STMANYYR', 'SEDANYFLAG', 'SEDANYYR', 'TQSDANYFLG', 'TQSDANYYR', 'PSYANYFLAG', 'PSYANYYR', 'PNRNMFLAG', 'PNRNMYR', 'PNRNMMON', 'OXYCNNMYR', 'TRQNMFLAG', 'TRQNMYR', 'TRQNMMON', 'STMNMFLAG', 'STMNMYR', 'STMNMMON', 'SEDNMFLAG', 'SEDNMYR', 'SEDNMMON', 'TQSDNMFLAG', 'TQSDNMYR', 'TQSDNMMON', 'PSYCHFLAG', 'PSYCHYR', 'PSYCHMON', 'OPINMYR', 'OPINMMON', 'HERPNRYR', 'ILLFLAG', 'ILLYR', 'ILLMON', 'MJONLYFLAG', 'MJONLYYR', 'MJONLYMON', 'ILLEMFLAG', 'ILLEMYR', 'ILLEMMON', 'CDUFLAG', 'DCIGMON', 'CDCGMO', 'CDNOCGMO', 'BNGDRKMON', 'HVYDRKMON', 'ILTOBALCFG', 'ILTOBALCYR', 'ILTOBALCMN', 'ILLALCMON', 'TOBALCFLG', 'TOBALCYR', 'TOBALCMN', 'ILLANDALC', 'ILLORALC', 'ILLALCFLG', 'PEYOTE2', 'MESC2', 'PSILCY2', 'AMYLNIT2', 'CLEFLU2', 'GAS2', 'GLUE2', 'ETHER2', 'SOLVENT2', 'LGAS2', 'NITOXID2', 'FELTMARKR2', 'SPPAINT2', 'AIRDUSTER2', 'OTHAEROS2', 'HYDCPDAPYU', 'ZOHYANYYR2', 'OXCOPDAPYU', 'TRAMPDAPYU', 'CODEPDAPYU', 'MORPPDAPYU', 'FENTPDAPYU', 'BUPRPDAPYU', 'OXYMPDAPYU', 'DEMEPDAPYU', 'HYDMPDAPYU', 'MTDNPDAPYU', 'PNROTANYR2', 'TRBENZAPYU', 'ALPRPDAPYU', 'LORAPDAPYU', 'CLONPDAPYU', 'DIAZPDAPYU', 'MUSRLXAPYU', 'CYCLPDAPYU', 'SOMAPDAPYU', 'TRQOTANYR2', 'AMMEPDAPYU', 'AMPHETAPYU', 'METHPDAPYU', 'ANOSTMAPYU', 'PROVPDAPYU', 'STMOTANYR2', 'ZOLPPDAPYU', 'ESZOPDAPYU', 'ZALEPDAPYU', 'SVBENZAPYU', 'TRIAPDAPYU', 'TEMAPDAPYU', 'FLURPDAPYU', 'BARBITAPYU', 'SEDOTANYR2', 'BENZOSAPYU', 'HYDCPDPYMU', 'OXCOPDPYMU', 'TRAMPDPYMU', 'CODEPDPYMU', 'MORPPDPYMU', 'FENTPDPYMU', 'BUPRPDPYMU', 'OXYMPDPYMU', 'DEMEPDPYMU', 'HYDMPDPYMU', 'MTDNPDPYMU', 'PNROTHPYMU2', 'TRBENZPYMU', 'ALPRPDPYMU', 'LORAPDPYMU', 'CLONPDPYMU', 'DIAZPDPYMU', 'MUSRLXPYMU', 'CYCLPDPYMU', 'SOMAPDPYMU', 'TRQOTHPYMU2', 'AMMEPDPYMU', 'AMPHETPYMU', 'METHPDPYMU', 'ANOSTMPYMU', 'PROVPDPYMU', 'STMOTHPYMU2', 'ZOLPPDPYMU', 'ESZOPDPYMU', 'SVBENZPYMU', 'SEDOTHPYMU2', 'BENZOSPYMU', 'ALCYDAYS', 'MRJYDAYS', 'COCYDAYS', 'CRKYDAYS', 'HERYDAYS', 'HALLNDAYYR', 'INHNDAYYR', 'METHNDAYYR', 'CIGMDAYS', 'CGRMDAYS', 'SMKLSMDAYS', 'ALCMDAYS', 'MRJMDAYS', 'COCMDAYS', 'CRKMDAYS', 'HERMDAYS', 'HALLNDAYPM', 'INHNDAYPM', 'METHNDAYPM', 'PNRNDAYPM', 'TRQNDAYPM', 'STMNDAYPM', 'SEDNDAYPM', 'BNGDRMDAYS', 'CIGPDAY', 'CIG1PACK', 'CIGAVGD', 'CIGAVGM', 'ALCNUMDKPM', 'FUCIG18', 'FUCIG21', 'FUCD218', 'FUCD221', 'FUCGR18', 'FUCGR21', 'FUSMKLSS18', 'FUSMKLSS21', 'FUALC18', 'FUALC21', 'FUMJ18', 'FUMJ21', 'FUCOC18', 'FUCOC21', 'FUCRK18', 'FUCRK21', 'FUHER18', 'FUHER21', 'FUHALLUC18', 'FUHALLUC21', 'FULSD18', 'FULSD21', 'FUPCP18', 'FUPCP21', 'FUECSTMO18', 'FUECSTMO21', 'FUINHAL18', 'FUINHAL21', 'FUMETHAM18', 'FUMETHAM21', 'FUPNRNM18', 'FUPNRNM21', 'FUTRQNM18', 'FUTRQNM21', 'FUSTMNM18', 'FUSTMNM21', 'FUSEDNM18', 'FUSEDNM21', 'PNRMAINRSN', 'TRQMAINRSN', 'STMMAINRSN', 'SEDMAINRSN', 'SRCPNRNM2', 'SRCTRQNM2', 'SRCSTMNM2', 'SRCSEDNM2', 'SRCFRPNRNM', 'SRCFRTRQNM', 'SRCFRSTMNM', 'SRCFRSEDNM', 'SRCCLFRPNR', 'SRCCLFRTRQ', 'SRCCLFRSTM', 'SRCCLFRSED', 'COLDMEDS', 'COLDREC', 'COLDYR1', 'COLDYR2', 'COLDYR3', 'COLDYR4', 'COLDYR5', 'OTCFLAG', 'GHB', 'GHBREC', 'COCNEEDL', 'CONDLREC', 'HERSMOKE', 'HRSMKREC', 'HERSNIFF', 'HRSNFREC', 'HERNEEDL', 'HEOTSMK', 'HEOTSNF', 'HEOTNDL', 'HEOTOTH', 'HEOTSP', 'HRNDLREC', 'METHNEEDL', 'METHNDLRC', 'OTDGNEDL', 'OTDGNDLA', 'OTDGNDLB', 'OTDGNDLC', 'OTDGNDLD', 'OTDGNDLE', 'OTDGNDLRC', 'GNNDREUS', 'GNNDLSH1', 'GNNDCLEN', 'GNNDLSH2', 'GNNDGET2', 'ANYNDLREC', 'CHMNDLREC', 'ANYNEEDL', 'NEDHER', 'NEDCOC', 'METHNEEDL2', 'HERSMOK2', 'HERSNIF2', 'COLDFLGR', 'COLDYRR', 'COLDMONR', 'GHBFLGR', 'GHBYRR', 'GHBMONR', 'RSKCIGPKD', 'RSKMRJMON', 'RSKMRJWK', 'RSKLSDTRY', 'RSKLSDWK', 'RSKHERTRY', 'RSKHERWK', 'RSKCOCMON', 'RSKCOCWK', 'RSKBNGDLY', 'RSKBNGWK', 'DIFGETMRJ', 'DIFGETLSD', 'DIFGETCOC', 'DIFGETCRK', 'DIFGETHER', 'APPDRGMON', 'RSKYFQDGR', 'RSKYFQTES', 'RKFQPBLT', 'RKFQDBLT', 'GRSKCIGPKD', 'GRSKMRJMON', 'GRSKMRJWK', 'GRSKCOCMON', 'GRSKCOCWK', 'GRSKHERTRY', 'GRSKHERWK', 'GRSKLSDTRY', 'GRSKLSDWK', 'GRSKBNGDLY', 'GRSKBNGWK', 'DIFOBTMRJ', 'DIFOBTCOC', 'DIFOBTCRK', 'DIFOBTHER', 'DIFOBTLSD', 'APPDRGMON2', 'BLNTEVER', 'BLNTAGE', 'BLNTYFU', 'BLNTMFU', 'BLNTREC', 'BLRECFL2', 'BLNT30DY','BLNT30C1', 'BLNT30C2', 'RSNOMRJ', 'RSNMRJMO', 'BLNTNOMJ', 'MEDMJYR', 'MEDMJALL', 'MEDMJPA2', 'CIGIRTBL', 'CIGCRAVE', 'CIGCRAGP', 'CIGINCTL', 'CIGAVOID', 'CIGFNSMK', 'CIGFNLKE', 'CIGPLANE', 'CIGRNOUT', 'CIGREGDY', 'CIGREGWK', 'CIGREGNM', 'CIGNMCHG', 'CIGSVLHR', 'CIGINFLU', 'CIGNOINF', 'CIGINCRS', 'CIGSATIS', 'CIGLOTMR', 'CIGWAKE', 'ALCLOTTM', 'ALCGTOVR', 'ALCLIMIT', 'ALCKPLMT', 'ALCNDMOR', 'ALCLSEFX', 'ALCCUTDN', 'ALCCUTEV', 'ALCCUT1X', 'ALCWD2SX', 'ALCWDSMT', 'ALCEMOPB', 'ALCEMCTD', 'ALCPHLPB', 'ALCPHCTD', 'ALCLSACT', 'ALCSERPB', 'ALCPDANG', 'ALCLAWTR', 'ALCFMFPB', 'ALCFMCTD', 'MRJLOTTM', 'MRJGTOVR', 'MRJLIMIT', 'MRJKPLMT', 'MRJNDMOR', 'MRJLSEFX', 'MRJCUTDN', 'MRJCUTEV', 'MRJEMOPB', 'MRJEMCTD', 'MRJPHLPB', 'MRJPHCTD', 'MRJLSACT', 'MRJSERPB', 'MRJPDANG', 'MRJLAWTR', 'MRJFMFPB', 'MRJFMCTD', 'COCLOTTM', 'COCGTOVR', 'COCLIMIT', 'COCKPLMT', 'COCNDMOR', 'COCLSEFX', 'COCCUTDN', 'COCCUTEV', 'COCCUT1X', 'COCFLBLU', 'COCWD2SX', 'COCWDSMT', 'COCEMOPB', 'COCEMCTD', 'COCPHLPB', 'COCPHCTD', 'COCLSACT', 'COCSERPB', 'COCPDANG', 'COCLAWTR', 'COCFMFPB', 'COCFMCTD', 'HERLOTTM', 'HERGTOVR', 'HERLIMIT', 'HERKPLMT', 'HERNDMOR', 'HERLSEFX', 'HERCUTDN', 'HERCUTEV', 'HERCUT1X', 'HERWD3SX', 'HERWDSMT', 'HEREMOPB', 'HEREMCTD', 'HERPHLPB', 'HERPHCTD', 'HERLSACT', 'HERSERPB', 'HERPDANG', 'HERLAWTR', 'HERFMFPB', 'HERFMCTD', 'HALULOTTM', 'HALUGTOVR', 'HALULIMIT', 'HALUKPLMT', 'HALUNDMOR', 'HALULSEFX', 'HALUCUTDN', 'HALUCUTEV', 'HALUEMOPB', 'HALUEMCTD', 'HALUPHLPB', 'HALUPHCTD', 'HALULSACT', 'HALUSERPB', 'HALUPDANG', 'HALULAWTR', 'HALUFMFPB', 'HALUFMCTD', 'INHLLOTTM', 'INHLGTOVR', 'INHLLIMIT', 'INHLKPLMT', 'INHLNDMOR', 'INHLLSEFX', 'INHLCUTDN', 'INHLCUTEV', 'INHLEMOPB', 'INHLEMCTD', 'INHLPHLPB', 'INHLPHCTD', 'INHLLSACT', 'INHLSERPB', 'INHLPDANG', 'INHLLAWTR', 'INHLFMFPB', 'INHLFMCTD', 'METHLOTTM', 'METHGTOVR']\n",
    "col_names_2 = ['METHLIMIT', 'METHKPLMT', 'METHNDMOR', 'METHLSEFX', 'METHCUTDN', 'METHCUTEV', 'METHCUT1X', 'METHFLBLU', 'METHWD2SX', 'METHWDSMT', 'METHEMOPB', 'METHEMCTD', 'METHPHLPB', 'METHPHCTD', 'METHLSACT', 'METHSERPB', 'METHPDANG', 'METHLAWTR', 'METHFMFPB', 'METHFMCTD', 'PNRLLOTTM', 'PNRLGTOVR', 'PNRLLIMIT', 'PNRLKPLMT', 'PNRLNDMOR', 'PNRLLSEFX', 'PNRLCUTDN', 'PNRLCUTEV', 'PNRLCUT1X', 'PNRLWD3SX', 'PNRLWDSMT', 'PNRLEMOPB', 'PNRLEMCTD', 'PNRLPHLPB', 'PNRLPHCTD', 'PNRLLSACT', 'PNRLSERPB', 'PNRLPDANG', 'PNRLLAWTR', 'PNRLFMFPB', 'PNRLFMCTD', 'TRQLLOTTM', 'TRQLGTOVR', 'TRQLLIMIT', 'TRQLKPLMT', 'TRQLNDMOR', 'TRQLLSEFT', 'TRQLCUTDN', 'TRQLCUTEV', 'TRQLEMOPB', 'TRQLEMCTD', 'TRQLPHLPB', 'TRQLPHCTD', 'TRQLLSACT', 'TRQLSERPB', 'TRQLPDANG', 'TRQLLAWTR', 'TRQLFMFPB', 'TRQLFMCTD', 'STIMLOTTM', 'STIMGTOVR', 'STIMLIMIT', 'STIMKPLMT', 'STIMNDMOR', 'STIMLSEFX', 'STIMCUTDN', 'STIMCUTEV', 'STIMCUT1X', 'STIMFLBLU', 'STIMWD2SX', 'STIMWDSMT', 'STIMEMOPB', 'STIMEMCTD', 'STIMPHLPB', 'STIMPHCTD', 'STIMLSACT', 'STIMSERPB', 'STIMPDANG', 'STIMLAWTR', 'STIMFMFPB', 'STIMFMCTD', 'SEDVLOTTM', 'SEDVGTOVR', 'SEDVLIMIT', 'SEDVKPLMT', 'SEDVNDMOR', 'SEDVLSEFX', 'SEDVCUTDN', 'SEDVCUTEV', 'SEDVCUT1X', 'SEDVWD1SX', 'SEDVWDSMT', 'SEDVEMOPB', 'SEDVEMCTD', 'SEDVPHLPB', 'SEDVPHCTD', 'SEDVLSACT', 'SEDVSERPB', 'SEDVPDANG', 'SEDVLAWTR', 'SEDVFMFPB', 'SEDVFMCTD', 'DEPENDHAL', 'DEPENDINH', 'DEPENDMTH', 'DEPENDPNR', 'DEPENDTRQ', 'DEPENDSTM', 'DEPENDSED', 'ABUPOSHAL', 'ABUPOSINH', 'ABUPOSMTH', 'ABUPOSPNR', 'ABUPOSTRQ', 'ABUPOSSTM', 'ABUPOSSED', 'IRCGIRTB', 'IICGIRTB', 'IRCGCRV', 'IICGCRV', 'IRCGCRGP', 'IICGCRGP', 'IRCGNCTL', 'IICGNCTL', 'IRCGAVD', 'IICGAVD', 'IRCGPLN', 'IICGPLN', 'IRCGROUT', 'IICGROUT', 'IRCGRGDY', 'IICGRGDY', 'IRCGRGWK', 'IICGRGWK', 'IRCGRGNM', 'IICGRGNM', 'IRCGNCG', 'IICGNCG', 'IRCGSLHR', 'IICGSLHR', 'IRCGINFL', 'IICGINFL', 'IRCGNINF', 'IICGNINF', 'IRCGINCR', 'IICGINCR', 'IRCGSAT', 'IICGSAT', 'IRCGLMR', 'IICGLMR', 'IRDEPENDHAL', 'IIDEPENDHAL', 'IRDEPENDINH', 'IIDEPENDINH', 'IRDEPENDMTH', 'IIDEPENDMTH', 'IRDEPENDPNR', 'IIDEPENDPNR', 'IRDEPENDTRQ', 'IIDEPENDTRQ', 'IRDEPENDSTM', 'IIDEPENDSTM', 'IRDEPENDSED', 'IIDEPENDSED', 'IRABUPOSHAL', 'IIABUPOSHAL', 'IRABUPOSINH', 'IIABUPOSINH', 'IRABUPOSMTH', 'IIABUPOSMTH', 'IRABUPOSPNR', 'IIABUPOSPNR', 'IRABUPOSTRQ', 'IIABUPOSTRQ', 'IRABUPOSSTM', 'IIABUPOSSTM', 'IRABUPOSSED', 'IIABUPOSSED', 'NDSSANSP', 'NDSSDNSP', 'FTNDDNSP', 'DNICNSP', 'DEPNDALC', 'DEPNDMRJ', 'DEPNDCOC', 'DEPNDHER', 'DEPNDPYHAL', 'DEPNDPYINH', 'DEPNDPYMTH', 'DEPNDPYPNR', 'DEPNDPYTRQ', 'DEPNDPYSTM', 'DEPNDPYSED', 'DEPNDPYPSY', 'DEPNDPYILL', 'DEPNDPYIEM', 'DPPYILLALC', 'ABUSEALC', 'ABUSEMRJ', 'ABUSECOC', 'ABUSEHER', 'ABUSEPYHAL', 'ABUSEPYINH', 'ABUSEPYMTH', 'ABUSEPYPNR', 'ABUSEPYTRQ', 'ABUSEPYSTM', 'ABUSEPYSED', 'ABUSEPYPSY', 'ABUSEPYILL', 'ABUSEPYIEM', 'ABPYILLALC', 'ABODALC', 'ABODMRJ', 'ABODCOC', 'ABODHER', 'UDPYHAL', 'UDPYINH', 'UDPYMTH', 'UDPYPNR', 'UDPYTRQ', 'UDPYSTM', 'UDPYSED', 'UDPYTRQSED', 'UDPYPSY', 'UDPYOPI', 'UDPYHRPNR', 'UDPYILL', 'UDPYIEM', 'UDPYILAL', 'UDPYILAAL', 'DUDNOAUD', 'AUDNODUD', 'BOOKED', 'NOBOOKY2', 'BKMVTHFT', 'BKLARCNY', 'BKBURGL', 'BKSRVIOL', 'BKSMASLT', 'BKROB', 'BKARSON', 'BKDRVINF', 'BKDRUNK', 'BKPOSTOB', 'BKDRUG', 'BKSEXNR', 'BKFRAUD', 'BKOTH', 'BKOTHOF2', 'PROBATON', 'PAROLREL', 'DRVINALCO', 'DRVINMARJ', 'DRVINCOCN', 'DRVINHERN', 'DRVINHALL', 'DRVININHL', 'DRVINMETH', 'DRVINALON', 'MXMJPNLT', 'DRVINALCO2', 'DRVINMARJ2', 'DRVINDRG', 'DRVINDROTMJ', 'DRVINALDRG', 'PAROL', 'PROB', 'MRJYRBFR', 'MRJAGLST', 'MRJYLU', 'MRJMLU', 'CIGAGLST', 'CIGYLU', 'CIGMLU', 'CIGDLLST', 'CIGDLYLU', 'CIGDLMLU', 'SMKAGLAST', 'SMKYRLAST', 'SMKMOLAST', 'CGRAGLST', 'CIGARYLU', 'CIGARMLU', 'ALCAGLST', 'ALCYLU', 'ALCMLU', 'COCAGLST', 'COCYLU', 'COCMLU', 'CRKAGLST', 'CRKYLU', 'CRKMLU', 'HERAGLST', 'HERYLU', 'HERMLU', 'HALLAGLST', 'HALLYRLST', 'HALLMOLST', 'LSDAGLST', 'LSDYLU', 'LSDMLU', 'PCPAGLST', 'PCPYLU', 'PCPMLU', 'ECSTMOAGL', 'ECSTMOYLU', 'ECSTMOMLU', 'INHLAGLST', 'INHLYRLST', 'INHLMOLST', 'METHAGLST', 'METHYRLST', 'METHMOLST', 'CIGYRBFR', 'ALCYRBFR', 'COCYRBFR', 'TXEVRRCVD', 'TXYRRECVD', 'TXYRALDGB', 'TXYRHOSOV', 'TXYRHOSAD', 'TXYRRESOV', 'TXYRRESAD', 'TXYROUTPT', 'TXYROUTAD', 'TXYRMHCOP', 'TXYRMHCAD', 'TXYREMRGN', 'TXYREMRAD', 'TXYRDRPRV', 'TXYRDRPAD', 'TXYRPRISN', 'TXYRPRIAD', 'TXYRSLFHP', 'TXYRSLFAD', 'TXYROTHER', 'TXYROTHSP2', 'TXYROTHAD', 'TXYRERDRG', 'TXYRERNUM2', 'TXCURRENT', 'NDTXYRADG', 'NDMORTXYR', 'NDMORTALC', 'NDMORTMRJ', 'NDMORTCOC', 'NDMORTHER', 'NDMORTHAL', 'NDMORTINH', 'NDMORTMTH', 'NDMORTPNR', 'NDMORTTRQ', 'NDMORTSTM', 'NDMORTSED', 'NDMORTOTH', 'NDTXYRALC', 'NDTXYRMRJ', 'NDTXYRCOC', 'NDTXYRHER', 'NDTXYRHAL', 'NDTXYRINH', 'NDTXYRMTH', 'NDTXYRPNR', 'NDTXYRTRQ', 'NDTXYRSTM', 'NDTXYRSED', 'NDTXYROTH', 'NDTXYOTH1', 'NDTXYOTH2', 'NDTXYOTH3', 'NDTXYOTH4', 'NDTXYOTH5', 'NDTXEFFRT', 'NDTXNOCOV', 'NDTXNOTPY', 'NDTXTSPHR', 'NDTXWANTD', 'NDTXNSTOP', 'NDTXPFULL', 'NDTXDKWHR', 'NDTXNBRNG', 'NDTXJOBNG', 'NDTXNONED', 'NDTXHANDL', 'NDTXNOHLP', 'NDTXNTIME', 'NDTXFNDOU', 'NDTXOTRSN', 'NDTXMIMPT', 'NDMREFFRT', 'NDMRNOCOV', 'NDMRNOTPY', 'NDMRTSPHR', 'NDMRWANTD', 'NDMRNSTOP', 'NDMRPFULL', 'NDMRDKWHR', 'NDMRNBRNG', 'NDMRJOBNG', 'NDMRNONED', 'NDMRHANDL', 'NDMRNOHLP', 'NDMRNTIME', 'NDMRFNDOU', 'NDMROTRSN', 'NDMRMIMPT', 'TXRCVDREC', 'TXLTYMNPL2', 'TXLTYALCO', 'TXLTYMRJH', 'TXLTYCOCN', 'TXLTYHERN', 'TXLTYHALL', 'TXLTYINHL', 'TXLTYMETH', 'TXLTYPNRL', 'TXLTYTRQL', 'TXLTYSTIM', 'TXLTYSEDV', 'TXLTYOTHR', 'TXLTYMAIN2', 'TXLTYOCOM2', 'TXLTYDAYS2', 'TXPAYHINS', 'TXPAYMCRE', 'TXPAYMCAD', 'TXPAYPUBL', 'TXPAYSVNG', 'TXPAYFAML', 'TXPAYCOUR', 'TXPAYMILT', 'TXPAYBOSS', 'TXPAYOTHR', 'TXPAYOTSP2', 'TXPAYFREE', 'TXENRLOCT', 'TXYRONDTX', 'TXALCONLY', 'TXALCONAG', 'TXDRGONLY', 'TXDRGONAG', 'TXALCDRGU', 'TXALCDAGE', 'TXDRGALCU', 'TXDRGAAGE', 'TXYALONAG', 'TXYALODRG', 'TXYALODAG', 'TXYDRONAG', 'TXYDROALC', 'TXYDROAAG', 'TXYALDAAG', 'TXYALDDAG', 'TXFGALAGE', 'TXFGDGAGE', 'TXFGADAGE', 'TXSHGWENT', 'TXSHGALDB', 'TXSHGFLAG', 'TXEVRRCVD2', 'TXYRALC', 'TXYRILL', 'TXYRALNIL', 'TXYRILNAL', 'TXYRRECVD2', 'TXYRILANAL', 'TXLTYALCO2', 'TXLTYMRJH2', 'TXLTYCOCN2', 'TXLTYHERN2', 'TXLTYHALL2', 'TXLTYINHL2', 'TXLTYMETH2', 'TXLTYPNRL2', 'TXLTYTRQL2', 'TXLTYSTIM2', 'TXLTYSEDV2', 'TXLTYILL', 'TXPAYHINS2', 'TXPAYMCRE2', 'TXPAYMCAD2', 'TXPAYPUBL2', 'TXPAYSVNG2', 'TXPAYFAML2', 'TXPAYCOUR2', 'TXPAYMILT2', 'TXPAYBOSS2', 'TXPDHINSAL', 'TXPDMCREAL', 'TXPDMCADAL', 'TXPDPUBLAL', 'TXPDSVNGAL', 'TXPDFAMLAL', 'TXPDCOURAL', 'TXPDMILTAL', 'TXPDBOSSAL', 'TXPDHINSIL', 'TXPDMCREIL', 'TXPDMCADIL', 'TXPDPUBLIL', 'TXPDSVNGIL', 'TXPDFAMLIL', 'TXPDCOURIL', 'TXPDMILTIL', 'TXPDBOSSIL', 'TXYRSPALC', 'TXYRSPILL', 'TXYSPALNIL', 'TXYSPILNAL', 'TXYRSPILAL', 'TXYSILANAL', 'TXLTCURRSP', 'TXYRHOSAL', 'TXYRRESAL', 'TXYROUTAL', 'TXYRMHCAL', 'TXYREMRAL', 'TXYRDRPAL', 'TXYRPRIAL', 'TXYRSLFAL', 'TXYRHOSIL', 'TXYRRESIL', 'TXYROUTIL', 'TXYRMHCIL', 'TXYREMRIL', 'TXYRDRPIL', 'TXYRPRIIL', 'TXYRSLFIL', 'TXYRHOSOV2', 'TXYRRESOV2', 'TXYROUTPT2', 'TXYRMHCOP2', 'TXYREMRGN2', 'TXYRDRPRV2', 'TXYRPRISN2', 'TXYRSLFHP2', 'TXYRNDALC', 'TXYRNDILL', 'TXYRNDILAL', 'NDFLTXALC', 'NDFLTXILL', 'NDFLTXILAL', 'NDTXEFTALC', 'NDTXEFTILL', 'NDTXEFILAL', 'TXYRNOSPAL', 'TXYRNOSPIL', 'TXYNSPILAL', 'NDTRNNOCOV', 'NDTRNNOTPY', 'NDTRNTSPHR', 'NDTRNWANTD', 'NDTRNNSTOP', 'NDTRNPFULL', 'NDTRNDKWHR', 'NDTRNNBRNG', 'NDTRNJOBNG', 'NDTRNNONED', 'NDTRNHANDL', 'NDTRNNOHLP', 'NDTRNNTIME', 'NDTRNFNDOU', 'NDTRNMIMPT', 'PREGNANT', 'HTANSWER', 'HTINCHE2', 'WTANSWER', 'WTPOUND2', 'NMERTMT2', 'INHOSPYR', 'NMNGTHS2', 'NMVSOPT2', 'NMVSOEST', 'HPUSETOB', 'HPUSEALC', 'HPUSEDRG', 'HPQTTOB', 'HPALCAMT', 'HPALCFRQ', 'HPALCPRB', 'HPALCCUT', 'HPALCTX', 'HPALCNOT', 'HPDRGTALK', 'STDANYYR', 'HRTCONDEV', 'DIABETEVR', 'COPDEVER', 'CIRROSEVR', 'HEPBCEVER', 'KIDNYDSEV', 'ASTHMAEVR', 'HIVAIDSEV', 'CANCEREVR', 'HIGHBPEVR', 'NONABOVEV', 'CABLADDER', 'CABLOLEULYM', 'CAOTHER2', 'CABREAST', 'CACERVIX', 'CACOLNRECT', 'CAESOPSTOM', 'CAGALLIVPAN', 'CAKIDNEY', 'CALARYLUNG', 'CAMELANOM', 'CAMOUTTHRO', 'CAOVARY', 'CAPROSTEST', 'CASKINOTH', 'CASKINDK', 'CATHYROID', 'CAUTERUS', 'CANCERYR', 'HRTCONDAG', 'HRTCONDYR', 'DIABETEAG', 'COPDAGE', 'CIRROSAGE', 'HEPBCAGE', 'KIDNYDSAG', 'ASTHMAAGE', 'ASTHMANOW', 'HIVAIDSAG', 'HIGHBPMED', 'HIGHBPAGE', 'PREG', 'PREG2', 'TRIMEST', 'BMI2']\n",
    "col_names_3 = ['AUINPYR', 'AUINPSYH', 'AUINPGEN', 'AUINMEDU', 'AUINAHSP', 'AUINRESD', 'AUINSFAC', 'AUNMPSY2', 'AUNMPGE2', 'AUNMMED2', 'AUNMAHS2', 'AUNMRES2', 'AUNMSFA2', 'AUPINSLF', 'AUPINOFM', 'AUPINPHI', 'AUPINMCR', 'AUPINMCD', 'AUPINREH', 'AUPINEMP', 'AUPINMIL', 'AUPINPUB', 'AUPINPRV', 'AUPINFRE', 'AUPINFM2', 'AUOPTYR', 'AUOPMENT', 'AUOPTHER', 'AUOPDOC', 'AUOPCLNC', 'AUOPDTMT', 'AUOPOTOP', 'AUOPYRS2', 'AUNMMEN2', 'AUNMTHE2', 'AUNMDOC2', 'AUNMCLN2', 'AUNMDTM2', 'AUNMOTO2', 'AUPOPSLF', 'AUPOPOFM', 'AUPOPPHI', 'AUPOPMCR', 'AUPOPMCD', 'AUPOPREH', 'AUPOPEMP', 'AUPOPMIL', 'AUPOPPUB', 'AUPOPPRV', 'AUPOPFRE', 'AUPOPMOS', 'AUPOPAMT', 'AURXYR', 'AUUNMTYR', 'AUUNCOST', 'AUUNNBR', 'AUUNJOB', 'AUUNNCOV', 'AUUNENUF', 'AUUNWHER', 'AUUNCFID', 'AUUNCMIT', 'AUUNNOND', 'AUUNHNDL', 'AUUNNHLP', 'AUUNBUSY', 'AUUNFOUT', 'AUUNNTSP', 'AUUNSOR', 'AUUNRIM2', 'AUALTYR', 'AUALACUP', 'AUALCHIR', 'AUALHERB', 'AUALSGRP', 'AUALINET', 'AUALRELG', 'AUALHLIN', 'AUALMASG', 'AUALOTH', 'AUALOTS2', 'AUMOTVYR', 'AMHINP2', 'AMHOUTP3', 'AMHRX2', 'AMHTXRC3', 'AMHSVTYP', 'AMHTXND2', 'AMHTXAND', 'MHLMNT3', 'MHLTHER3', 'MHLDOC3', 'MHLCLNC3', 'MHLDTMT3', 'MHLSCHL3', 'MHLOTH3', 'MHPDSLF2', 'MHPDOFM2', 'MHPDPHI2', 'MHPDMCR2', 'MHPDMCD2', 'MHPDREH2', 'MHPDEMP2', 'MHPDMIL2', 'MHPDPUB2', 'MHPDPRV2', 'MHPDFRE2', 'MHRCOST2', 'MHRNBRS2', 'MHRJOBS2', 'MHRNCOV2', 'MHRENUF2', 'MHRWHER2', 'MHRCFID2', 'MHRCMIT2', 'MHRNOND2', 'MHRHAND2', 'MHRNOHP2', 'MHRTIME2', 'MHRFOUT2', 'MHRTRAN2', 'MHRSOTH2', 'RCVMHOSPTX', 'RCVMHNSPTX', 'RCVSPTXNMH', 'RCVMHASPTX', 'SNYSELL', 'SNYSTOLE', 'SNYATTAK', 'SNFAMJEV', 'SNRLGSVC', 'SNRLGIMP', 'SNRLDCSN', 'SNRLFRND', 'YEATNDYR', 'YEHMSLYR', 'YESCHFLT', 'YESCHWRK', 'YESCHIMP', 'YESCHINT', 'YETCGJOB', 'YELSTGRD', 'YESTSCIG', 'YESTSMJ', 'YESTSALC', 'YESTSDNK', 'YEPCHKHW', 'YEPHLPHW', 'YEPCHORE', 'YEPLMTTV', 'YEPLMTSN', 'YEPGDJOB', 'YEPPROUD', 'YEYARGUP', 'YEYFGTSW', 'YEYFGTGP', 'YEYHGUN', 'YEYSELL', 'YEYSTOLE', 'YEYATTAK', 'YEPPKCIG', 'YEPMJEVR', 'YEPMJMO', 'YEPALDLY', 'YEGPKCIG', 'YEGMJEVR', 'YEGMJMO', 'YEGALDLY', 'YEFPKCIG', 'YEFMJEVR', 'YEFMJMO', 'YEFALDLY', 'YETLKNON', 'YETLKPAR', 'YETLKBGF', 'YETLKOTA', 'YETLKSOP', 'YEPRTDNG', 'YEPRBSLV', 'YEVIOPRV', 'YEDGPRGP', 'YESLFHLP', 'YEPRGSTD', 'YESCHACT', 'YECOMACT', 'YEFAIACT', 'YEOTHACT', 'YEDECLAS', 'YEDERGLR', 'YEDESPCL', 'YEPVNTYR', 'YERLGSVC', 'YERLGIMP', 'YERLDCSN', 'YERLFRND', 'SCHFELT', 'TCHGJOB', 'AVGGRADE', 'STNDSCIG', 'STNDSMJ', 'STNDALC', 'STNDDNK', 'PARCHKHW', 'PARHLPHW', 'PRCHORE2', 'PRLMTTV2', 'PARLMTSN', 'PRGDJOB2', 'PRPROUD2', 'ARGUPAR', 'YOFIGHT2', 'YOGRPFT2', 'YOHGUN2', 'YOSELL2', 'YOSTOLE2', 'YOATTAK2', 'PRPKCIG2', 'PRMJEVR2', 'PRMJMO', 'PRALDLY2', 'YFLPKCG2', 'YFLTMRJ2', 'YFLMJMO', 'YFLADLY2', 'FRDPCIG2', 'FRDMEVR2', 'FRDMJMON', 'FRDADLY2', 'TALKPROB', 'PRTALK3', 'PRBSOLV2', 'PREVIOL2', 'PRVDRGO2', 'GRPCNSL2', 'PREGPGM2', 'YTHACT2', 'DRPRVME3', 'ANYEDUC3', 'RLGATTD', 'RLGIMPT', 'RLGDCSN', 'RLGFRND', 'DSTNRV30', 'DSTHOP30', 'DSTRST30', 'DSTCHR30', 'DSTEFF30', 'DSTNGD30', 'DSTWORST', 'DSTNRV12', 'DSTHOP12', 'DSTRST12', 'DSTCHR12', 'DSTEFF12', 'DSTNGD12', 'IMPREMEM', 'IMPCONCN', 'IMPGOUT', 'IMPGOUTM', 'IMPPEOP', 'IMPPEOPM', 'IMPSOC', 'IMPSOCM', 'IMPHHLD', 'IMPHHLDM', 'IMPRESP', 'IMPRESPM', 'IMPWORK', 'IMPWEEKS', 'IMPDYFRQ', 'IMPYDAYS', 'SUICTHNK', 'SUICPLAN', 'SUICTRY', 'K6SCMON', 'SPDMON', 'K6SCYR', 'K6SCMAX', 'SPDYR', 'MHSUITHK', 'MHSUTK_U', 'MHSUIPLN', 'MHSUITRY', 'WSPDSC2', 'WHODASC2', 'WHODASC3', 'SMIPP_U', 'SMIYR_U', 'AMIYR_U', 'SMMIYR_U', 'MMIYR_U', 'LMIYR_U', 'LMMIYRU', 'MI_CAT_U', 'SMISUDPY', 'AMISUDPY', 'LMMISUDPY', 'ADDPREV', 'ADDSCEV', 'ADLOSEV', 'ADDPDISC', 'ADDPLSIN', 'ADDSLSIN', 'ADLSI2WK', 'ADDPR2WK', 'ADWRHRS', 'ADWRDST', 'ADWRCHR', 'ADWRIMP', 'ADDPPROB', 'ADWRPROB', 'ADWRAGE', 'ADWRDEPR', 'ADWRDISC', 'ADWRLSIN', 'ADWRPLSR', 'ADWRELES', 'ADWREMOR', 'ADWRGAIN', 'ADWRGROW', 'ADWRPREG', 'ADWRGNL2', 'ADWRLOSE', 'ADWRDIET', 'ADWRLSL2', 'ADWRSLEP', 'ADWRSMOR', 'ADWRENRG', 'ADWRSLOW', 'ADWRSLNO', 'ADWRJITT', 'ADWRJINO', 'ADWRTHOT', 'ADWRCONC', 'ADWRDCSN', 'ADWRNOGD', 'ADWRWRTH', 'ADWRDLOT', 'ADWRDBTR', 'ADWRSTHK', 'ADWRSPLN', 'ADWRSATP', 'AD_MDEA1', 'AD_MDEA2', 'AD_MDEA3', 'AD_MDEA4', 'AD_MDEA5', 'AD_MDEA6', 'AD_MDEA7', 'AD_MDEA8', 'AD_MDEA9', 'ADSMMDEA', 'ADPBINTF', 'ADPBDLYA', 'ADPBRMBR', 'ADPBAGE', 'ADPBNUM', 'ADPB2WK', 'ADPSHMGT', 'ADPSWORK', 'ADPSRELS', 'ADPSSOC', 'ADPSDAYS', 'ADSEEDOC', 'ADFAMDOC', 'ADOTHDOC', 'ADPSYCH', 'ADPSYMD', 'ADSOCWRK', 'ADCOUNS', 'ADOTHMHP', 'ADNURSE', 'ADRELIG', 'ADHERBAL', 'ADOTHHLP', 'ADTMTNOW', 'ADRX12MO', 'ADRXNOW', 'ADRXHLP', 'ADTMTHLP', 'AMDELT', 'AMDEYR', 'AMDEY2_U', 'ATXMDEYR', 'ARXMDEYR', 'AMDETXRX', 'ADOCMDE', 'AOMDMDE', 'APSY1MDE', 'APSY2MDE', 'ASOCMDE', 'ACOUNMDE', 'AOMHMDE', 'ANURSMDE', 'ARELMDE', 'AHBCHMDE', 'AOTHMDE', 'AHLTMDE', 'AALTMDE', 'ASDSHOM2', 'ASDSWRK2', 'ASDSREL2', 'ASDSSOC2', 'ASDSOVL2', 'AMDEIMP', 'YUHOSPYR', 'YUHOSPN2', 'YUHOSUIC', 'YUHODEPR', 'YUHOFEAR', 'YUHOBKRU', 'YUHOEATP', 'YUHOANGR', 'YUHOFITE', 'YUHOFMLY', 'YUHOFRND', 'YUHOOTPP', 'YUHOSCHL', 'YUHOSOR', 'YURSIDYR', 'YURSIDN2', 'YURSSUIC', 'YURSDEPR', 'YURSFEAR', 'YURSBKRU', 'YURSEATP', 'YURSANGR', 'YURSFITE', 'YURSFMLY', 'YURSFRND', 'YURSOTPP', 'YURSSCHL', 'YURSSOR', 'YUFCARYR', 'YUFCARN2', 'YUFCSUIC', 'YUFCDEPR', 'YUFCFEAR', 'YUFCBKRU', 'YUFCEATP', 'YUFCANGR', 'YUFCFITE', 'YUFCFMLY', 'YUFCFRND', 'YUFCOTPP', 'YUFCSCHL', 'YUFCSOR', 'YUDYTXYR', 'YUDYTXN2', 'YUDYSUIC', 'YUDYDEPR', 'YUDYFEAR', 'YUDYBKRU', 'YUDYEATP', 'YUDYANGR', 'YUDYFITE', 'YUDYFMLY', 'YUDYFRND', 'YUDYOTPP', 'YUDYSCHL', 'YUDYSOR', 'YUMHCRYR', 'YUMHCRN2', 'YUMHSUIC', 'YUMHDEPR', 'YUMHFEAR', 'YUMHBKRU', 'YUMHEATP', 'YUMHANGR', 'YUMHFITE', 'YUMHFMLY', 'YUMHFRND', 'YUMHOTPP', 'YUMHSCHL', 'YUMHSOR', 'YUTPSTYR', 'YUTPSTN2', 'YUTPSUIC', 'YUTPDEPR', 'YUTPFEAR', 'YUTPBKRU', 'YUTPEATP', 'YUTPANGR', 'YUTPFITE', 'YUTPFMLY', 'YUTPFRND', 'YUTPOTPP', 'YUTPSCHL', 'YUTPSOR', 'YUIHTPYR', 'YUIHTPN2', 'YUIHSUIC', 'YUIHDEPR', 'YUIHFEAR', 'YUIHBKRU', 'YUIHEATP', 'YUIHANGR', 'YUIHFITE', 'YUIHFMLY', 'YUIHFRND', 'YUIHOTPP', 'YUIHSCHL', 'YUIHSOR', 'YUFDOCYR', 'YUFDOCN2', 'YUFDSUIC', 'YUFDDEPR', 'YUFDFEAR', 'YUFDBKRU', 'YUFDEATP', 'YUFDANGR', 'YUFDFITE', 'YUFDFMLY', 'YUFDFRND', 'YUFDOTPP', 'YUFDSCHL', 'YUFDSOR', 'YUSWSCYR', 'YUSWSUIC', 'YUSWDEPR', 'YUSWFEAR', 'YUSWBKRU', 'YUSWEATP', 'YUSWANGR', 'YUSWFITE', 'YUSWFMLY', 'YUSWFRND', 'YUSWOTPP', 'YUSWSCHL', 'YUSWSOR', 'YUSCEMYR', 'YUSCPGYR', 'YUJVDTON', 'YUJVDTN2', 'YUJVDTYR', 'YHOSP', 'YRESID', 'YFOST', 'YDAYTRT', 'YCLIN', 'YTHER', 'YHOME', 'YPED', 'YSPEC', 'YSHSW', 'YJAIL', 'ANYMHIN2', 'ANYMHOUT', 'ANYSMH2', 'ANYNSMH', 'ANYMHED2', 'ANYSEDMF', 'HOSPVST', 'RESIDVST', 'FOSTVST', 'DYTXVST', 'CLINVST', 'THERVST', 'HOMEVST', 'SPINVST2', 'SPOUTVST', 'SMHVST2', 'SIMHSUI2', 'SIMHDPR2', 'SIMHFEA2', 'SIMHBRK2', 'SIMHEAT2', 'SIMHANG2', 'SIMHFIT2', 'SIMHFML2', 'SIMHFRD2', 'SIMHOTP2', 'SIMHSCH2', 'SIMHMEN2', 'SIMHOTH3', 'SOMHSUI', 'SOMHDPR', 'SOMHFEA', 'SOMHBRK', 'SOMHEAT', 'SOMHANGR', 'SOMHFITE', 'SOMHFMLY', 'SOMHFRND', 'SOMHOTPP', 'SOMHSCHL', 'SOMHMEND', 'SOMHOTH2', 'SMHSUI2', 'SMHDPR2', 'SMHFEA2', 'SMHBRK2', 'SMHEAT2', 'SMHANGR2', 'SMHFITE2', 'SMHFMLY2', 'SMHFRND2', 'SMHOTPP2', 'SMHSCHL2', 'SMHMEND2', 'SMHOTH3', 'SHSWSUI', 'SHSWDPR', 'SHSWFEA', 'SHSWBRK', 'SHSWEAT', 'SHSWANGR', 'SHSWFITE', 'SHSWFMLY', 'SHSWFRND', 'SHSWOTPP', 'SHSWSCHL', 'SHSWMEND', 'SHSWOTH2', 'FDOCSUI', 'FDOCDPR', 'FDOCFEA', 'FDOCBRK', 'FDOCEAT', 'FDOCANGR', 'FDOCFITE', 'FDOCFMLY', 'FDOCFRND', 'FDOCOTPP', 'FDOCSCHL', 'FDOCMEND', 'FDOCOTH2', 'YMHOSPTX', 'YMHNSPTX', 'YSPTXNMH', 'YMHASPTX', 'YODPREV', 'YODSCEV', 'YOLOSEV', 'YODPDISC', 'YODPLSIN', 'YODSLSIN', 'YOLSI2WK', 'YODPR2WK', 'YOWRHRS', 'YOWRDST', 'YOWRCHR', 'YOWRIMP', 'YODPPROB', 'YOWRPROB', 'YOWRAGE', 'YOWRDEPR', 'YOWRDISC', 'YOWRLSIN', 'YOWRPLSR', 'YOWRELES', 'YOWREMOR', 'YOWRGAIN', 'YOWRGROW', 'YOWRPREG', 'YOWRGNL2', 'YOWRLOSE', 'YOWRDIET', 'YOWRLSL2', 'YOWRSLEP', 'YOWRSMOR', 'YOWRENRG', 'YOWRSLOW', 'YOWRSLNO', 'YOWRJITT', 'YOWRJINO', 'YOWRTHOT', 'YOWRCONC', 'YOWRDCSN', 'YOWRNOGD', 'YOWRWRTH', 'YOWRDLOT', 'YOWRDBTR', 'YOWRSTHK', 'YOWRSPLN', 'YOWRSATP', 'YO_MDEA1', 'YO_MDEA2', 'YO_MDEA3', 'YO_MDEA4', 'YO_MDEA5', 'YO_MDEA6', 'YO_MDEA7', 'YO_MDEA8', 'YO_MDEA9', 'YODSMMDE', 'YOPBINTF', 'YOPBDLYA', 'YOPBRMBR', 'YOPBAGE', 'YOPBNUM', 'YOPB2WK']\n",
    "col_names_4 = ['YOPSHMGT', 'YOPSWORK', 'YOPSRELS', 'YOPSSOC', 'YOPSDAYS', 'YOSEEDOC', 'YOFAMDOC', 'YOOTHDOC', 'YOPSYCH', 'YOPSYMD', 'YOSOCWRK', 'YOCOUNS', 'YOOTHMHP', 'YONURSE', 'YORELIG', 'YOHERBAL', 'YOOTHHLP', 'YOTMTNOW', 'YORX12MO', 'YORXNOW', 'YORXHLP', 'YOTMTHLP', 'YMDELT', 'YMDEYR', 'YMDEAUDPY', 'YMDEIUDPY', 'YMDEUDPY', 'YTXMDEYR', 'YRXMDEYR', 'YMDETXRX', 'YDOCMDE', 'YOMDMDE', 'YPSY1MDE', 'YPSY2MDE', 'YSOCMDE', 'YCOUNMDE', 'YOMHMDE', 'YNURSMDE', 'YRELMDE', 'YHBCHMDE', 'YOTHMDE', 'YHLTMDE', 'YALTMDE', 'YMDEHPRX', 'YMDEHPO', 'YMDERXO2', 'YMDEHARX', 'YSDSHOME', 'YSDSWRK', 'YSDSREL', 'YSDSSOC', 'YSDSOVRL', 'MDEIMPY', 'YMDEIMAUD', 'YMDEIMIUD', 'YMDEIMUDPY', 'CADRLAST', 'CADRPEOP', 'CADRCAR', 'CADRHOME', 'CADROTHM', 'CADRPUBL', 'CADRBAR', 'CADREVNT', 'CADRSCHL', 'CADROTH', 'CADROTS2', 'CABUYFRE', 'CAGVMONY', 'CABUYWHO', 'CABPLACE', 'CABUNDAG', 'CAGVWHO', 'CAFREWHO', 'CAFRESP2', 'CADRKDRUG', 'CADRKMARJ', 'CADRKCOCN', 'CADRKHERN', 'CADRKHALL', 'CADRKINHL', 'CADRKMETH', 'CABINGFLG', 'CABINGEVR', 'CABINGAGE', 'CABINGYFU', 'CABINGMFU', 'EIBINGAGE', 'EIBINGYFU', 'EIBINGMFU', 'CASUPROB', 'CASURCVR', 'CAMHPROB', 'CAMHRCVR', 'KRATEVER', 'KRATREC', 'UADPEOP', 'UADCAR', 'UADHOME', 'UADOTHM', 'UADPUBL', 'UADBAR', 'UADEVNT', 'UADSCHL', 'UADROTH', 'UADOTSP', 'UADPAID', 'UADMONY', 'UADBWHO', 'UADPLACE', 'UADBUND', 'UADCAG', 'UADFWHO', 'UADFRD', 'CADRKMARJ2', 'CADRKCOCN2', 'CADRKHERN2', 'CADRKHALL2', 'CADRKINHL2', 'CADRKMETH2', 'CASUPROB2', 'RCVYSUBPRB', 'CAMHPROB2', 'RCVYMHPRB', 'ALMEDYR2', 'OPMEDYR2', 'ALOPMEDYR', 'KRATFLG', 'KRATYR', 'KRATMON', 'MMGETMJ', 'MMBTREC1', 'MMBTPYR', 'MMBTREC2', 'MMBT30DY', 'MMJNTLOO', 'MMJNTNUM', 'MMJNPCTB', 'MMJNPCAT', 'MMLSUNIT', 'MMLSGMS', 'MMLS10GM', 'MMLSOZS', 'MMLSLBS', 'MMLSPCTB', 'MMLSPCAT', 'MMBUYWHO', 'MMBPLACE', 'MMBPLOS2', 'MMBATJOB', 'MMBCLOSE', 'MMBSELL', 'MMBGIVE', 'MMTRADE', 'MMTRD30D', 'MMTRDREC', 'MMT30FRQ', 'MMTJNTLS', 'MMTJNTNM', 'MMTJWRCB', 'MMTJWRTH', 'MMTLUNIT', 'MMTLGMS', 'MMTLOZS', 'MMTLWRCB', 'MMTLWRTH', 'MMTRDWHO', 'MMTPLACE', 'MMTPLOS2', 'MMTCLOSE', 'MMTKEEP', 'MMTSELL', 'MMTGIVE', 'MMGKEEP', 'MMGSELL', 'MMGGIVE', 'MMFREWHO', 'MMFPLACE', 'MMFPLOS2', 'MMFCLOSE', 'MMFKEEP', 'MMFSELL', 'MMFGIVE', 'LANGVER', 'QUARTER', 'GQTYPE2', 'AGE2', 'NOMARR2', 'SERVICE', 'MILSTAT', 'ACTDEVER', 'ACTD2001', 'ACTD9001', 'ACTD7590', 'ACTDVIET', 'ACTDPRIV', 'COMBATPY', 'HEALTH', 'MOVSINPYR2', 'SEXATRACT', 'SEXIDENT', 'SPEAKENGL', 'DIFFHEAR', 'DIFFSEE', 'DIFFTHINK', 'DIFFWALK', 'DIFFDRESS', 'DIFFERAND', 'IRSEX', 'IRMARIT', 'IIMARIT', 'IREDUHIGHST2', 'IIEDUHIGHST2', 'CATAGE', 'CATAG2', 'CATAG3', 'CATAG6', 'CATAG7', 'PREGAGE2', 'DRVINAGE', 'DRVINDETAG', 'SEXAGE', 'NEWRACE2', 'SEXRACE', 'EDUHIGHCAT', 'HEALTH2', 'EDUSCHLGO', 'EDUSCHGRD2', 'EDUFULPAR', 'EDUSCKMON', 'EDUSCKEST', 'EDUSCKCOM', 'EDUSKPMON', 'EDUSKPEST', 'EDUSKPCOM', 'MILTFAMLY', 'MILTSPPAR', 'MILTPARNT', 'MILTCHLDR', 'MILTSIBLN', 'COLLENRLFT', 'COLLENRLST', 'WRKSTATWK2', 'WRKDPSTWK', 'WRKHADJOB', 'WRKDHRSWK2', 'WRK35WKUS', 'WRKRSNNOT', 'WRKRSNJOB', 'WRKEFFORT', 'WRKDPSTYR', 'WRKSELFEM', 'WRKNUMJOB2', 'WRKNJBPYR', 'WRKNJBWKS', 'WRKLASTYR2', 'WRKSICKMO', 'WRKSKIPMO', 'WRKDRGPOL', 'WRKDRGALB', 'WRKDRGEDU', 'WRKDRGHLP', 'WRKTSTALC', 'WRKTSTDRG', 'WRKTSTHIR', 'WRKTSTRDM', 'WRKTST1ST', 'WRKOKPREH', 'WRKOKRAND', 'IRWRKSTAT', 'IIWRKSTAT', 'II2WRKSTAT', 'IRWRKSTAT18', 'IIWRKSTAT18', 'II2WRKST18', 'EDFAM18', 'IMOTHER', 'IFATHER', 'NRCH17_2', 'IRHHSIZ2', 'IIHHSIZ2', 'IRKI17_2', 'IIKI17_2', 'IRHH65_2', 'IIHH65_2', 'PRXRETRY', 'PRXYDATA', 'MEDICARE', 'CAIDCHIP', 'CHAMPUS', 'PRVHLTIN', 'GRPHLTIN', 'HLTINALC', 'HLTINDRG', 'HLTINMNT', 'HLTINNOS', 'HLCNOTYR', 'HLCNOTMO', 'HLCLAST', 'HLLOSRSN', 'HLNVCOST', 'HLNVOFFR', 'HLNVREF', 'HLNVNEED', 'HLNVSOR', 'IRMEDICR', 'IIMEDICR', 'IRMCDCHP', 'IIMCDCHP', 'IRCHMPUS', 'IICHMPUS', 'IRPRVHLT', 'IIPRVHLT', 'IROTHHLT', 'IIOTHHLT', 'HLCALLFG', 'HLCALL99', 'ANYHLTI2', 'IRINSUR4', 'IIINSUR4', 'OTHINS', 'CELLWRKNG', 'CELLNOTCL', 'IRFAMSOC', 'IIFAMSOC', 'IRFAMSSI', 'IIFAMSSI', 'IRFSTAMP', 'IIFSTAMP', 'IRFAMPMT', 'IIFAMPMT', 'IRFAMSVC', 'IIFAMSVC', 'IRWELMOS', 'IIWELMOS', 'IRPINC3', 'IIPINC3', 'IRFAMIN3', 'IIFAMIN3', 'GOVTPROG', 'INCOME', 'POVERTY3', 'TOOLONG', 'TROUBUND', 'PDEN10', 'COUTYP4', 'MAIIN102', 'AIIND102', 'ANALWT_C', 'VESTR', 'VEREP']\n",
    "\n",
    "starting_length = len(col_names_1) + len(col_names_2) + len(col_names_3) + len(col_names_4)\n",
    "\n",
    "c1 = [c for c in col_names_1 if c[:2].lower()!='ii']\n",
    "c2 = [c for c in col_names_2 if c[:2].lower()!='ii']\n",
    "c3 = [c for c in col_names_3 if c[:2].lower()!='ii']\n",
    "c4 = [c for c in col_names_4 if c[:2].lower()!='ii']\n",
    "\n",
    "# _1 because I'm optimistic about finding more patterns like this\n",
    "reduced_length_1 = len(c1) + len(c2) + len(c3) + len(c4)\n",
    "\n",
    "# Now I'm going to print them out and go through them one section at a time. \n",
    "# no point in copying alllllll of that back in at once.\n",
    "\n",
    "print(c1)\n",
    "\n",
    "# me just figuring stuff out\n",
    "# it's not important\n",
    "# but I'm preserving it because I'm proud of it\n",
    "# I'm a genius, I'm a genius, I'm the smartest person in the world\n",
    "# >>> col_names_1[0]\n",
    "# 'IRCGRFM'\n",
    "# >>> col_names_1[0][:1]=='II'\n",
    "# False\n",
    "# >>> col_names_1[0][:1]=='IR'\n",
    "# False\n",
    "# >>> col_names_1[0][:1]\n",
    "# 'I'\n",
    "# >>> col_names_1[0][:2]\n",
    "# 'IR'\n",
    "# >>> col_names_1[0][:2]=='II'\n",
    "# False\n",
    "# >>> col_names_1[0][:2]=='IR'\n",
    "# True\n",
    "# >>> col_names_1[0][:2].lower()\n",
    "# 'ir'\n",
    "# >>> col_names_1[0][:2].lower()=='ir'\n",
    "# True\n",
    "# >>> col_names_1[0][:2].lower()=='ii'\n",
    "# False\n",
    "# >>> col_names_4[-4]\n",
    "# 'AIIND102'\n",
    "# >>> col_names_4[-4][:2].lower()=='ii'\n",
    "# False\n",
    "# >>> c1 = [c for c in col_names_1]\n",
    "# >>> c1 = c1[:5]\n",
    "# >>> c1\n",
    "# ['IRCGRFM', 'IICGRFM', 'II2CGRFM', 'IRSMKLSS30N', 'IISMKLSS30N']\n",
    "# >>> c1 = [c for c in c1 if c[:2].lower()!='ii']\n",
    "# >>> c1\n",
    "# ['IRCGRFM', 'IRSMKLSS30N']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thursday, May 23, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "today I went searching for alternative datasets because the one I'd been working on was taking so long and I thought I probably wouldn't end up being happy with it.  I'm sure I could have done something with it, but I was so excited to be back in queerland and I didn't want to give up on that.  so I went looking and was SHOCKED at how little was available (et tu, taskforce!?), but then I found a dataset from Ilan H. Meyer!!!!  It's a lot smaller than the samhsa one, but it's more specific to queer stuff, and it's Dr. Meyer!!!!  and there are citations from Fingerhut and Frost and Herek and Lisa Diamond and the whole gang!!  I've been so happy playing with it all day; I'm almost getting misty eyed!\n",
    "\n",
    "anyway, work notes.\n",
    "\n",
    "I created my first ever \"project\" in R, which seems to just be a folder that's tacked to one of the side panels.  You can click to open files from there; it's cool!\n",
    "\n",
    "it's a study of queer people across several age groups and across several years. so there's time series stuff in there too, but tbh that scares me a bit.  it'd be cool to do later and i definitely don't want to write it off as \"I can't do that,\" but knowing myself and my proclivity towards taking forever on things and that I do NOT have a resub accomodation on this project AND that it is the super important last project, I want to KISS.  so what I decided to do is look only at the first wave (before the attrition effects kicked in, which were sizable).  it's only n=1518, but it's Ilan Meyer!  I bet they're a good 1518.  Hank seemed somewhat worried about the sample size, but not to the point of it being a dealbreaker.  he said to plow on and we still have time to pivot if my stuff doesn't work.\n",
    "\n",
    "so, I dropped all the features that were exclusive to wave 2 and/or 3, leaving me with 505 features.  then I went thru the codebook and found all the scales they used.  I tracked down the columns where they had calculated the scores, and dropped all the component parts.  I then investigated their imputation process a little bit.  I have some concerns about it (seems weird that more than half of the people who didn't answer the question on IPV would be imputed as NO), but it is very sound in theory and, more importantly, I don't have enough data to drop NAs nor do I have any better ideas on how to impute them.  They *did* the imputation I would have done - regress the missing value on related, populated values, then randomly select a value from the nearest 5 extant values based on something that sounds a lot like KNN.  it seems very robust.  after looking into that, I dropped the non-imputed scale score features, and only kept the imputed ones.\n",
    "\n",
    "after that, I had a little under 400 columns left (quite an achievement, down from 1300+, in just a few hours of work!).  having finally figured out how the scales work, and that they did in fact calculate a lot of these things, I went back to the demographics section of the codebook and looked to see if they did similar stuff there.  it seems like they may have, but I'm not sure.  so far it seems more like they just duplicated their w1q00 variables into columns wiht more semantic names, but I'm just getting started.  that's where to pick up tomorrow.\n",
    "\n",
    "**I think the way forward here is to *NOT* edit scripts from previous days; rather, run them again and pick up where they left off in a new one.**\n",
    "\n",
    "files created: \n",
    "- 2024-05-23_exploring_Meyer_2023_dataset.py\n",
    "- 2024-05-23_set_up.py **NEW SET UP SCRIPT FOR NEW DF!!!!**\n",
    "- 2024-05-23_cols_to_check.txt\n",
    "- 2 folders called 2024-05-23_dowload... in the potential_datasets folder, plus all files within, plus corresponding zip folders.\n",
    "- - the one ending in attempt_2 is the one I ended up using.  they're the same, but the data is formatted nicer.\n",
    "\n",
    "GOALS FOR ~TOMORROW~  NEXT TIME\\*:\n",
    "- finish column cleaning\n",
    "- save a cleaned up (reduced) CSV\n",
    "- start setting up github\n",
    "- make an initial commit\n",
    "\n",
    "\\*may need to use Friday night to work on NN stuff so that you can talk to Hank in flex time if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script 1 in full:\n",
    "# 2024-05-23_set_up.py\n",
    "\n",
    "# Capstone Setup\n",
    "\n",
    "# Module Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, date, time\n",
    "from string import capwords\n",
    "\n",
    "# Working Directory\n",
    "os.getcwd()\n",
    "#os.chdir('C:/Users/emily/Git_Stuff/General_Assembly/04_Projects/project-capstone')\n",
    "os.listdir()\n",
    "\n",
    "# Import Data\n",
    "meyer = pd.read_csv('./potential_datasets/2024_05_23_download_ICPSR_Meyer_2023_generations_data_attempt_2/ICPSR_37166/DS0007/37166-0007-Data.tsv', sep = '\\t', low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script 2 in full\n",
    "# 2024-05-23_exploring_Meyer_2023_dataset.py\n",
    "\n",
    "# 5/23/24\n",
    "\n",
    "# check out this dataset from Ilan Meyer, my beloved\n",
    "\n",
    "# Import Data\n",
    "meyer = pd.read_csv('./potential_datasets/2024_05_23_download_ICPSR_Meyer_2023_generations_data_attempt_2/ICPSR_37166/DS0007/37166-0007-Data.tsv', sep = '\\t', low_memory=False)\n",
    "meyer.shape #(1518, 1329), but we can cut that down.\n",
    "og_columns = list(meyer.columns)\n",
    "\n",
    "# for RIGHT NOW, I think I'm only going to look at wave 1\n",
    "# the wave stuff is REALLY COOL, but i don't think I understand\n",
    "# time series well enough to get into it for this project\n",
    "# if I get a model working based on wave 1, and write it up\n",
    "# and all that \"good enough,\" then I'll come back and add \n",
    "# some time series stuff\n",
    "# oh it can even just be another phase of the project!\n",
    "# no need to replace anything!  just section B.\n",
    "\n",
    "meyer['WAVEPARTICIPATED'].value_counts(dropna =False)\n",
    "# matches what's in the documentation\n",
    "\n",
    "# ok, so, for now, I am going to drop all the variables marked W2 and W3.  \n",
    "cols = list(meyer.columns)\n",
    "w1_cols = [c for c in cols if c[:2]!='W2']\n",
    "w1_cols = [c for c in w1_cols if c[:2]!='W3']\n",
    "len(w1_cols)\n",
    "\n",
    "# drop columns\n",
    "meyer = meyer[w1_cols]\n",
    "meyer.shape\n",
    "\n",
    "# NAs?\n",
    "meyer.isna().sum().sum()\n",
    "# it says 0, but it's not 0\n",
    "# they must have imputed it with an empty string or something\n",
    "# when I do .value_counts(), a lot of columns have a blank row, e.g.,\n",
    "#                      14\n",
    "# where there absolutely should not be a blank.\n",
    "# so that's fun!  gonna have to clean that up!\n",
    "\n",
    "# i originally thought they only included the base variables\n",
    "# and I would have to compute the scale scores myself, but\n",
    "# OMG THEY DID INCLUDE THE CALC'D SCALES!!!!!!\n",
    "# I JUST COULDN'T FIND THEM BECAUSE ALL CAPS!!\n",
    "\n",
    "meyer.columns = [c.lower() for c in list(meyer.columns)]\n",
    "\n",
    "# GOAL: drop the columns that contributed to combination scores\n",
    "\n",
    "# Make lists of the items that comprise scales\n",
    "soc_supp_items = ['w1q164a', 'w1q164b', 'w1q164c', \n",
    "  'w1q164d', 'w1q164e', 'w1q164f', 'w1q164g', 'w1q164h', \n",
    "  'w1q164i', 'w1q164j', 'w1q164k', 'w1q164l']\n",
    "\n",
    "ace_items = ['w1q151', 'w1q152', 'w1q153', \n",
    "  'w1q154', 'w1q155', 'w1q156', 'w1q157', \n",
    "  'w1q158', 'w1q159', 'w1q160', 'w1q161']\n",
    "\n",
    "childhd_gnc_items = [\n",
    "  'w1q147', 'w1q148', 'w1q149', 'w1q150']\n",
    "\n",
    "############# NOT CONDENSED (YET) #############\n",
    "# strain_items = ['w1q146a', 'w1q146b', \n",
    "#   'w1q146c', 'w1q146d', 'w1q146e', 'w1q146f', \n",
    "#   'w1q146g', 'w1q146h', 'w1q146i', 'w1q146j', \n",
    "#   'w1q146k', 'w1q146l']\n",
    "############# NOT CONDENSED (YET) #############\n",
    "\n",
    "daily_discr_items = ['w1q144a', 'w1q144b', \n",
    "  'w1q144c','w1q144d', 'w1q144e', 'w1q144f', \n",
    "  'w1q144g', 'w1q144h', 'w1q144i']\n",
    "\n",
    "# bi_stigma_items = [  # only in wave 2\n",
    "#   'w2q117', 'w2q118', 'w2q119', 'w2q120']\n",
    "\n",
    "int_homo_items = ['w1q128', 'w1q129', \n",
    "  'w1q130', 'w1q131', 'w1q132']\n",
    "\n",
    "felt_stigma_items = ['w1q125', 'w1q126', 'w1q127']\n",
    "\n",
    "drug_items = ['w1q90', 'w1q91', 'w1q92', \n",
    "  'w1q93', 'w1q94', 'w1q95', 'w1q96', \n",
    "  'w1q97', 'w1q98', 'w1q99', 'w1q100']\n",
    "\n",
    "alc_items = ['w1q85', 'w1q86', 'w1q87']\n",
    "\n",
    "ment_dis_items = ['w1q77a', 'w1q77b', \n",
    "  'w1q77c', 'w1q77d', 'w1q77e', 'w1q77f']\n",
    "\n",
    "hc_ster_threat_items = [\n",
    "  'w1q60', 'w1q61', 'w1q62', 'w1q63']\n",
    "\n",
    "comm_conn_items = ['w1q53', 'w1q54', \n",
    "  'w1q55', 'w1q56', 'w1q57', 'w1q58', 'w1q59']\n",
    "\n",
    "lgbis_items = [\n",
    "  'w1q40', 'w1q41', 'w1q42', 'w1q43', 'w1q44']\n",
    "\n",
    "meim_items = [\n",
    "  'w1q21', 'w1q22', 'w1q23', 'w1q24', 'w1q25', 'w1q26']\n",
    "\n",
    "swl_items = [\n",
    "  'w1q186', 'w1q187', 'w1q188', 'w1q189', 'w1q190']\n",
    "\n",
    "swb_items = ['w1q04', 'w1q05', 'w1q06', 'w1q07', \n",
    "  'w1q08', 'w1q09', 'w1q10', 'w1q11', 'w1q12', \n",
    "  'w1q13', 'w1q14', 'w1q15', 'w1q16', 'w1q17', 'w1q18']\n",
    "\n",
    "drop_cols = (soc_supp_items + ace_items + childhd_gnc_items + \n",
    "  daily_discr_items + int_homo_items + felt_stigma_items + \n",
    "  drug_items + alc_items + ment_dis_items + hc_ster_threat_items + \n",
    "  comm_conn_items + lgbis_items + meim_items + swl_items + swb_items)\n",
    "len(drop_cols)\n",
    "\n",
    "meyer.drop(columns = drop_cols, inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "# these are the scales\n",
    "combo_features = ['w1socialwb', 'w1socialwb_i', \n",
    "  'w1lifesat', 'w1lifesat_i', 'w1meim', 'w1meim_i', \n",
    "  'w1idcentral', 'w1idcentral_i', 'w1connectedness',\n",
    "  'w1connectedness_i', 'w1hcthreat', 'w1hcthreat_i', \n",
    "  'w1kessler6', 'w1kessler6_i', 'w1auditc', \n",
    "  'w1auditc_i', 'w1dudit', 'w1dudit_i', 'w1feltstigma', \n",
    "  'w1feltstigma_i', 'w1internalized', 'w1internalized_i', \n",
    "  'w1everyday', 'w1everyday_i', 'w1childgnc', 'w1childgnc_i',\n",
    "  'w1ace', 'w1ace_i', 'w1ace_emo', 'w1ace_emo_i', \n",
    "  'w1ace_inc', 'w1ace_inc_i', 'w1ace_ipv', 'w1ace_ipv_i', \n",
    "  'w1ace_men', 'w1ace_men_i', 'w1ace_phy', 'w1ace_phy_i', \n",
    "  'w1ace_sep', 'w1ace_sep_i', 'w1ace_sex', 'w1ace_sex_i', \n",
    "  'w1ace_sub', 'w1ace_sub_i', 'w1socsupport', \n",
    "  'w1socsupport_fam', 'w1socsupport_fam_i', \n",
    "  'w1socsupport_fr', 'w1socsupport_fr_i', \n",
    "  'w1socsupport_i', 'w1socsupport_so', 'w1socsupport_so_i']\n",
    "\n",
    "# split them up so I can check whether \n",
    "# the _i columns really are free of missing values\n",
    "combo_imputed = [c for c in combo_features if c[-2:]=='_i']\n",
    "combo_not_imputed = [c for c in combo_features if c[-2:]!='_i']\n",
    "\n",
    "# check my splits\n",
    "a = [c for c in combo_imputed if c in combo_not_imputed]\n",
    "b = [c for c in combo_not_imputed if c in combo_imputed]\n",
    "if len(a)==len(b)==0:\n",
    "  print('no overlap')\n",
    "  del (a, b)\n",
    "if (len(combo_imputed)+len(combo_not_imputed))==len(combo_features):\n",
    "  print('all accounted for')\n",
    "  \n",
    "# check for \"missing\" values\n",
    "for i in combo_imputed:\n",
    "  print('')\n",
    "  print(f'{i}, {meyer[i].isna().sum()} NAs')\n",
    "  print(meyer[i].value_counts(dropna = False).sort_values())\n",
    "\n",
    "# see how bad it was in the original\n",
    "for i in combo_not_imputed:\n",
    "  print('')\n",
    "  print(f'{i}, {meyer[i].isna().sum()} NAs')\n",
    "  print(meyer[i].value_counts(dropna = False).sort_values())\n",
    "missing_values = [('w1ace_sex', 75), ('w1ace_phy', 61), \n",
    "  ('w1ace_ipv', 139), ('w1ace_emo', 91), ('w1ace', 277), \n",
    "  ('w1everyday', 40), ('w1dudit', 66), ('w1connectedness', 51), \n",
    "  ('w1socialwb', 59), ('most others', '15-35')]\n",
    "\n",
    "# very grim and not at all surprising that column with the most missings is IPV\n",
    "# (from the ACE survey - so IPV among parents, not participants themselves)\n",
    "# >>> meyer['w1ace_ipv'].value_counts(dropna = False)\n",
    "# w1ace_ipv\n",
    "# 0    950\n",
    "# 1    429\n",
    "#      139\n",
    "# Name: count, dtype: int64\n",
    "# >>> meyer['w1ace_ipv_i'].value_counts(dropna = False)\n",
    "# w1ace_ipv_i\n",
    "# 0    1024\n",
    "# 1     494\n",
    "# Name: count, dtype: int64\n",
    "# >>> 1024-950\n",
    "# 74\n",
    "# >>> 139-74\n",
    "# 65\n",
    "\n",
    "# ok.  their imputation method seems really solid, but that is sus af.\n",
    "# of the people who declined to answer about IPV, you think the majority \n",
    "# of them would have said NO!?!??!??????\n",
    "# I'd be tempted to put every one of those guys down as a yes!!!!\n",
    "# let me see the overlap with other stigmatized stuff\n",
    "len(meyer[(meyer['w1ace_ipv']==' ') & (meyer['w1ace_sex']==' ')])\n",
    "# 30 huh damn I would have thought like all of them\n",
    "# wait.\n",
    "len(meyer[(meyer['w1ace_ipv']=='1') & (meyer['w1ace_sex']==' ')])  # 19\n",
    "len(meyer[(meyer['w1ace_ipv']==' ') & (meyer['w1ace_sex']=='1')])  # 51\n",
    "# ok! that actually lends credibility to their imputation though\n",
    "# if people were too ashamed to admit IPV, they'd probably decline to answer\n",
    "# about CSA too.  these people, at least, really could go either way on IPV.\n",
    "\n",
    "# This is all actually moot \n",
    "# because I don't have a better idea for imputation than what they did.\n",
    "# I think the thing to do, if I can, is to model it on the imputed data\n",
    "# then model it again on the subsample where I've dropped all these NAs\n",
    "# at least the ACE NAs, where there's really a lot.\n",
    "# could also try modeling it just without those columns\n",
    "# a random forest would probably be good, \n",
    "# make sure the whole thing isn't hinging on imputed data\n",
    "# Meyer also probably ran some tests in his paper to make sure this is ok\n",
    "\n",
    "# OK SO.\n",
    "\n",
    "# I've gotta reduce my dimensions\n",
    "# the imputation methods they used seem very reasonable\n",
    "# (see p. 37 of 37166-Documentation-methodology.pdf)\n",
    "# so I'm going to drop the non-imputed ones\n",
    "meyer.drop(columns = combo_not_imputed, inplace = True)\n",
    "meyer.shape  #(1518, 373)\n",
    "\n",
    "# ok!  love that!  let's see what else I can chuck.\n",
    "cols_to_check = [c for c in list(meyer.columns) if c not in combo_imputed]\n",
    "print(cols_to_check)\n",
    "\n",
    "# I am looking thru the documentation and making 2 lists\n",
    "# the variables should ROUGHLY correspond to each other\n",
    "# maybe I'll do some line breaks or whatever.\n",
    "drop_cols_2 = [\n",
    "  'w1q165', 'w1q27', 'w1q28', \n",
    "  'w1q20_1', 'w1q20_2', 'w1q20_3', 'w1q20_4', 'w1q20_5', 'w1q20_6', 'w1q20_7', \n",
    "]\n",
    "\n",
    "keep_cols = [\n",
    "  'w1age', 'cohort', 'w1sex', 'w1gender', 'w1sex_gender', \n",
    "  'screen_race', 'w1race',\n",
    "]\n",
    "\n",
    "check_cols = [\n",
    "  # no age, sex stuff\n",
    "  'w1q20_t_verb', # I suspect this is part of q20 and got rolled into w1race, but I'm not sure off the top of my head\n",
    "]\n",
    "\n",
    "# KEEP GOING ON THESE ON FRIDAY\n",
    "# I JUST FINISHED SEX AND GENDER\n",
    "# PICK UP WITH SEXUAL IDENTITY, PAGE 14 (16) OF THE DOCUMENTATION\n",
    "# 37166-Documentation-methodology.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# not-script 3 in full \n",
    "\\# 2024-05-23_cols_to_check.txt\n",
    "\n",
    "cols_to_check\n",
    "\n",
    "These are the columns that remained after I dropped all the scale items and non-imputed scale features.  What I'm working on now is going through them to try and reduce dimensionality.\n",
    "\n",
    "['studyid', 'waveparticipated', 'w1weight_full', 'w1weight_orig', 'w1cumulative_wt_nr1', 'w1cumulative_wt_nr2', 'w1cumulative_wt_nr3', 'w1cumulative_wt_sampling', 'w1weighting_cell_nr1', 'w1weighting_cell_nr2and3', 'w1frame_wt', 'w1survey_yr', 'geduc1', 'geduc2', 'geducation', 'gemployment2010', 'gmethod_type', 'gmsaname', 'gp1', 'gruca', 'gruca_i', 'gurban', 'gurban_i', 'gzipcode', 'gzipstate', 'gcendiv', 'gcenreg', 'gmilesaway', 'gmilesaway2', 'w1q01', 'w1q02', 'w1q03', 'w1q19a', 'w1q19b', 'w1q19c', 'w1q19d', 'w1q29', 'w1q29_t_verb', 'w1q30_1', 'w1q30_2', 'w1q30_3', 'w1q30_4', 'w1q30_5', 'w1q31a', 'w1q31b', 'w1q31c', 'w1q31d', 'w1q32', 'w1q33', 'w1q34', 'w1q35', 'w1q36', 'w1q37', 'w1q38', 'w1q39_1', 'w1q39_2', 'w1q39_3', 'w1q39_4', 'w1q39_5', 'w1q39_6', 'w1q39_7', 'w1q39_8', 'w1q39_9', 'w1q39_10', 'w1q39_11', 'w1q39_12', 'w1q39_t_verb', 'w1q45', 'w1q46', 'w1q47', 'w1q48', 'w1q49', 'w1q50', 'w1q51', 'w1q52', 'w1q64_1', 'w1q64_2', 'w1q64_3', 'w1q64_4', 'w1q64_5', 'w1q64_6', 'w1q64_7', 'w1q64_8', 'w1q64_9', 'w1q64_10', 'w1q64_11', 'w1q64_12', 'w1q64_13', 'w1q64_t_verb', 'w1q65', 'w1q66_1', 'w1q66_2', 'w1q66_3', 'w1q66_4', 'w1q66_5', 'w1q66_t_verb', 'w1q67', 'w1q68_1', 'w1q68_2', 'w1q68_3', 'w1q69', 'w1q70', 'w1q71', 'w1q72', 'w1q73', 'w1q74_1', 'w1q74_2', 'w1q74_3', 'w1q74_4', 'w1q74_5', 'w1q74_6', 'w1q74_7', 'w1q74_8', 'w1q74_9', 'w1q74_10', 'w1q74_11', 'w1q74_12', 'w1q74_13', 'w1q74_14', 'w1q74_15', 'w1q74_16', 'w1q74_17', 'w1q74_18', 'w1q74_19', 'w1q74_20', 'w1q74_21', 'w1q74_22', 'w1q74_23', 'w1q75', 'w1q76', 'w1q78', 'w1q79', 'w1q80', 'w1q81', 'w1q82', 'w1q83', 'w1q84', 'w1q88', 'w1q89', 'w1q101', 'w1q102', 'w1q103', 'w1q104', 'w1q105', 'w1q106', 'w1q107', 'w1q108', 'w1q109', 'w1q110', 'w1q111', 'w1q112', 'w1q113', 'w1q114', 'w1q115', 'w1q116', 'w1q117', 'w1q118', 'w1q119', 'w1q120', 'w1q121', 'w1q122', 'w1q123a', 'w1q123b', 'w1q123c', 'w1q123d', 'w1q124', 'w1q133', 'w1q133_1', 'w1q133_2', 'w1q133_3', 'w1q134', 'w1q135a', 'w1q135b', 'w1q135c', 'w1q135d', 'w1q135e', 'w1q135f', 'w1q136_1', 'w1q136_2', 'w1q136_3', 'w1q136_4', 'w1q136_5', 'w1q136_6', 'w1q136_7', 'w1q136_8', 'w1q136_9', 'w1q136_10', 'w1q137', 'w1q138', 'w1q139_1', 'w1q139_2', 'w1q139_3', 'w1q139_4', 'w1q139_5', 'w1q139_6', 'w1q139_7', 'w1q139_8', 'w1q139_9', 'w1q139_10', 'w1q140', 'w1q141_1', 'w1q141_2', 'w1q141_3', 'w1q141_4', 'w1q141_5', 'w1q141_6', 'w1q141_7', 'w1q141_8', 'w1q141_9', 'w1q141_10', 'w1q142a', 'w1q142b', 'w1q142c', 'w1q142d', 'w1q142e', 'w1q142f', 'w1q142g', 'w1q142h', 'w1q142i', 'w1q142j', 'w1q142k', 'w1q143_1', 'w1q143_2', 'w1q143_3', 'w1q143_4', 'w1q143_5', 'w1q143_6', 'w1q143_7', 'w1q143_8', 'w1q143_9', 'w1q143_10', 'w1q145_1', 'w1q145_2', 'w1q145_3', 'w1q145_4', 'w1q145_5', 'w1q145_6', 'w1q145_7', 'w1q145_8', 'w1q145_9', 'w1q145_10', 'w1q146a', 'w1q146b', 'w1q146c', 'w1q146d', 'w1q146e', 'w1q146f', 'w1q146g', 'w1q146h', 'w1q146i', 'w1q146j', 'w1q146k', 'w1q146l', 'w1q162', 'w1q163_1', 'w1q163_2', 'w1q163_3', 'w1q163_4', 'w1q163_5', 'w1q163_6', 'w1q163_7', 'w1q163_8', 'w1q163_9', 'w1q163_10', 'w1q166', 'w1q167', 'w1q168', 'w1q169', 'w1q170_1', 'w1q170_2', 'w1q170_3', 'w1q170_4', 'w1q171_1', 'w1q171_2', 'w1q171_3', 'w1q171_4', 'w1q171_5', 'w1q171_6', 'w1q171_7', 'w1q171_8', 'w1q171_9', 'w1q172', 'w1q173', 'w1q174', 'w1q175', 'w1q176', 'w1q177_1', 'w1q177_2', 'w1q177_3', 'w1q177_4', 'w1q177_5', 'w1q177_6', 'w1q177_7', 'w1q177_8', 'w1q177_9', 'w1q177_10', 'w1q177_11', 'w1q177_12', 'w1q178', 'w1q179', 'w1q180', 'w1q181', 'w1q182', 'w1q183', 'w1q184', 'w1q185',  'w1sample',  'w1sexualid', 'w1sexminid', 'w1pinc', 'w1pinc_i', 'w1hinc', 'w1hinc_i', 'w1poverty', 'w1poverty_i', 'w1povertycat', 'w1povertycat_i', 'w1conversion', 'w1conversionhc', 'w1conversionrel', 'gmethod_type_w2', 'grespondent_date_w2', 'gsurvey', 'gmethod_type_w3', 'gp2', 'grace', 'grespondent_date_w3', 'wave3', 'nopolicecontact']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wednesday, May 29, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "today I continued cleaning the Meyer dataset.  I finished all the scales that were defined in the documentation, and dropped columns accordingly.  at that point i was down to 337 columns.  I then started going through the 831 page documenation to piece together the rest of what's what.  i started writing a dictionary of how I want to combine these columns, after which I plan to drop most of them.  there are a few I want to keep to see if they perform better alone than they do as a composite, but I want to be careful of introducing too much colinearity.  because I'd really like to be able to do interpretation, I might have to sacrifice some features I think are interesting in order to be able to make sense of the others.\n",
    "\n",
    "I also found a TON of really interesting features that I had to drop just to make sure I have a straightforward enough project to pull off in 2 weeks (left).  I'd love to go back and tinker with them for my portfolio though!\n",
    "\n",
    "files created: \n",
    "- 2024-05-23_AND_2024-05-29_exploring_Meyer_2023_dataset.py\n",
    "- 2024-05-29_cols_to_check.txt\n",
    "- 2024-05-29_notes_about_non-scale_columns.txt\n",
    "both of the first two files are actually just longer versions of the older ones, so most of their contents are redundant.  I used the same setup file as on 5-23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script 1 in full: 2024-05-23_AND_2024-05-29_exploring_Meyer_2023_dataset.py\n",
    "\n",
    "# 5/23/24\n",
    "\n",
    "# check out this dataset from Ilan Meyer, my beloved\n",
    "\n",
    "# Import Data\n",
    "meyer = pd.read_csv('./potential_datasets/2024_05_23_download_ICPSR_Meyer_2023_generations_data_attempt_2/ICPSR_37166/DS0007/37166-0007-Data.tsv', sep = '\\t', low_memory=False)\n",
    "meyer.shape #(1518, 1329), but we can cut that down.\n",
    "og_columns = list(meyer.columns)\n",
    "\n",
    "# for RIGHT NOW, I think I'm only going to look at wave 1\n",
    "# the wave stuff is REALLY COOL, but i don't think I understand\n",
    "# time series well enough to get into it for this project\n",
    "# if I get a model working based on wave 1, and write it up\n",
    "# and all that \"good enough,\" then I'll come back and add \n",
    "# some time series stuff\n",
    "# oh it can even just be another phase of the project!\n",
    "# no need to replace anything!  just section B.\n",
    "\n",
    "meyer['WAVEPARTICIPATED'].value_counts(dropna =False)\n",
    "# matches what's in the documentation\n",
    "\n",
    "# ok, so, for now, I am going to drop all the variables marked W2 and W3.  \n",
    "cols = list(meyer.columns)\n",
    "w1_cols = [c for c in cols if c[:2]!='W2']\n",
    "w1_cols = [c for c in w1_cols if c[:2]!='W3']\n",
    "len(w1_cols)\n",
    "\n",
    "# drop columns\n",
    "meyer = meyer[w1_cols]\n",
    "meyer.shape\n",
    "\n",
    "# NAs?\n",
    "meyer.isna().sum().sum()\n",
    "# it says 0, but it's not 0\n",
    "# they must have imputed it with an empty string or something\n",
    "# when I do .value_counts(), a lot of columns have a blank row, e.g.,\n",
    "#                      14\n",
    "# where there absolutely should not be a blank.\n",
    "# so that's fun!  gonna have to clean that up!\n",
    "\n",
    "# i originally thought they only included the base variables\n",
    "# and I would have to compute the scale scores myself, but\n",
    "# OMG THEY DID INCLUDE THE CALC'D SCALES!!!!!!\n",
    "# I JUST COULDN'T FIND THEM BECAUSE ALL CAPS!!\n",
    "\n",
    "meyer.columns = [c.lower() for c in list(meyer.columns)]\n",
    "\n",
    "# GOAL: drop the columns that contributed to combination scores\n",
    "\n",
    "# Make lists of the items that comprise scales\n",
    "soc_supp_items = ['w1q164a', 'w1q164b', 'w1q164c', \n",
    "  'w1q164d', 'w1q164e', 'w1q164f', 'w1q164g', 'w1q164h', \n",
    "  'w1q164i', 'w1q164j', 'w1q164k', 'w1q164l']\n",
    "\n",
    "ace_items = ['w1q151', 'w1q152', 'w1q153', \n",
    "  'w1q154', 'w1q155', 'w1q156', 'w1q157', \n",
    "  'w1q158', 'w1q159', 'w1q160', 'w1q161']\n",
    "\n",
    "childhd_gnc_items = [\n",
    "  'w1q147', 'w1q148', 'w1q149', 'w1q150']\n",
    "\n",
    "############# NOT CONDENSED (YET) #############\n",
    "# strain_items = ['w1q146a', 'w1q146b', \n",
    "#   'w1q146c', 'w1q146d', 'w1q146e', 'w1q146f', \n",
    "#   'w1q146g', 'w1q146h', 'w1q146i', 'w1q146j', \n",
    "#   'w1q146k', 'w1q146l']\n",
    "############# NOT CONDENSED (YET) #############\n",
    "\n",
    "daily_discr_items = ['w1q144a', 'w1q144b', \n",
    "  'w1q144c','w1q144d', 'w1q144e', 'w1q144f', \n",
    "  'w1q144g', 'w1q144h', 'w1q144i']\n",
    "\n",
    "# bi_stigma_items = [  # only in wave 2\n",
    "#   'w2q117', 'w2q118', 'w2q119', 'w2q120']\n",
    "\n",
    "int_homo_items = ['w1q128', 'w1q129', \n",
    "  'w1q130', 'w1q131', 'w1q132']\n",
    "\n",
    "felt_stigma_items = ['w1q125', 'w1q126', 'w1q127']\n",
    "\n",
    "drug_items = ['w1q90', 'w1q91', 'w1q92', \n",
    "  'w1q93', 'w1q94', 'w1q95', 'w1q96', \n",
    "  'w1q97', 'w1q98', 'w1q99', 'w1q100']\n",
    "\n",
    "alc_items = ['w1q85', 'w1q86', 'w1q87']\n",
    "\n",
    "ment_dis_items = ['w1q77a', 'w1q77b', \n",
    "  'w1q77c', 'w1q77d', 'w1q77e', 'w1q77f']\n",
    "\n",
    "hc_ster_threat_items = [\n",
    "  'w1q60', 'w1q61', 'w1q62', 'w1q63']\n",
    "\n",
    "comm_conn_items = ['w1q53', 'w1q54', \n",
    "  'w1q55', 'w1q56', 'w1q57', 'w1q58', 'w1q59']\n",
    "\n",
    "lgbis_items = [\n",
    "  'w1q40', 'w1q41', 'w1q42', 'w1q43', 'w1q44']\n",
    "\n",
    "meim_items = [\n",
    "  'w1q21', 'w1q22', 'w1q23', 'w1q24', 'w1q25', 'w1q26']\n",
    "\n",
    "swl_items = [\n",
    "  'w1q186', 'w1q187', 'w1q188', 'w1q189', 'w1q190']\n",
    "\n",
    "swb_items = ['w1q04', 'w1q05', 'w1q06', 'w1q07', \n",
    "  'w1q08', 'w1q09', 'w1q10', 'w1q11', 'w1q12', \n",
    "  'w1q13', 'w1q14', 'w1q15', 'w1q16', 'w1q17', 'w1q18']\n",
    "\n",
    "drop_cols = (soc_supp_items + ace_items + childhd_gnc_items + \n",
    "  daily_discr_items + int_homo_items + felt_stigma_items + \n",
    "  drug_items + alc_items + ment_dis_items + hc_ster_threat_items + \n",
    "  comm_conn_items + lgbis_items + meim_items + swl_items + swb_items)\n",
    "len(drop_cols)\n",
    "\n",
    "meyer.drop(columns = drop_cols, inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "# these are the scales\n",
    "combo_features = ['w1socialwb', 'w1socialwb_i', \n",
    "  'w1lifesat', 'w1lifesat_i', 'w1meim', 'w1meim_i', \n",
    "  'w1idcentral', 'w1idcentral_i', 'w1connectedness',\n",
    "  'w1connectedness_i', 'w1hcthreat', 'w1hcthreat_i', \n",
    "  'w1kessler6', 'w1kessler6_i', 'w1auditc', \n",
    "  'w1auditc_i', 'w1dudit', 'w1dudit_i', 'w1feltstigma', \n",
    "  'w1feltstigma_i', 'w1internalized', 'w1internalized_i', \n",
    "  'w1everyday', 'w1everyday_i', 'w1childgnc', 'w1childgnc_i',\n",
    "  'w1ace', 'w1ace_i', 'w1ace_emo', 'w1ace_emo_i', \n",
    "  'w1ace_inc', 'w1ace_inc_i', 'w1ace_ipv', 'w1ace_ipv_i', \n",
    "  'w1ace_men', 'w1ace_men_i', 'w1ace_phy', 'w1ace_phy_i', \n",
    "  'w1ace_sep', 'w1ace_sep_i', 'w1ace_sex', 'w1ace_sex_i', \n",
    "  'w1ace_sub', 'w1ace_sub_i', 'w1socsupport', \n",
    "  'w1socsupport_fam', 'w1socsupport_fam_i', \n",
    "  'w1socsupport_fr', 'w1socsupport_fr_i', \n",
    "  'w1socsupport_i', 'w1socsupport_so', 'w1socsupport_so_i']\n",
    "\n",
    "# split them up so I can check whether \n",
    "# the _i columns really are free of missing values\n",
    "combo_imputed = [c for c in combo_features if c[-2:]=='_i']\n",
    "combo_not_imputed = [c for c in combo_features if c[-2:]!='_i']\n",
    "\n",
    "# check my splits\n",
    "a = [c for c in combo_imputed if c in combo_not_imputed]\n",
    "b = [c for c in combo_not_imputed if c in combo_imputed]\n",
    "if len(a)==len(b)==0:\n",
    "  print('no overlap')\n",
    "  del (a, b)\n",
    "if (len(combo_imputed)+len(combo_not_imputed))==len(combo_features):\n",
    "  print('all accounted for')\n",
    "  \n",
    "# check for \"missing\" values\n",
    "for i in combo_imputed:\n",
    "  print('')\n",
    "  print(f'{i}, {meyer[i].isna().sum()} NAs')\n",
    "  print(meyer[i].value_counts(dropna = False).sort_values())\n",
    "\n",
    "# see how bad it was in the original\n",
    "for i in combo_not_imputed:\n",
    "  print('')\n",
    "  print(f'{i}, {meyer[i].isna().sum()} NAs')\n",
    "  print(meyer[i].value_counts(dropna = False).sort_values())\n",
    "missing_values = [('w1ace_sex', 75), ('w1ace_phy', 61), \n",
    "  ('w1ace_ipv', 139), ('w1ace_emo', 91), ('w1ace', 277), \n",
    "  ('w1everyday', 40), ('w1dudit', 66), ('w1connectedness', 51), \n",
    "  ('w1socialwb', 59), ('most others', '15-35')]\n",
    "\n",
    "# very grim and not at all surprising that column with the most missings is IPV\n",
    "# (from the ACE survey - so IPV among parents, not participants themselves)\n",
    "# >>> meyer['w1ace_ipv'].value_counts(dropna = False)\n",
    "# w1ace_ipv\n",
    "# 0    950\n",
    "# 1    429\n",
    "#      139\n",
    "# Name: count, dtype: int64\n",
    "# >>> meyer['w1ace_ipv_i'].value_counts(dropna = False)\n",
    "# w1ace_ipv_i\n",
    "# 0    1024\n",
    "# 1     494\n",
    "# Name: count, dtype: int64\n",
    "# >>> 1024-950\n",
    "# 74\n",
    "# >>> 139-74\n",
    "# 65\n",
    "\n",
    "# ok.  their imputation method seems really solid, but that is sus af.\n",
    "# of the people who declined to answer about IPV, you think the majority \n",
    "# of them would have said NO!?!??!??????\n",
    "# I'd be tempted to put every one of those guys down as a yes!!!!\n",
    "# let me see the overlap with other stigmatized stuff\n",
    "len(meyer[(meyer['w1ace_ipv']==' ') & (meyer['w1ace_sex']==' ')])\n",
    "# 30 huh damn I would have thought like all of them\n",
    "# wait.\n",
    "len(meyer[(meyer['w1ace_ipv']=='1') & (meyer['w1ace_sex']==' ')])  # 19\n",
    "len(meyer[(meyer['w1ace_ipv']==' ') & (meyer['w1ace_sex']=='1')])  # 51\n",
    "# ok! that actually lends credibility to their imputation though\n",
    "# if people were too ashamed to admit IPV, they'd probably decline to answer\n",
    "# about CSA too.  these people, at least, really could go either way on IPV.\n",
    "\n",
    "# This is all actually moot \n",
    "# because I don't have a better idea for imputation than what they did.\n",
    "# I think the thing to do, if I can, is to model it on the imputed data\n",
    "# then model it again on the subsample where I've dropped all these NAs\n",
    "# at least the ACE NAs, where there's really a lot.\n",
    "# could also try modeling it just without those columns\n",
    "# a random forest would probably be good, \n",
    "# make sure the whole thing isn't hinging on imputed data\n",
    "# Meyer also probably ran some tests in his paper to make sure this is ok\n",
    "\n",
    "# OK SO.\n",
    "\n",
    "# I've gotta reduce my dimensions\n",
    "# the imputation methods they used seem very reasonable\n",
    "# (see p. 37 of 37166-Documentation-methodology.pdf)\n",
    "# so I'm going to drop the non-imputed ones\n",
    "meyer.drop(columns = combo_not_imputed, inplace = True)\n",
    "meyer.shape  #(1518, 373)\n",
    "\n",
    "# ok!  love that!  let's see what else I can chuck.\n",
    "cols_to_check = [c for c in list(meyer.columns) if c not in combo_imputed]\n",
    "print(cols_to_check)\n",
    "\n",
    "# I am looking thru the documentation and making 2 lists\n",
    "# the variables should ROUGHLY correspond to each other\n",
    "# maybe I'll do some line breaks or whatever.\n",
    "drop_cols_2 = [\n",
    "  'w1q165', 'w1q27', 'w1q28', \n",
    "  'w1q20_1', 'w1q20_2', 'w1q20_3', 'w1q20_4', 'w1q20_5', 'w1q20_6', 'w1q20_7', \n",
    "  'w1q29', 'w1q29_t_verb', \n",
    "  # kept all ed columns\n",
    "  'gruca', 'gruca_i', 'gurban', 'gzipstate',  'gzipcode', \n",
    "  'w1hinc', 'w1poverty', 'w1povertycat', \n",
    "  'w1q133', 'w1q133_1', 'w1q133_2', 'w1q133_3', \n",
    "  # from here I'm just going thru the 831 page documentation and picking stuff out\n",
    "  'w1weight_orig', \n",
    "  'gmsaname', 'gmethod_type', 'gmethod_type_w2', 'gmethod_type_w3', \n",
    "  'w1cumulative_wt_nr1', 'w1cumulative_wt_nr2', 'w1cumulative_wt_nr3', \n",
    "  'w1cumulative_wt_sampling', 'w1weighting_cell_nr1', 'w1weighting_cell_nr2and3', \n",
    "  'w1frame_wt', \n",
    "]\n",
    "\n",
    "keep_cols = ['studyid', 'waveparticipated', 'w1survey_yr', \n",
    "  'w1age', 'cohort', 'w1sex', 'w1gender', 'w1sex_gender', \n",
    "  'screen_race', 'w1race',\n",
    "  'w1sexualid', 'w1sexminid', \n",
    "  'geduc1', 'geduc2', 'geducation', \n",
    "  'gurban_i', 'gcendiv', 'gcenreg', 'gmilesaway', 'gmilesaway2', \n",
    "  'w1hinc_i', 'w1poverty_i', 'w1povertycat_i', \n",
    "  'w1conversion', 'w1conversionhc', 'w1conversionrel', \n",
    "  'w1weight_full', 'gemployment2010', \n",
    "]\n",
    "\n",
    "check_cols = [\n",
    "  # no age, sex stuff\n",
    "  'w1q20_t_verb', # I suspect this is part of q20 and got rolled into w1race, but I'm not sure off the top of my head\n",
    "]\n",
    "\n",
    "# can I weed out the redacted ones quickly?\n",
    "\n",
    "x = list(meyer.columns)\n",
    "x_redacted = [c for c in x if meyer[c].nunique()==1]\n",
    "# it works!  but I already caught these guys\n",
    "\n",
    "\n",
    "# I finished all the variables that were easy to parse out and assigned\n",
    "# them to a list above.  I am now going to drop the ones that I indicated,\n",
    "# and then I will go through the 831 page documentation to make sense \n",
    "# of what's left and try to drop more.\n",
    "\n",
    "meyer.drop(columns = drop_cols_2, inplace = True) # (1518, 337)\n",
    "\n",
    "suicidal_idea_beh = [\n",
    "  'w1q101', 'w1q102', 'w1q103', 'w1q104', 'w1q105', 'w1q106', 'w1q107', \n",
    "  'w1q108', 'w1q109', 'w1q110', 'w1q111', 'w1q112', 'w1q113', 'w1q114', \n",
    "  'w1q115', 'w1q116', 'w1q117', 'w1q118', 'w1q119', 'w1q120', 'w1q121', \n",
    "]\n",
    "\n",
    "drop_cols_2_again = [\n",
    "  'w1q31a', 'w1q31b', 'w1q31c', 'w1q31d', # redundant with sexual identity\n",
    "  'w1q66_1', 'w1q66_2', 'w1q66_3', 'w1q66_4', 'w1q66_5', 'w1q66_t_verb', # interesting to compare \n",
    "  #ER to Dr ofc, but not for this study.  too many columns already ^\n",
    "  'w1q67', 'w1q68_1', 'w1q68_2', 'w1q68_3', 'w1q70', 'w1q71', 'w1q73', # <- same ^ and V\n",
    "  'w1q74_1', 'w1q74_2', 'w1q74_3', 'w1q74_4', 'w1q74_7', 'w1q74_8', 'w1q74_9', \n",
    "  'w1q74_12', 'w1q74_13', 'w1q74_15', 'w1q74_16', 'w1q74_19', 'w1q80', 'w1q81',\n",
    "  'w1q82', 'w1q83', 'w1q84', 'w1q88', 'w1q118', \n",
    "]\n",
    "\n",
    "keep_cols_good_on_own = [\n",
    "  'w1q01', 'w1q02', 'w1q03', 'w1q32', 'w1q33', 'w1q34', 'w1q35', 'w1q36', \n",
    "  'w1q37', 'w1q38', 'w1q52', 'w1q65', 'w1q69', 'w1q72', 'w1q74_21', \n",
    "  'w1q74_22', 'w1q74_23', 'w1q78', 'w1q79', 'w1q89', 'w1q119', 'w1q134', \n",
    "  'w1q136_7', 'w1q139_7', 'w1q140', 'w1q141_7', \n",
    "]\n",
    "\n",
    "keep_cols_ohe_done = [\n",
    "  'w1q30_1', 'w1q30_2', 'w1q30_3', 'w1q30_4', 'w1q30_5', \n",
    "  'w1q39_1', 'w1q39_2', 'w1q39_3', 'w1q39_4', 'w1q39_5', 'w1q39_6', 'w1q39_7', \n",
    "  'w1q39_8', 'w1q39_9', 'w1q39_10', 'w1q39_11', 'w1q39_12', 'w1q39_t_verb',\n",
    "]\n",
    "\n",
    "# manually combine columns in ways that *I* think make sense\n",
    "# here I am creating a dictionary where the keys are the new column names I want\n",
    "# and the values are lists, wherein the first value on the list is the method \n",
    "# of combination, and the remainder are the columns to be combined\n",
    "# if the method is 'recode', then I probably need to give it more direct attn\n",
    "# but all the others I am hoping to be able to automate\n",
    "# the end goal is to use this dictionary to either generate or directly run\n",
    "# the code to create the new columns and drop the ones no longer needed\n",
    "# before dropping the columns, I should check them against keep_cols\n",
    "# to make sure I'm not hoping to keep them individually in addition to combo'ing\n",
    "\n",
    "feat_eng_dict = {\n",
    "  'pers_well_being': ['sum', 'w1q01', 'w1q02', ],\n",
    "  'neighb_welcoming': ['mean', 'w1q19a', 'w1q19b', 'w1q19c', 'w1q19d', ],\n",
    "  'age_awakening': ['min','w1q45', 'w1q46', 'w1q47', 'w1q48', ],\n",
    "  'age_out': ['min', 'w1q49', 'w1q50', 'w1q51', ]\n",
    "  'health_insurance': ['binarize', 'w1q64_1', 'w1q64_2', 'w1q64_3', 'w1q64_4', \n",
    "    'w1q64_5', 'w1q64_6', 'w1q64_7', 'w1q64_8', 'w1q64_9', 'w1q64_10', \n",
    "    'w1q64_11', 'w1q64_12', 'w1q64_13', 'w1q64_t_verb', ], \n",
    "  'serious_health_cond': ['binarize', 'w1q74_5', 'w1q74_6', 'w1q74_10', \n",
    "    'w1q74_11', 'w1q74_14', 'w1q74_17', 'w1q74_18', 'w1q74_20', ], \n",
    "  'disabled': ['binarize', 'w1q75', 'w1q76', ],\n",
    "  'suicidal_ideation': ['sum', 'w1q101', 'w1q105', 'w1q109', ], \n",
    "  'suicide_attempts': ['recode', 'w1q113', 'w1q114', ],\n",
    "  'sui_idea_age_first': ['min', 'w1q102', 'w1q103', 'w1q104', \n",
    "    'w1q106', 'w1q107', 'w1q108', 'w1q110', 'w1q111', 'w1q112', ],\n",
    "  'sui_idea_age_recent': ['max', 'w1q102', 'w1q103', 'w1q104', \n",
    "    'w1q106', 'w1q107', 'w1q108', 'w1q110', 'w1q111', 'w1q112', ],\n",
    "  'sui_attem_age_first': ['min', 'w1q115', 'w1q116', 'w1q117', ], \n",
    "  'sui_attem_age_recent': ['max', 'w1q115', 'w1q116', 'w1q117', ], \n",
    "  'nssh_age_first': ['min', 'w1q120', 'w1q121', 'w1q122', ],\n",
    "  'nssh_age_recent': ['max', 'w1q120', 'w1q121', 'w1q122', ],\n",
    "  'outness': ['sum', 'w1q123a', 'w1q123b', 'w1q123c', 'w1q123d', 'w1q124', ],\n",
    "  \n",
    "  'abusive_treatment': ['sum', 'w1q135a', 'w1q135b', \n",
    "    'w1q135c', 'w1q135d', 'w1q135e', 'w1q135f', ],\n",
    "  'work_neg_outcomes': ['recode', 'w1q137', 'w1q138', ], # account for age\n",
    "  \n",
    "  'abus_treat_non_queer': ['binarize', 'w1q136_1', 'w1q136_5', 'w1q136_6', \n",
    "    'w1q136_8', 'w1q136_9', 'w1q136_10', ],\n",
    "  'work_disc_non_queer': ['binarize', 'w1q139_1', 'w1q139_5', 'w1q139_6', \n",
    "    'w1q139_8', 'w1q139_9', 'w1q139_10', ],\n",
    "  'housing_disc_non_queer': ['binarize', 'w1q141_1', 'w1q141_5', 'w1q141_6', \n",
    "    'w1q141_8', 'w1q141_9', 'w1q141_10', ]\n",
    "  \n",
    "  'abus_treat_sex_gender': ['binarize', 'w1q136_2', 'w1q136_3', 'w1q136_4', ],\n",
    "  'work_disc_sex_gender': ['binarize', 'w1q139_2', 'w1q139_3', 'w1q139_4', ],\n",
    "  'housing_disc_sex_gender': ['binarize', \n",
    "    'w1q141_2', 'w1q141_3', 'w1q141_4', ]\n",
    "  \n",
    "  \n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# script 2 in full: 2024-05-29_cols_to_check.txt\n",
    "\n",
    "cols_to_check\n",
    "\n",
    "These are the columns that remained after I dropped all the scale items and non-imputed scale features.  What I'm working on now is going through them to try and reduce dimensionality.\n",
    "\n",
    "THIS IS A CONTINUATION OF THE PREVIOUS DOC BY THE SAME NAME\n",
    "\n",
    "['gp1',  'w1q142a', 'w1q142b', 'w1q142c', 'w1q142d', 'w1q142e', 'w1q142f', 'w1q142g', 'w1q142h', 'w1q142i', 'w1q142j', 'w1q142k', 'w1q143_1', 'w1q143_2', 'w1q143_3', 'w1q143_4', 'w1q143_5', 'w1q143_6', 'w1q143_7', 'w1q143_8', 'w1q143_9', 'w1q143_10', 'w1q145_1', 'w1q145_2', 'w1q145_3', 'w1q145_4', 'w1q145_5', 'w1q145_6', 'w1q145_7', 'w1q145_8', 'w1q145_9', 'w1q145_10', 'w1q146a', 'w1q146b', 'w1q146c', 'w1q146d', 'w1q146e', 'w1q146f', 'w1q146g', 'w1q146h', 'w1q146i', 'w1q146j', 'w1q146k', 'w1q146l', 'w1q162', 'w1q163_1', 'w1q163_2', 'w1q163_3', 'w1q163_4', 'w1q163_5', 'w1q163_6', 'w1q163_7', 'w1q163_8', 'w1q163_9', 'w1q163_10', 'w1q166', 'w1q167', 'w1q168', 'w1q169', 'w1q170_1', 'w1q170_2', 'w1q170_3', 'w1q170_4', 'w1q171_1', 'w1q171_2', 'w1q171_3', 'w1q171_4', 'w1q171_5', 'w1q171_6', 'w1q171_7', 'w1q171_8', 'w1q171_9', 'w1q172', 'w1q173', 'w1q174', 'w1q175', 'w1q176', 'w1q177_1', 'w1q177_2', 'w1q177_3', 'w1q177_4', 'w1q177_5', 'w1q177_6', 'w1q177_7', 'w1q177_8', 'w1q177_9', 'w1q177_10', 'w1q177_11', 'w1q177_12', 'w1q178', 'w1q179', 'w1q180', 'w1q181', 'w1q182', 'w1q183', 'w1q184', 'w1q185',  'w1sample',  'w1pinc', 'w1pinc_i', 'grespondent_date_w2', 'gsurvey', 'gp2', 'grace', 'grespondent_date_w3', 'wave3', 'nopolicecontact']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# script 3 in full: 2024-05-29_notes_about_non-scale_columns.txt\n",
    "\n",
    "notes about non-scale columns\n",
    "\n",
    "- for right now I've kept the current well being and the future predicted well being columns, because I think it could be interesting to see if predicting higher or lower than current correlates with anything.  I've also made instructions to combine them (higher is better, regardless fo hether it's current WB or optimism for future WB), which may end up being more parsimonious.\n",
    "- I've also kept Q3, which is about happiness, but I suspect it's very corr'd with WB.  check that, and then either combine or drop it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thursday, May 30, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "today I finished goin through all the columns!  I was less productive than usual because I went outside to protect the goslings from a hawk.  I used the 2024-05-23 setup file, and continued to add on to the exploring file.\n",
    "\n",
    "files created: \n",
    "- 2024-05-23_thru_2024-05-30_exploring_Meyer_2023_dataset.py\n",
    "- 2024-05-30_cols_to_check.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script 1 in full:\n",
    "# 2024-05-23_thru_2024-05-30_exploring_Meyer_2023_dataset.py\n",
    "\n",
    "# 5/23/24 AND 5/29/24\n",
    "\n",
    "# check out this dataset from Ilan Meyer, my beloved\n",
    "\n",
    "# Import Data\n",
    "meyer = pd.read_csv('./potential_datasets/2024_05_23_download_ICPSR_Meyer_2023_generations_data_attempt_2/ICPSR_37166/DS0007/37166-0007-Data.tsv', sep = '\\t', low_memory=False)\n",
    "meyer.shape #(1518, 1329), but we can cut that down.\n",
    "og_columns = list(meyer.columns)\n",
    "\n",
    "# for RIGHT NOW, I think I'm only going to look at wave 1\n",
    "# the wave stuff is REALLY COOL, but i don't think I understand\n",
    "# time series well enough to get into it for this project\n",
    "# if I get a model working based on wave 1, and write it up\n",
    "# and all that \"good enough,\" then I'll come back and add \n",
    "# some time series stuff\n",
    "# oh it can even just be another phase of the project!\n",
    "# no need to replace anything!  just section B.\n",
    "\n",
    "meyer['WAVEPARTICIPATED'].value_counts(dropna =False)\n",
    "# matches what's in the documentation\n",
    "\n",
    "# ok, so, for now, I am going to drop all the variables marked W2 and W3.  \n",
    "cols = list(meyer.columns)\n",
    "w1_cols = [c for c in cols if c[:2]!='W2']\n",
    "w1_cols = [c for c in w1_cols if c[:2]!='W3']\n",
    "len(w1_cols)\n",
    "\n",
    "# drop columns\n",
    "meyer = meyer[w1_cols]\n",
    "meyer.shape\n",
    "\n",
    "# NAs?\n",
    "meyer.isna().sum().sum()\n",
    "# it says 0, but it's not 0\n",
    "# they must have imputed it with an empty string or something\n",
    "# when I do .value_counts(), a lot of columns have a blank row, e.g.,\n",
    "#                      14\n",
    "# where there absolutely should not be a blank.\n",
    "# so that's fun!  gonna have to clean that up!\n",
    "\n",
    "# i originally thought they only included the base variables\n",
    "# and I would have to compute the scale scores myself, but\n",
    "# OMG THEY DID INCLUDE THE CALC'D SCALES!!!!!!\n",
    "# I JUST COULDN'T FIND THEM BECAUSE ALL CAPS!!\n",
    "\n",
    "meyer.columns = [c.lower() for c in list(meyer.columns)]\n",
    "\n",
    "# GOAL: drop the columns that contributed to combination scores\n",
    "\n",
    "# Make lists of the items that comprise scales\n",
    "soc_supp_items = ['w1q164a', 'w1q164b', 'w1q164c', \n",
    "  'w1q164d', 'w1q164e', 'w1q164f', 'w1q164g', 'w1q164h', \n",
    "  'w1q164i', 'w1q164j', 'w1q164k', 'w1q164l']\n",
    "\n",
    "ace_items = ['w1q151', 'w1q152', 'w1q153', \n",
    "  'w1q154', 'w1q155', 'w1q156', 'w1q157', \n",
    "  'w1q158', 'w1q159', 'w1q160', 'w1q161']\n",
    "\n",
    "childhd_gnc_items = [\n",
    "  'w1q147', 'w1q148', 'w1q149', 'w1q150']\n",
    "\n",
    "############# NOT CONDENSED (YET) #############\n",
    "# strain_items = ['w1q146a', 'w1q146b', \n",
    "#   'w1q146c', 'w1q146d', 'w1q146e', 'w1q146f', \n",
    "#   'w1q146g', 'w1q146h', 'w1q146i', 'w1q146j', \n",
    "#   'w1q146k', 'w1q146l']\n",
    "############# NOT CONDENSED (YET) #############\n",
    "\n",
    "daily_discr_items = ['w1q144a', 'w1q144b', \n",
    "  'w1q144c','w1q144d', 'w1q144e', 'w1q144f', \n",
    "  'w1q144g', 'w1q144h', 'w1q144i']\n",
    "\n",
    "# bi_stigma_items = [  # only in wave 2\n",
    "#   'w2q117', 'w2q118', 'w2q119', 'w2q120']\n",
    "\n",
    "int_homo_items = ['w1q128', 'w1q129', \n",
    "  'w1q130', 'w1q131', 'w1q132']\n",
    "\n",
    "felt_stigma_items = ['w1q125', 'w1q126', 'w1q127']\n",
    "\n",
    "drug_items = ['w1q90', 'w1q91', 'w1q92', \n",
    "  'w1q93', 'w1q94', 'w1q95', 'w1q96', \n",
    "  'w1q97', 'w1q98', 'w1q99', 'w1q100']\n",
    "\n",
    "alc_items = ['w1q85', 'w1q86', 'w1q87']\n",
    "\n",
    "ment_dis_items = ['w1q77a', 'w1q77b', \n",
    "  'w1q77c', 'w1q77d', 'w1q77e', 'w1q77f']\n",
    "\n",
    "hc_ster_threat_items = [\n",
    "  'w1q60', 'w1q61', 'w1q62', 'w1q63']\n",
    "\n",
    "comm_conn_items = ['w1q53', 'w1q54', \n",
    "  'w1q55', 'w1q56', 'w1q57', 'w1q58', 'w1q59']\n",
    "\n",
    "lgbis_items = [\n",
    "  'w1q40', 'w1q41', 'w1q42', 'w1q43', 'w1q44']\n",
    "\n",
    "meim_items = [\n",
    "  'w1q21', 'w1q22', 'w1q23', 'w1q24', 'w1q25', 'w1q26']\n",
    "\n",
    "swl_items = [\n",
    "  'w1q186', 'w1q187', 'w1q188', 'w1q189', 'w1q190']\n",
    "\n",
    "swb_items = ['w1q04', 'w1q05', 'w1q06', 'w1q07', \n",
    "  'w1q08', 'w1q09', 'w1q10', 'w1q11', 'w1q12', \n",
    "  'w1q13', 'w1q14', 'w1q15', 'w1q16', 'w1q17', 'w1q18']\n",
    "\n",
    "drop_cols = (soc_supp_items + ace_items + childhd_gnc_items + \n",
    "  daily_discr_items + int_homo_items + felt_stigma_items + \n",
    "  drug_items + alc_items + ment_dis_items + hc_ster_threat_items + \n",
    "  comm_conn_items + lgbis_items + meim_items + swl_items + swb_items)\n",
    "len(drop_cols)\n",
    "\n",
    "meyer.drop(columns = drop_cols, inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "# these are the scales\n",
    "combo_features = ['w1socialwb', 'w1socialwb_i', \n",
    "  'w1lifesat', 'w1lifesat_i', 'w1meim', 'w1meim_i', \n",
    "  'w1idcentral', 'w1idcentral_i', 'w1connectedness',\n",
    "  'w1connectedness_i', 'w1hcthreat', 'w1hcthreat_i', \n",
    "  'w1kessler6', 'w1kessler6_i', 'w1auditc', \n",
    "  'w1auditc_i', 'w1dudit', 'w1dudit_i', 'w1feltstigma', \n",
    "  'w1feltstigma_i', 'w1internalized', 'w1internalized_i', \n",
    "  'w1everyday', 'w1everyday_i', 'w1childgnc', 'w1childgnc_i',\n",
    "  'w1ace', 'w1ace_i', 'w1ace_emo', 'w1ace_emo_i', \n",
    "  'w1ace_inc', 'w1ace_inc_i', 'w1ace_ipv', 'w1ace_ipv_i', \n",
    "  'w1ace_men', 'w1ace_men_i', 'w1ace_phy', 'w1ace_phy_i', \n",
    "  'w1ace_sep', 'w1ace_sep_i', 'w1ace_sex', 'w1ace_sex_i', \n",
    "  'w1ace_sub', 'w1ace_sub_i', 'w1socsupport', \n",
    "  'w1socsupport_fam', 'w1socsupport_fam_i', \n",
    "  'w1socsupport_fr', 'w1socsupport_fr_i', \n",
    "  'w1socsupport_i', 'w1socsupport_so', 'w1socsupport_so_i']\n",
    "\n",
    "# split them up so I can check whether \n",
    "# the _i columns really are free of missing values\n",
    "combo_imputed = [c for c in combo_features if c[-2:]=='_i']\n",
    "combo_not_imputed = [c for c in combo_features if c[-2:]!='_i']\n",
    "\n",
    "# check my splits\n",
    "a = [c for c in combo_imputed if c in combo_not_imputed]\n",
    "b = [c for c in combo_not_imputed if c in combo_imputed]\n",
    "if len(a)==len(b)==0:\n",
    "  print('no overlap')\n",
    "  del (a, b)\n",
    "if (len(combo_imputed)+len(combo_not_imputed))==len(combo_features):\n",
    "  print('all accounted for')\n",
    "  \n",
    "# check for \"missing\" values\n",
    "for i in combo_imputed:\n",
    "  print('')\n",
    "  print(f'{i}, {meyer[i].isna().sum()} NAs')\n",
    "  print(meyer[i].value_counts(dropna = False).sort_values())\n",
    "\n",
    "# see how bad it was in the original\n",
    "for i in combo_not_imputed:\n",
    "  print('')\n",
    "  print(f'{i}, {meyer[i].isna().sum()} NAs')\n",
    "  print(meyer[i].value_counts(dropna = False).sort_values())\n",
    "missing_values = [('w1ace_sex', 75), ('w1ace_phy', 61), \n",
    "  ('w1ace_ipv', 139), ('w1ace_emo', 91), ('w1ace', 277), \n",
    "  ('w1everyday', 40), ('w1dudit', 66), ('w1connectedness', 51), \n",
    "  ('w1socialwb', 59), ('most others', '15-35')]\n",
    "\n",
    "# very grim and not at all surprising that column with the most missings is IPV\n",
    "# (from the ACE survey - so IPV among parents, not participants themselves)\n",
    "# >>> meyer['w1ace_ipv'].value_counts(dropna = False)\n",
    "# w1ace_ipv\n",
    "# 0    950\n",
    "# 1    429\n",
    "#      139\n",
    "# Name: count, dtype: int64\n",
    "# >>> meyer['w1ace_ipv_i'].value_counts(dropna = False)\n",
    "# w1ace_ipv_i\n",
    "# 0    1024\n",
    "# 1     494\n",
    "# Name: count, dtype: int64\n",
    "# >>> 1024-950\n",
    "# 74\n",
    "# >>> 139-74\n",
    "# 65\n",
    "\n",
    "# ok.  their imputation method seems really solid, but that is sus af.\n",
    "# of the people who declined to answer about IPV, you think the majority \n",
    "# of them would have said NO!?!??!??????\n",
    "# I'd be tempted to put every one of those guys down as a yes!!!!\n",
    "# let me see the overlap with other stigmatized stuff\n",
    "len(meyer[(meyer['w1ace_ipv']==' ') & (meyer['w1ace_sex']==' ')])\n",
    "# 30 huh damn I would have thought like all of them\n",
    "# wait.\n",
    "len(meyer[(meyer['w1ace_ipv']=='1') & (meyer['w1ace_sex']==' ')])  # 19\n",
    "len(meyer[(meyer['w1ace_ipv']==' ') & (meyer['w1ace_sex']=='1')])  # 51\n",
    "# ok! that actually lends credibility to their imputation though\n",
    "# if people were too ashamed to admit IPV, they'd probably decline to answer\n",
    "# about CSA too.  these people, at least, really could go either way on IPV.\n",
    "\n",
    "# This is all actually moot \n",
    "# because I don't have a better idea for imputation than what they did.\n",
    "# I think the thing to do, if I can, is to model it on the imputed data\n",
    "# then model it again on the subsample where I've dropped all these NAs\n",
    "# at least the ACE NAs, where there's really a lot.\n",
    "# could also try modeling it just without those columns\n",
    "# a random forest would probably be good, \n",
    "# make sure the whole thing isn't hinging on imputed data\n",
    "# Meyer also probably ran some tests in his paper to make sure this is ok\n",
    "\n",
    "# OK SO.\n",
    "\n",
    "# I've gotta reduce my dimensions\n",
    "# the imputation methods they used seem very reasonable\n",
    "# (see p. 37 of 37166-Documentation-methodology.pdf)\n",
    "# so I'm going to drop the non-imputed ones\n",
    "meyer.drop(columns = combo_not_imputed, inplace = True)\n",
    "meyer.shape  #(1518, 373)\n",
    "\n",
    "# ok!  love that!  let's see what else I can chuck.\n",
    "cols_to_check = [c for c in list(meyer.columns) if c not in combo_imputed]\n",
    "print(cols_to_check)\n",
    "\n",
    "# I am looking thru the documentation and making 2 lists\n",
    "# the variables should ROUGHLY correspond to each other\n",
    "# maybe I'll do some line breaks or whatever.\n",
    "drop_cols_2 = [\n",
    "  'w1q165', 'w1q27', 'w1q28', \n",
    "  'w1q20_1', 'w1q20_2', 'w1q20_3', 'w1q20_4', 'w1q20_5', 'w1q20_6', 'w1q20_7', \n",
    "  'w1q29', 'w1q29_t_verb', \n",
    "  # kept all ed columns\n",
    "  'gruca', 'gruca_i', 'gurban', 'gzipstate',  'gzipcode', \n",
    "  'w1hinc', 'w1poverty', 'w1povertycat', \n",
    "  'w1q133', 'w1q133_1', 'w1q133_2', 'w1q133_3', \n",
    "  # from here I'm just going thru the 831 page documentation and picking stuff out\n",
    "  'w1weight_orig', \n",
    "  'gmsaname', 'gmethod_type', 'gmethod_type_w2', 'gmethod_type_w3', \n",
    "  'w1cumulative_wt_nr1', 'w1cumulative_wt_nr2', 'w1cumulative_wt_nr3', \n",
    "  'w1cumulative_wt_sampling', 'w1weighting_cell_nr1', 'w1weighting_cell_nr2and3', \n",
    "  'w1frame_wt', \n",
    "]\n",
    "\n",
    "keep_cols = ['studyid', 'waveparticipated', 'w1survey_yr', \n",
    "  'w1age', 'cohort', 'w1sex', 'w1gender', 'w1sex_gender', \n",
    "  'screen_race', 'w1race',\n",
    "  'w1sexualid', 'w1sexminid', \n",
    "  'geduc1', 'geduc2', 'geducation', \n",
    "  'gurban_i', 'gcendiv', 'gcenreg', 'gmilesaway', 'gmilesaway2', \n",
    "  'w1hinc_i', 'w1poverty_i', 'w1povertycat_i', \n",
    "  'w1conversion', 'w1conversionhc', 'w1conversionrel', \n",
    "  'w1weight_full', 'gemployment2010', \n",
    "]\n",
    "\n",
    "check_cols = [\n",
    "  # no age, sex stuff\n",
    "  'w1q20_t_verb', # I suspect this is part of q20 and got rolled into w1race, but I'm not sure off the top of my head\n",
    "]\n",
    "\n",
    "# can I weed out the redacted ones quickly?\n",
    "\n",
    "x = list(meyer.columns)\n",
    "x_redacted = [c for c in x if meyer[c].nunique()==1]\n",
    "# it works!  but I already caught these guys\n",
    "\n",
    "\n",
    "# I finished all the variables that were easy to parse out and assigned\n",
    "# them to a list above.  I am now going to drop the ones that I indicated,\n",
    "# and then I will go through the 831 page documentation to make sense \n",
    "# of what's left and try to drop more.\n",
    "\n",
    "meyer.drop(columns = drop_cols_2, inplace = True) # (1518, 337)\n",
    "\n",
    "suicidal_idea_beh = [\n",
    "  'w1q101', 'w1q102', 'w1q103', 'w1q104', 'w1q105', 'w1q106', 'w1q107', \n",
    "  'w1q108', 'w1q109', 'w1q110', 'w1q111', 'w1q112', 'w1q113', 'w1q114', \n",
    "  'w1q115', 'w1q116', 'w1q117', 'w1q118', 'w1q119', 'w1q120', 'w1q121',\n",
    "  'w1q122']\n",
    "\n",
    "drop_cols_2_again = [\n",
    "  'w1q31a', 'w1q31b', 'w1q31c', 'w1q31d', # redundant with sexual identity\n",
    "  'w1q66_1', 'w1q66_2', 'w1q66_3', 'w1q66_4', 'w1q66_5', 'w1q66_t_verb', # interesting to compare \n",
    "  #ER to Dr ofc, but not for this study.  too many columns already ^\n",
    "  'w1q67', 'w1q68_1', 'w1q68_2', 'w1q68_3', 'w1q70', 'w1q71', 'w1q73', # <- same ^ and V\n",
    "  'w1q74_1', 'w1q74_2', 'w1q74_3', 'w1q74_4', 'w1q74_7', 'w1q74_8', 'w1q74_9', \n",
    "  'w1q74_12', 'w1q74_13', 'w1q74_15', 'w1q74_16', 'w1q74_19', 'w1q80', 'w1q81',\n",
    "  'w1q82', 'w1q83', 'w1q84', 'w1q88', 'w1q118', 'w1q170_1', 'w1q170_2', \n",
    "  'w1q170_3', 'w1q170_4', 'w1q172', 'w1q173', 'w1q174', 'w1q176', 'w1q177_1', \n",
    "  'w1q177_2', 'w1q177_3', 'w1q177_4', 'w1q177_5', 'w1q177_6', 'w1q177_7', \n",
    "  'w1q177_8', 'w1q177_9', 'w1q177_10', 'w1q177_11', 'w1q177_12', 'w1q178', \n",
    "  'w1q182', 'w1q183', 'w1q184', 'w1q185',  'w1sample',  'w1pinc', \n",
    "  'grespondent_date_w2', 'gsurvey', 'gp2', 'grace', 'grespondent_date_w3', \n",
    "  'wave3', 'nopolicecontact']\n",
    "\n",
    "keep_cols_good_on_own = ['w1q01', 'w1q02', 'w1q03', 'w1q32', \n",
    "  'w1q33', 'w1q34', 'w1q35', 'w1q36', 'w1q37', 'w1q38', 'w1q52', \n",
    "  'w1q65',  'w1q69', 'w1q72', 'w1q74_21', 'w1q74_22', 'w1q74_23', \n",
    "  'w1q78', 'w1q79', 'w1q89', 'w1q119', 'w1q134', 'w1q136_7', 'w1q139_7', \n",
    "  'w1q140', 'w1q141_7', 'w1q143_7', 'w1q145_7', 'w1q162',  'w1q163_7', \n",
    "  'w1q166', 'w1q167', 'w1q168', 'w1q169', 'w1q175', 'gp1', 'w1pinc_i']\n",
    "\n",
    "keep_cols_ohe_done = ['w1q30_1', 'w1q30_2', 'w1q30_3', 'w1q30_4', 'w1q30_5', \n",
    "  'w1q39_1', 'w1q39_2', 'w1q39_3', 'w1q39_4', 'w1q39_5', 'w1q39_6', 'w1q39_7', \n",
    "  'w1q39_8', 'w1q39_9', 'w1q39_10', 'w1q39_11', 'w1q39_12', 'w1q39_t_verb',\n",
    "  'w1q171_1', 'w1q171_2', 'w1q171_3', 'w1q171_4', 'w1q171_5', 'w1q171_6', \n",
    "  'w1q171_7', 'w1q171_8', 'w1q171_9']\n",
    "\n",
    "# manually combine columns in ways that *I* think make sense\n",
    "# here I am creating a dictionary where the keys are the new column names I want\n",
    "# and the values are lists, wherein the first value on the list is the method \n",
    "# of combination, and the remainder are the columns to be combined\n",
    "# if the method is 'recode', then I probably need to give it more direct attn\n",
    "# but all the others I am hoping to be able to automate\n",
    "# the end goal is to use this dictionary to either generate or directly run\n",
    "# the code to create the new columns and drop the ones no longer needed\n",
    "# before dropping the columns, I should check them against keep_cols\n",
    "# to make sure I'm not hoping to keep them individually in addition to combo'ing\n",
    "\n",
    "feat_eng_dict = {'pers_well_being': ['sum', 'w1q01', 'w1q02'],\n",
    "  'neighb_welcoming': ['mean', 'w1q19a', 'w1q19b', 'w1q19c', 'w1q19d'],\n",
    "  'age_awakening': ['min','w1q45', 'w1q46', 'w1q47', 'w1q48'],\n",
    "  'age_out': ['min', 'w1q49', 'w1q50', 'w1q51'],\n",
    "  'health_insurance': ['binarize', 'w1q64_1', 'w1q64_2', 'w1q64_3', 'w1q64_4', \n",
    "    'w1q64_5', 'w1q64_6', 'w1q64_7', 'w1q64_8', 'w1q64_9', 'w1q64_10', \n",
    "    'w1q64_11', 'w1q64_12', 'w1q64_13', 'w1q64_t_verb'], \n",
    "  'serious_health_cond': ['binarize', 'w1q74_5', 'w1q74_6', 'w1q74_10', \n",
    "    'w1q74_11', 'w1q74_14', 'w1q74_17', 'w1q74_18', 'w1q74_20'], \n",
    "  'disabled': ['binarize', 'w1q75', 'w1q76'],\n",
    "  'suicidal_ideation': ['sum', 'w1q101', 'w1q105', 'w1q109'], \n",
    "  'suicide_attempts': ['recode', 'w1q113', 'w1q114'],\n",
    "  'sui_idea_age_first': ['min', 'w1q102', 'w1q103', 'w1q104', \n",
    "    'w1q106', 'w1q107', 'w1q108', 'w1q110', 'w1q111', 'w1q112'],\n",
    "  'sui_idea_age_recent': ['max', 'w1q102', 'w1q103', 'w1q104', \n",
    "    'w1q106', 'w1q107', 'w1q108', 'w1q110', 'w1q111', 'w1q112'],\n",
    "  'sui_attem_age_first': ['min', 'w1q115', 'w1q116', 'w1q117'], \n",
    "  'sui_attem_age_recent': ['max', 'w1q115', 'w1q116', 'w1q117'], \n",
    "  'nssh_age_first': ['min', 'w1q120', 'w1q121', 'w1q122'],\n",
    "  'nssh_age_recent': ['max', 'w1q120', 'w1q121', 'w1q122'],\n",
    "  'outness': ['sum', 'w1q123a', 'w1q123b', 'w1q123c', 'w1q123d', 'w1q124'],\n",
    "  'abusive_treatment': ['sum', 'w1q135a', 'w1q135b', \n",
    "    'w1q135c', 'w1q135d', 'w1q135e', 'w1q135f'],\n",
    "  'work_neg_outcomes': ['recode', 'w1q137', 'w1q138'], # account for age\n",
    "  'abus_treat_non_queer': ['binarize', 'w1q136_1', 'w1q136_5', 'w1q136_6', \n",
    "    'w1q136_8', 'w1q136_9', 'w1q136_10'],\n",
    "  'stress_past_year_gen': ['recode', 'w1q142a', 'w1q142h', 'w1q142i'],\n",
    "  'stress_past_year_work': ['recode', 'w1q142b', 'w1q142c', 'w1q142e'],\n",
    "  'stress_past_year_interpersonal': ['recode', 'w1q142d', 'w1q142f', 'w1q142g'],\n",
    "  'stress_past_year_crime': ['recode', 'w1q142j', 'w1q142k'],\n",
    "  'work_disc_non_queer': ['binarize', 'w1q139_1', 'w1q139_5', 'w1q139_6', \n",
    "    'w1q139_8', 'w1q139_9', 'w1q139_10'],\n",
    "  'housing_disc_non_queer': ['binarize', 'w1q141_1', 'w1q141_5', 'w1q141_6', \n",
    "    'w1q141_8', 'w1q141_9', 'w1q141_10'],\n",
    "  'stress_past_year_non_queer': ['binarize', 'w1q143_1', 'w1q143_5', 'w1q143_6', \n",
    "    'w1q143_8', 'w1q143_9', 'w1q143_10'],\n",
    "  'daily_discr_non_queer': ['binarize', 'w1q145_1', 'w1q145_5', 'w1q145_6'],\n",
    "  'childhd_bullying_non_queer': ['binarize', 'w1q163_1', 'w1q163_5', 'w1q163_6', \n",
    "    'w1q163_8', 'w1q163_9', 'w1q163_10'],\n",
    "  'abus_treat_sex_gender': ['binarize', 'w1q136_2', 'w1q136_3', 'w1q136_4'],\n",
    "  'work_disc_sex_gender': ['binarize', 'w1q139_2', 'w1q139_3', 'w1q139_4'],\n",
    "  'housing_disc_sex_gender': ['binarize', 'w1q141_2', 'w1q141_3', 'w1q141_4'],\n",
    "  'stress_past_year_sex_gender': ['binarize', 'w1q143_2', 'w1q143_3', 'w1q143_4'],\n",
    "  'daily_discr_sex_gender': ['binarize', 'w1q145_2', 'w1q145_3', 'w1q145_4', \n",
    "    'w1q145_8', 'w1q145_9', 'w1q145_10'],\n",
    "  'childhd_bullying_sex_gender': ['binarize', 'w1q163_2', 'w1q163_3', 'w1q163_4'],\n",
    "  'religiosity': ['recode', 'w1q179', 'w1q180', 'w1q181'], \n",
    "  'chronic_strain': ['sum', 'w1q146a', 'w1q146b', 'w1q146c', 'w1q146d', 'w1q146e', \n",
    "    'w1q146f', 'w1q146g', 'w1q146h', 'w1q146i', 'w1q146j', 'w1q146k', 'w1q146l']}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# script 2 in full:  \n",
    "\\# 2024-05-30_cols_to_check.txt\n",
    "\n",
    "cols_to_check\n",
    "\n",
    "These are the columns that remained after I dropped all the scale items and non-imputed scale features.  What I'm working on now is going through them to try and reduce dimensionality.\n",
    "\n",
    "THIS IS A CONTINUATION OF THE PREVIOUS DOC BY THE SAME NAME\n",
    "\n",
    "[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Friday, May 31, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "\n",
    "\n",
    "files created: \n",
    "- x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script 1 in full:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monday, June 3, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "today I continued working on a plan for null values and imputations.  I played around with a few different approaches, but ultimately I just made a list of every column with null values and made a plan for them one by one.  I am still debating whether ... something.   I forgot what the end of that sentence was.  I really want to do fancy, high quality imputation, but I think mode and median are the way to go, just for expediency.  Once all the null values are gone, it will be time to do the combinations as defined in the dictionary, do some more column dropping, and write up a data cleaning notebook.\n",
    "\n",
    "I dropped 11 rows that had straight people in them.\n",
    "\n",
    "**I'm going to use _ei in my column names to indicate \"Emily imputed.\"  The original authors used _i for their imputations.**\n",
    "\n",
    "I used the 5-31 setup script again, and the giant data cleaning script.  I then created a few documents as I worked.\n",
    "\n",
    "files created: \n",
    "- imputation_plan.xlsx -- a list of all columns with null values, how many, and what to do with them.\n",
    "- can-i-impute-0s.md was actually created on 5/31, but it's a good one.  i looked through all the columns with naturally occuring 0s, and decided whether or not I could impute 0s for the NAs without borking everything up\n",
    "- I created 04_scratch_work/ in the repo, and 01_notes/ and 02_scripts/ within it.  notes/ has copies of all my notes, and scripts/ has copies of the scripts\n",
    "- I updated my gitignore to allow .py files, for now at least\n",
    "- 2024-05-31_continuing_messing_with_meyer_columns.py was created on Friday with just a little bit of bridge work between the column stuff in the giant cleaning script and the imputation planning I did today.\n",
    "- 2024-06-03_imputation_groundwork.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script 1 in full:\n",
    "# 2024-05-31_continuing_messing_with_meyer_columns.py\n",
    "\n",
    "meyer.shape\n",
    "meyer.columns\n",
    "\n",
    "\n",
    "# Before we do this next bit, let's preserve these guys\n",
    "keep_cols = combo_imputed + keep_cols + keep_cols_good_on_own + keep_cols_ohe_done\n",
    "\n",
    "# Now I need to handle the columns in the dictionary\n",
    "\n",
    "# did I duplicate any names?\n",
    "for i in feat_eng_dict.keys():\n",
    "  if i in keep_cols:\n",
    "    print(i)\n",
    "\n",
    "# nope!\n",
    "\n",
    "# meyer_test = pd.read_csv('./potential_datasets/2024_05_23_download_ICPSR_Meyer_2023_generations_data_attempt_2/ICPSR_37166/DS0007/37166-0007-Data.tsv', sep = '\\t', low_memory=False, na_values = ' ')\n",
    "# meyer_test_nas = pd.DataFrame(data = meyer_test.isna().sum())\n",
    "\n",
    "# check again because I figured out how to handle the \"NAs\" as strings\n",
    "meyer.isna().sum().sum()\n",
    "for i in list(zip(list(meyer.dtypes), list(meyer.columns))):\n",
    "  print(i)\n",
    "\n",
    "# drop these guys\n",
    "meyer.drop(columns = drop_cols_2_again, inplace = True)\n",
    "\n",
    "# I want to fill all my NAs with 0s, so I can do math on things.\n",
    "# Can I do that?  Do any of these columns have natural 0s?\n",
    "\n",
    "for i in list(meyer.columns):\n",
    "  a = list(meyer[i].unique())\n",
    "  if 0 in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script 2 in full: \n",
    "# 2024-06-03_imputation_groundwork.py\n",
    "\n",
    "# June 3, 2024\n",
    "# Imputation\n",
    "\n",
    "# This is continuing some work I started last week.\n",
    "# I have defined a dictionary full of columns that I\n",
    "# want to combine one way or another.  I can't do that\n",
    "# if they're full of NAs.  Imputing 0s would be the \n",
    "# easiest way to do it, but I don't want to introduce\n",
    "# unreasonable amounts of error into my data.\n",
    "\n",
    "# I'm going to use _ei to indicate \"Emily imputed.\"\n",
    "# The original authors used _i for their imputations.\n",
    "\n",
    "# Last week I made a list of all the columns with \n",
    "# naturally occurring 0s in them. Today I want a \n",
    "# list of all remaining NAs.\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "meyer.isna().sum().sort_values(ascending = False)\n",
    "\n",
    "# Re-Import Data for clean copy\n",
    "mey = pd.read_csv(\n",
    "  './potential_datasets/2024_05_23_download_ICPSR_Meyer_2023_generations_data_attempt_2/ICPSR_37166/DS0007/37166-0007-Data.tsv', \n",
    "  sep = '\\t', low_memory=False, na_values = ' ') # Thanks to ibrahim rupawala for highlighting the na_values argument\n",
    "  # https://stackoverflow.com/questions/13445241/replacing-blank-values-white-space-with-nan-in-pandas/47105408#47105408\n",
    "\n",
    "mey.shape\n",
    "mey.columns = [c.lower() for c in list(mey.columns)]\n",
    "\n",
    "# I am going to try to find related columns for logical \n",
    "# imputation if possible, or at least to justify 0s.\n",
    "# (I'm going to try very hard not to get too derailed.)\n",
    "\n",
    "# Friends of gmilesaway2\n",
    "gmilesaway2 = mey[mey['gmilesaway2'].isna()]\n",
    "gmilesaway2['w1q67'].value_counts(dropna = False)\n",
    "mey['gmilesaway2'].groupby(mey['w1q67']).value_counts(dropna = False, normalize = True)\n",
    "# w1q67  gmilesaway2\n",
    "# 1.0    0.0            0.882353\n",
    "#        1.0            0.094118\n",
    "#        NaN            0.023529\n",
    "# 2.0    0.0            0.838710\n",
    "#        1.0            0.153226\n",
    "#        NaN            0.008065\n",
    "# 3.0    0.0            0.714844\n",
    "#        1.0            0.273438\n",
    "#        NaN            0.011719\n",
    "\n",
    "mey['gmilesaway2'].groupby(mey['gurban']).value_counts(dropna = False, normalize = True)\n",
    "# gurban  gmilesaway2\n",
    "# 0.0     1.0            0.675824\n",
    "#         0.0            0.313187\n",
    "#         NaN            0.010989\n",
    "# 1.0     0.0            0.801370\n",
    "#         1.0            0.197108\n",
    "#         NaN            0.001522\n",
    "\n",
    "# It's actually not a very good assumption that they live\n",
    "# more than 60 miles away. 73.5% of people live w/i 60m!\n",
    "# And yet, most people have never gone.  \n",
    "# I think the thing to impute here is a 0 under the original \n",
    "# meaning, that is, within 60 miles.\n",
    "\n",
    "\n",
    "# OK ACTUALLY\n",
    "\n",
    "# I've made a list of the NAs and I'm looking at THOSE\n",
    "# for how to impute.  THAT'S what this next section is.\n",
    "\n",
    "# w1q101\n",
    "# subset investigation\n",
    "s = mey.loc[cond, suicidal_idea_beh]\n",
    "s = s[['w1q101',  'w1q105', 'w1q109', 'w1q118', 'w1q119', 'w1q121']]\n",
    "\n",
    "# row condition\n",
    "cond = mey['w1q101'].isna()\n",
    "\n",
    "# column conditions\n",
    "y1 = meyer[((meyer['w1q105'].notna()) & (meyer['w1q105']!=1))]\n",
    "y2 = meyer[((meyer['w1q109'].notna()) & (meyer['w1q109']!=1))]\n",
    "y3 = meyer[((meyer['w1q113'].notna()) & (meyer['w1q113']!=1))]\n",
    "\n",
    "# In another pass I'd love to match the numbers\n",
    "# but for now we're just going to impute \"once.\"\n",
    "# It is at least not overcounting.\n",
    "meyer.loc[[(y1 | y2 | y3) & cond], 'w1q101'] = 2\n",
    "len(meyer[y1])\n",
    "\n",
    "# check these age columns\n",
    "cond = mey['w1q102'].isna() # none!\n",
    "cond = mey['w1q103'].isna()\n",
    "# ok this would take forever. n<10 -> impute median\n",
    "cond = mey['w1q112'].isna()\n",
    "s = mey.loc[cond, suicidal_idea_beh]\n",
    "\n",
    "# nssh\n",
    "cond = mey['w1q119'].isna()\n",
    "s = mey.loc[cond, suicidal_idea_beh]\n",
    "\n",
    "# hey, is it just a few rows with a ton of missing data?\n",
    "# I hate to lose data but if they truly didn't fill out anything...\n",
    "meyer.iloc[10,:].isna().sum()\n",
    "# oh right, there are all those OHE columns with planned missing\n",
    "# so I need to start imputing first, then recheck these\n",
    "# then decide whether to drop anything\n",
    "\n",
    "# why are there straight people in here?\n",
    "cond = mey['w1sexminid'].isna()\n",
    "s = mey[cond]\n",
    "s['w1sexualid']\n",
    "\n",
    "# begone, straights!\n",
    "meyer.shape # (1518, 267)\n",
    "meyer = meyer[meyer['w1sexualid']!=1]\n",
    "meyer.shape # (1507, 267)\n",
    "\n",
    "# ugh I really wish I had done that before!!\n",
    "# let's look at this again\n",
    "pd.set_option('display.max_rows', None)\n",
    "meyer.isna().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuesday, June 4, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "I did a lot of imputation today!  I worked really hard and did most of my imputation.  I left a handful of the complicated ones for tomorrow, and I need to tinker with the simple imputer ones again tomorrow too.\n",
    "\n",
    "I followed the guidelines I laid out in the spreadsheet yesterday.  It took several tries to get an `np.where()` statement working (I tried a whole function first, and then the humble `np.where()` is what actually got the job done!), but was then successful.  I checked and rechecked and rechecked very carefully to make sure I wasn't introducing error into my data.\n",
    "\n",
    "### This paragraph can probably go in as is!\n",
    "\n",
    "I chose to complete imputation on the entire dataset, prior to the train test split, because my primary interest lies in an inferential model.  This means that it is less crucial that my workflow and model be generalizable to new data gathered in the future.  Furthermore, because this data concerns sensitive personal and political issues, the median or mode responses would likely change over time if the survey were to be readministered, certainly over a long enough timeframe.  (Note: The obvious implications of this temporal drift for a 2016 dataset will be further discussed in the limitations and recommendations sections.)  It would be entirely illogical to fit the imputer on a subset of data from 2016 and then continue projecting those values forward into new data indefinitely.  Therefore, I prioritized fidelity to the current data over adaptability to hypothetical future data, and fit the imputers on my entire dataset.\n",
    "\n",
    "BUT the stupid 97s-99s threw off all the summary stats so the si fits I did today don't work.  I need to clean those columns up first and then re-fit them.  I coped the si syntax into its own script to make this easier, and so that I could continue the imputation I was doing today without making that script a mobius strip of time shenanigans.\n",
    "\n",
    "ALSO, I finally emailed UMich for permission to post their stuff.  Hopefully they get back to me quicker than I got to them.\n",
    "\n",
    "files created: \n",
    "\n",
    "- 2024-06-04_imputation_ATTEMPT-1_dealing-with-7s-and-97s-indiy-ohe-columns.py\n",
    "- - first attempt at this nonsense, tried a ton of things, abandoned this script once one of them worked\n",
    "\n",
    "- 2024-06-04_imputation_dealing-with-7s-and-97s-indiy-ohe-columns.py\n",
    "- - copied just the thing that worked from the last script and continued from there\n",
    "\n",
    "- 2024-06-04_imputation_misc.py\n",
    "- - once I got that first batch of stuff worked out, I jumped over here to get a clean start on the rest of it\n",
    "\n",
    "- 2024-06-04_simple_imputer_stuff.py\n",
    "- - this is a COPY of the si stuff from 2024-06-04_imputation_misc.py, because I discovered more 97s-99s that I need to clear out before it actually makes sense to do si\n",
    "\n",
    "- meyer_backup_diy_ohe_no_97s_7s.csv\n",
    "- - safety first\n",
    "\n",
    "- meyer_much_imputation_done_but_not_si_yet.csv\n",
    "- - safety first\n",
    "\n",
    "- MODIFIED imputation_plan.xlsx\n",
    "- - notes and color coding as I go along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 1 in full:\n",
    "2024-06-04_imputation_ATTEMPT-1_dealing-with-7s-and-97s-indiy-ohe-columns.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuesday, June 4, 2024\n",
    "# Imputation, Take 2\n",
    "\n",
    "# Ok, so yesterday I made an excel spreadsheet list of all of NAs\n",
    "# and decided what to do with them.  There are a bunch of different \n",
    "# methods (I chose what I thought was best for each type of data), \n",
    "# and I think the neatest way to handle all of them without making\n",
    "# a giant mess is to break up the dataframe (include the ID in \n",
    "# each subset so that they can be cleanly re-merged), use a \n",
    "# real, official imputer, and then put it back together again.\n",
    "\n",
    "# Oooh boy this could be dangerous.  I'd have to drop them from\n",
    "# the original to avoid overwriting or otherwise losing stuff.\n",
    "# Let me see what the shape is now.\n",
    "\n",
    "# REMEMBER TO DROP THE STRAIGHTS! IT'S THE ONLY THING I NEED \n",
    "# FROM THE 2024-06-03 IMPUTATION SCRIPT!\n",
    "\n",
    "meyer.shape # (1507, 267)\n",
    "# So that it is the shape I want to get back at the end,\n",
    "# minus any that I consciously, deliberately drop.\n",
    "\n",
    "\n",
    "# lmao ok let's start by dropping\n",
    "\n",
    "s = '''\n",
    "w1q20_t_verb\n",
    "w1q39_7\n",
    "w1q39_12\n",
    "w1q39_t_verb\n",
    "w1q39_6\n",
    "w1q39_5\n",
    "w1q39_2\n",
    "w1q39_3\n",
    "w1q39_4\n",
    "w1q39_1\n",
    "w1q39_9\n",
    "w1q39_8\n",
    "w1q39_10\n",
    "w1q39_11\n",
    "gp1\n",
    "gemployment2010\n",
    "gmilesaway'''\n",
    "\n",
    "print(s.replace('\\n', '\", \"'))\n",
    "\n",
    "drop_cols_3 = [\"w1q20_t_verb\", \"w1q39_7\", \"w1q39_12\", \n",
    "  \"w1q39_t_verb\", \"w1q39_6\", \"w1q39_5\", \"w1q39_2\", \"w1q39_3\", \n",
    "  \"w1q39_4\", \"w1q39_1\", \"w1q39_9\", \"w1q39_8\", \"w1q39_10\", \n",
    "  \"w1q39_11\", \"gp1\", \"gemployment2010\", \"gmilesaway\"]\n",
    "\n",
    "len(drop_cols_3)\n",
    "\n",
    "# Ok I started with 267 and dropped 17. Should now be 250.\n",
    "meyer.drop(columns = drop_cols_3).shape # Yep, (1507, 250)!\n",
    "meyer.drop(columns = drop_cols_3, inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "\n",
    "# Alrighty! Now we're just imputing 0s!\n",
    "# Then changing everything that's not 1 to 0.\n",
    "\n",
    "s = '''\n",
    "w1q145_3\n",
    "w1q163_3\n",
    "w1q136_3\n",
    "w1q145_9\n",
    "w1q143_3\n",
    "w1q136_10\n",
    "w1q145_10\n",
    "w1q136_9\n",
    "w1q163_10\n",
    "w1q163_9\n",
    "w1q143_9\n",
    "w1q143_4\n",
    "w1q136_6\n",
    "w1q143_10\n",
    "w1q143_5\n",
    "w1q145_4\n",
    "w1q136_5\n",
    "w1q143_8\n",
    "w1q163_5\n",
    "w1q136_4\n",
    "w1q163_6\n",
    "w1q145_6\n",
    "w1q136_1\n",
    "w1q143_7\n",
    "w1q163_1\n",
    "w1q143_2\n",
    "w1q143_1\n",
    "w1q145_5\n",
    "w1q143_6\n",
    "w1q163_2\n",
    "w1q163_4\n",
    "w1q136_8\n",
    "w1q145_8\n",
    "w1q145_1\n",
    "w1q145_7\n",
    "w1q163_7\n",
    "w1q136_2\n",
    "w1q145_2\n",
    "w1q139_3\n",
    "w1q136_7\n",
    "w1q139_9\n",
    "w1q139_10\n",
    "w1q139_5\n",
    "w1q139_4\n",
    "w1q139_8\n",
    "w1q139_6\n",
    "w1q139_2\n",
    "w1q139_1\n",
    "w1q139_7\n",
    "w1q163_8\n",
    "w1q141_3\n",
    "w1q141_9\n",
    "w1q141_10\n",
    "w1q141_8\n",
    "w1q141_4\n",
    "w1q141_1\n",
    "w1q141_5\n",
    "w1q141_2\n",
    "w1q141_7\n",
    "w1q141_6'''\n",
    "\n",
    "\n",
    "print(s.replace('\\n', '\", \"'))\n",
    "\n",
    "diy_ohe_part_1 = [\"w1q145_3\", \"w1q163_3\", \"w1q136_3\", \"w1q145_9\", \n",
    "  \"w1q143_3\", \"w1q136_10\", \"w1q145_10\", \"w1q136_9\", \"w1q163_10\", \n",
    "  \"w1q163_9\", \"w1q143_9\", \"w1q143_4\", \"w1q136_6\", \"w1q143_10\", \n",
    "  \"w1q143_5\", \"w1q145_4\", \"w1q136_5\", \"w1q143_8\", \"w1q163_5\", \n",
    "  \"w1q136_4\", \"w1q163_6\", \"w1q145_6\", \"w1q136_1\", \"w1q143_7\", \n",
    "  \"w1q163_1\", \"w1q143_2\", \"w1q143_1\", \"w1q145_5\", \"w1q143_6\", \n",
    "  \"w1q163_2\", \"w1q163_4\", \"w1q136_8\", \"w1q145_8\", \"w1q145_1\", \n",
    "  \"w1q145_7\", \"w1q163_7\", \"w1q136_2\", \"w1q145_2\", \"w1q139_3\", \n",
    "  \"w1q136_7\", \"w1q139_9\", \"w1q139_10\", \"w1q139_5\", \"w1q139_4\", \n",
    "  \"w1q139_8\", \"w1q139_6\", \"w1q139_2\", \"w1q139_1\", \"w1q139_7\", \n",
    "  \"w1q163_8\", \"w1q141_3\", \"w1q141_9\", \"w1q141_10\", \"w1q141_8\", \n",
    "  \"w1q141_4\", \"w1q141_1\", \"w1q141_5\", \"w1q141_2\", \"w1q141_7\", \n",
    "  \"w1q141_6\"]\n",
    "\n",
    "# Let me just check that these really are all 1s.\n",
    "\n",
    "for i in diy_ohe_part_1:\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "\n",
    "# Nope!  They're not! Ok let's combine them, screw it.\n",
    "del diy_ohe_part_1\n",
    "\n",
    "s = '''\n",
    "w1q64_12\n",
    "w1q64_5\n",
    "w1q74_5\n",
    "w1q74_6\n",
    "w1q74_20\n",
    "w1q64_11\n",
    "w1q64_10\n",
    "w1q74_18\n",
    "w1q74_14\n",
    "w1q74_17\n",
    "w1q171_8\n",
    "w1q30_3\n",
    "w1q64_13\n",
    "w1q64_7\n",
    "w1q30_4\n",
    "w1q171_6\n",
    "w1q171_4\n",
    "w1q64_8\n",
    "w1q74_10\n",
    "w1q74_21\n",
    "w1q64_6\n",
    "w1q74_11\n",
    "w1q171_5\n",
    "w1q64_3\n",
    "w1q64_1\n",
    "w1q171_9\n",
    "w1q30_5\n",
    "w1q171_3\n",
    "w1q74_22\n",
    "w1q64_9\n",
    "w1q171_2\n",
    "w1q171_7\n",
    "w1q74_23\n",
    "w1q64_4\n",
    "w1q64_2\n",
    "w1q30_1\n",
    "w1q171_1\n",
    "w1q30_2'''\n",
    "\n",
    "print(s.replace('\\n', '\", \"'))\n",
    "\n",
    "diy_ohe = [\"w1q145_3\", \"w1q163_3\", \"w1q136_3\", \"w1q145_9\", \n",
    "  \"w1q143_3\", \"w1q136_10\", \"w1q145_10\", \"w1q136_9\", \"w1q163_10\", \n",
    "  \"w1q163_9\", \"w1q143_9\", \"w1q143_4\", \"w1q136_6\", \"w1q143_10\", \n",
    "  \"w1q143_5\", \"w1q145_4\", \"w1q136_5\", \"w1q143_8\", \"w1q163_5\", \n",
    "  \"w1q136_4\", \"w1q163_6\", \"w1q145_6\", \"w1q136_1\", \"w1q143_7\", \n",
    "  \"w1q163_1\", \"w1q143_2\", \"w1q143_1\", \"w1q145_5\", \"w1q143_6\", \n",
    "  \"w1q163_2\", \"w1q163_4\", \"w1q136_8\", \"w1q145_8\", \"w1q145_1\", \n",
    "  \"w1q145_7\", \"w1q163_7\", \"w1q136_2\", \"w1q145_2\", \"w1q139_3\", \n",
    "  \"w1q136_7\", \"w1q139_9\", \"w1q139_10\", \"w1q139_5\", \"w1q139_4\", \n",
    "  \"w1q139_8\", \"w1q139_6\", \"w1q139_2\", \"w1q139_1\", \"w1q139_7\", \n",
    "  \"w1q163_8\", \"w1q141_3\", \"w1q141_9\", \"w1q141_10\", \"w1q141_8\", \n",
    "  \"w1q141_4\", \"w1q141_1\", \"w1q141_5\", \"w1q141_2\", \"w1q141_7\", \n",
    "  \"w1q141_6\", \"w1q64_12\", \"w1q64_5\", \"w1q74_5\", \"w1q74_6\", \n",
    "  \"w1q74_20\", \"w1q64_11\", \"w1q64_10\", \"w1q74_18\", \"w1q74_14\", \n",
    "  \"w1q74_17\", \"w1q171_8\", \"w1q30_3\", \"w1q64_13\", \"w1q64_7\", \n",
    "  \"w1q30_4\", \"w1q171_6\", \"w1q171_4\", \"w1q64_8\", \"w1q74_10\", \n",
    "  \"w1q74_21\", \"w1q64_6\", \"w1q74_11\", \"w1q171_5\", \"w1q64_3\", \n",
    "  \"w1q64_1\", \"w1q171_9\", \"w1q30_5\", \"w1q171_3\", \"w1q74_22\", \n",
    "  \"w1q64_9\", \"w1q171_2\", \"w1q171_7\", \"w1q74_23\", \"w1q64_4\", \n",
    "  \"w1q64_2\", \"w1q30_1\", \"w1q171_1\", \"w1q30_2\"]\n",
    "\n",
    "value_list = []\n",
    "for i in diy_ohe:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list += a\n",
    "value_list = list(set(value_list))\n",
    "sorted(value_list)\n",
    "\n",
    "sorted(value_list)[-1]-97 # = nan!\n",
    "\n",
    "test = pd.DataFrame(sorted(value_list))-97\n",
    "test[0].unique()\n",
    "\n",
    "# This works!\n",
    "# OK, here's the breakdown\n",
    "# Subtract 97 from everything, because that's the designated expected-NA value\n",
    "# 97-97=0, which is what I'm going to set the NAs to, so it's basically NA=NA\n",
    "# NA-97 = NA, then I just need to fill the NAs\n",
    "# some_number - 97 = some_other_number\n",
    "# So now all I have to do is convert the remaining NAs to 0, \n",
    "# and then convert everything that's not 0 to 1.\n",
    "\n",
    "# Let me check one more thing before proceeding.\n",
    "i=diy_ohe[12]\n",
    "for i in diy_ohe:\n",
    "  if len(list(meyer[i].unique()))!=3:\n",
    "    print(i)\n",
    "    print(meyer[i].value_counts(dropna = False))\n",
    "    print('='*20)\n",
    "# The only ones printing out are ones where there are only *2* values\n",
    "# the target one, and NaN.  We're good to proceed.\n",
    "\n",
    "# I think the most efficient way to do this is probably with a .map()\n",
    "\n",
    "\n",
    "diy_ohe_new = [''.join([x, '_ei']) for x in diy_ohe]\n",
    "\n",
    "meyer.shape # (1507, 250)\n",
    "len(diy_ohe) # 98\n",
    "len(diy_ohe_new) # 98\n",
    "meyer.loc[:, diy_ohe_new] = meyer.loc[:, diy_ohe].apply(lambda x: float(x)-97)\n",
    "meyer.shape # (1507, 348) huzzah!\n",
    "\n",
    "# check this again, should be no more 97s, but nothing else weird\n",
    "value_list = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list += a\n",
    "value_list = list(set(value_list))\n",
    "sorted(value_list)\n",
    "\n",
    "# Nope, it's all nans now\n",
    "\n",
    "def nan_97_fixer(x):\n",
    "  if float(x) == 97.0:\n",
    "    x = np.nan\n",
    "  else:\n",
    "    x = x\n",
    "  return x\n",
    "\n",
    "meyer.loc[:, diy_ohe_new] = meyer.loc[:, diy_ohe].map(nan_97_fixer)\n",
    "\n",
    "# check this again, should be no more 97s, but nothing else weird\n",
    "value_list = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list += a\n",
    "value_list = list(set(value_list))\n",
    "sorted(value_list)\n",
    "\n",
    "# Still all nans???????\n",
    "\n",
    "# ok BASIC test\n",
    "meyer.loc[:, diy_ohe_new] = meyer.loc[:, diy_ohe]\n",
    "\n",
    "value_list = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list += a\n",
    "value_list = list(set(value_list))\n",
    "sorted(value_list)\n",
    "\n",
    "# I just checked the original columns and they're fine.\n",
    "\n",
    "# let me test this\n",
    "pd.notna(98)\n",
    "\n",
    "# try again\n",
    "\n",
    "def nan_97_fixer(x):\n",
    "  if pd.isna(x)==True:\n",
    "    return 0\n",
    "  else: # meaning it's NOT NA\n",
    "    if float(x)==97.:\n",
    "      return 0\n",
    "    elif ((float(x)>0) & (float(x)<97)):\n",
    "      return 1\n",
    "    else:\n",
    "      return 1_000_000_000\n",
    "\n",
    "nan_97_fixer(5.) # it works.\n",
    "\n",
    "meyer.loc[:, diy_ohe_new] = meyer.loc[:, diy_ohe].map(nan_97_fixer)\n",
    "\n",
    "# check this again, should be no more 97s, but nothing else weird\n",
    "value_list = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list += a\n",
    "value_list = list(set(value_list))\n",
    "sorted(value_list)\n",
    "\n",
    "# oh ffs\n",
    "meyer.loc[:, diy_ohe_new] = meyer.loc[:, diy_ohe].replace(97, np.nan)\n",
    "\n",
    "# check this again, should be no more 97s, but nothing else weird\n",
    "value_list = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list += a\n",
    "value_list = list(set(value_list))\n",
    "sorted(value_list)\n",
    "\n",
    "\n",
    "# oh ffs\n",
    "meyer.loc[:, diy_ohe_new] = np.where(meyer.loc[:, diy_ohe]==97, 0, meyer.loc[:, diy_ohe])\n",
    "\n",
    "# check this again, should be no more 97s, but nothing else weird\n",
    "value_list_2 = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list_2 += a\n",
    "value_list_2 = list(set(value_list_2))\n",
    "sorted(value_list_2)\n",
    "\n",
    "[x for x in value_list if x not in value_list_2]  # 97\n",
    "[x for x in value_list_2 if x not in value_list]  # 0\n",
    "\n",
    "# OH THANK GOODNESS.  FINALLY!\n",
    "\n",
    "# check this again because I'm losing my mind\n",
    "meyer.shape\n",
    "\n",
    "# ok!  on we go!\n",
    "meyer[diy_ohe_new] = meyer[diy_ohe_new].fillna(0)\n",
    "\n",
    "# should be short now!\n",
    "value_list_3 = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list_3 += a\n",
    "value_list_3 = list(set(value_list_3))\n",
    "sorted(value_list_3)\n",
    "\n",
    "# AHAAHAHAAAAA I'M SO EXCITED I'M HYPERVENTILATING!!!!!!!\n",
    "meyer.loc[:, diy_ohe_new] = np.where(meyer.loc[:, diy_ohe_new]!=1, 0, meyer.loc[:, diy_ohe_new])\n",
    "\n",
    "# one more time???\n",
    "value_list_4 = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list_4 += a\n",
    "value_list_4 = list(set(value_list_4))\n",
    "sorted(value_list_4)\n",
    "\n",
    "# YAAAAAAAAAAASSSSSSSSSSSS!!!!!\n",
    "\n",
    "meyer.drop(columns = diy_ohe, inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "# ok, I actually probably should have done this before dropping those columns\n",
    "# but whatever, I can check it against the documentation\n",
    "for i in diy_ohe_new:\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('='*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 2 in full:\n",
    "2024-06-04_imputation_dealing-with-7s-and-97s-indiy-ohe-columns.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuesday, June 4, 2024\n",
    "# Imputation, Take 3\n",
    "\n",
    "# This script is a re-write of the other, earlier \n",
    "# script from today.  I tried a bunch of different\n",
    "# ways to deal with the 97s, and it took a while\n",
    "# to get one to work.\n",
    "\n",
    "meyer.shape # (1507, 267)\n",
    "\n",
    "# drop some columns\n",
    "drop_cols_3 = [\"w1q20_t_verb\", \"w1q39_7\", \"w1q39_12\", \n",
    "  \"w1q39_t_verb\", \"w1q39_6\", \"w1q39_5\", \"w1q39_2\", \"w1q39_3\", \n",
    "  \"w1q39_4\", \"w1q39_1\", \"w1q39_9\", \"w1q39_8\", \"w1q39_10\", \n",
    "  \"w1q39_11\", \"gp1\", \"gemployment2010\", \"gmilesaway\"]\n",
    "\n",
    "len(drop_cols_3)\n",
    "\n",
    "# Ok I started with 267 and dropped 17. Should now be 250.\n",
    "meyer.drop(columns = drop_cols_3).shape # Yep, (1507, 250)!\n",
    "meyer.drop(columns = drop_cols_3, inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "diy_ohe = [\"w1q145_3\", \"w1q163_3\", \"w1q136_3\", \"w1q145_9\", \n",
    "  \"w1q143_3\", \"w1q136_10\", \"w1q145_10\", \"w1q136_9\", \"w1q163_10\", \n",
    "  \"w1q163_9\", \"w1q143_9\", \"w1q143_4\", \"w1q136_6\", \"w1q143_10\", \n",
    "  \"w1q143_5\", \"w1q145_4\", \"w1q136_5\", \"w1q143_8\", \"w1q163_5\", \n",
    "  \"w1q136_4\", \"w1q163_6\", \"w1q145_6\", \"w1q136_1\", \"w1q143_7\", \n",
    "  \"w1q163_1\", \"w1q143_2\", \"w1q143_1\", \"w1q145_5\", \"w1q143_6\", \n",
    "  \"w1q163_2\", \"w1q163_4\", \"w1q136_8\", \"w1q145_8\", \"w1q145_1\", \n",
    "  \"w1q145_7\", \"w1q163_7\", \"w1q136_2\", \"w1q145_2\", \"w1q139_3\", \n",
    "  \"w1q136_7\", \"w1q139_9\", \"w1q139_10\", \"w1q139_5\", \"w1q139_4\", \n",
    "  \"w1q139_8\", \"w1q139_6\", \"w1q139_2\", \"w1q139_1\", \"w1q139_7\", \n",
    "  \"w1q163_8\", \"w1q141_3\", \"w1q141_9\", \"w1q141_10\", \"w1q141_8\", \n",
    "  \"w1q141_4\", \"w1q141_1\", \"w1q141_5\", \"w1q141_2\", \"w1q141_7\", \n",
    "  \"w1q141_6\", \"w1q64_12\", \"w1q64_5\", \"w1q74_5\", \"w1q74_6\", \n",
    "  \"w1q74_20\", \"w1q64_11\", \"w1q64_10\", \"w1q74_18\", \"w1q74_14\", \n",
    "  \"w1q74_17\", \"w1q171_8\", \"w1q30_3\", \"w1q64_13\", \"w1q64_7\", \n",
    "  \"w1q30_4\", \"w1q171_6\", \"w1q171_4\", \"w1q64_8\", \"w1q74_10\", \n",
    "  \"w1q74_21\", \"w1q64_6\", \"w1q74_11\", \"w1q171_5\", \"w1q64_3\", \n",
    "  \"w1q64_1\", \"w1q171_9\", \"w1q30_5\", \"w1q171_3\", \"w1q74_22\", \n",
    "  \"w1q64_9\", \"w1q171_2\", \"w1q171_7\", \"w1q74_23\", \"w1q64_4\", \n",
    "  \"w1q64_2\", \"w1q30_1\", \"w1q171_1\", \"w1q30_2\"]\n",
    "\n",
    "value_list = []\n",
    "for i in diy_ohe:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list += a\n",
    "value_list = list(set(value_list))\n",
    "sorted(value_list)\n",
    "\n",
    "# create some new column names\n",
    "diy_ohe_new = [''.join([x, '_ei']) for x in diy_ohe]\n",
    "\n",
    "# let's see if this replaces the 97s right\n",
    "meyer.loc[:, diy_ohe_new] = np.where(meyer.loc[:, diy_ohe]==97, 0, meyer.loc[:, diy_ohe])\n",
    "\n",
    "# check this\n",
    "for i, j in list(zip(diy_ohe, diy_ohe_new))[50:]:\n",
    "  print(i)\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('-'*20)\n",
    "  print(j)\n",
    "  meyer[j].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "  print('')\n",
    "  print('='*20)\n",
    "\n",
    "# I'm looking for valid 7s.\n",
    "# It seems like 7 was also used as an NA code\n",
    "# because why be consistent lolololol!!!!!!!!\n",
    "\n",
    "nat_7s = ['w1q143_7', 'w1q145_7', 'w1q163_7', 'w1q136_7', \n",
    "  'w1q139_7', 'w1q141_7', 'w1q64_7', 'w1q171_7']\n",
    "  \n",
    "nat_7s == [c for c in diy_ohe if c.split('_')[-1]=='7']\n",
    "# True.\n",
    "\n",
    "nat_7s_ei = [''.join([x, '_ei']) for x in nat_7s]\n",
    "\n",
    "# let's turn those 7s into 1s, and then the rest into 0s\n",
    "meyer.loc[:, nat_7s_ei] = np.where(meyer.loc[:, nat_7s]==7, 1, meyer.loc[:, nat_7s])\n",
    "\n",
    "# check this\n",
    "for i in nat_7s_ei:\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "\n",
    "# check one against the documentation\n",
    "meyer['w1q141_7'].value_counts(dropna = False)\n",
    "\n",
    "# check this again, should be no more 97s, but nothing else weird\n",
    "remaining_7s = []\n",
    "value_list_2 = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  if 7 in a:\n",
    "    remaining_7s.append(i)\n",
    "  value_list_2 += a\n",
    "value_list_2 = list(set(value_list_2))\n",
    "sorted(value_list_2)\n",
    "\n",
    "remaining_7s_qs = [x.split('_')[0] for x in remaining_7s]\n",
    "sorted(set(remaining_7s_qs))\n",
    "# I went through VERY CAREFULLY and confirmed that all the \n",
    "# remaining 7s are \"planned missings.\"\n",
    "\n",
    "# let's see if this replaces the 97s right\n",
    "meyer.loc[:, diy_ohe_new] = np.where(meyer.loc[:, diy_ohe_new]==7, 0, meyer.loc[:, diy_ohe_new])\n",
    "\n",
    "# check this\n",
    "for i, j in list(zip(diy_ohe, diy_ohe_new))[:50]:\n",
    "  print(i)\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('-'*20)\n",
    "  print(j)\n",
    "  meyer[j].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "  print('')\n",
    "  print('='*20)\n",
    "\n",
    "meyer[diy_ohe_new] = meyer[diy_ohe_new].fillna(0)\n",
    "\n",
    "# check again!\n",
    "for i, j in list(zip(diy_ohe, diy_ohe_new))[:50]:\n",
    "  print(i)\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('-'*20)\n",
    "  print(j)\n",
    "  meyer[j].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "  print('')\n",
    "  print('='*20)\n",
    "\n",
    "# wait, some of them still have 97s??\n",
    "\n",
    "# let's see if this replaces the 97s right\n",
    "meyer.loc[:, diy_ohe_new] = np.where(meyer.loc[:, diy_ohe_new]==97.0, 0, meyer.loc[:, diy_ohe_new])\n",
    "\n",
    "for i, j in list(zip(diy_ohe, diy_ohe_new))[:50]:\n",
    "  print(i)\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('-'*20)\n",
    "  print(j)\n",
    "  meyer[j].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "  print('')\n",
    "  print('='*20)\n",
    "  \n",
    "# this again too\n",
    "remaining_7s_2 = []\n",
    "remaining_97s = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  if 7 in a:\n",
    "    remaining_7s_2.append(i)\n",
    "  if 97 in a:\n",
    "    remaining_97s.append(i)\n",
    "sorted(set(remaining_7s_2))\n",
    "sorted(set(remaining_97s))\n",
    "\n",
    "# check AGAIN\n",
    "value_list_x = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list_x += a\n",
    "value_list_x = list(set(value_list_x))\n",
    "sorted(value_list_x)\n",
    "\n",
    "# make a backup\n",
    "os.getcwd()\n",
    "meyer.to_csv('meyer_backup_diy_ohe_no_97s_7s.csv', index = False)\n",
    "\n",
    "\n",
    "# reduce to 0s and 1s\n",
    "meyer.loc[:, diy_ohe_new] = np.where(meyer.loc[:, diy_ohe_new]!=0, 1, meyer.loc[:, diy_ohe_new])\n",
    "\n",
    "for i, j in list(zip(diy_ohe, diy_ohe_new))[:50]:\n",
    "  print(i)\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('-'*20)\n",
    "  print(j)\n",
    "  meyer[j].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "  print('')\n",
    "  print('='*20)\n",
    "\n",
    "# no NAs?\n",
    "meyer[diy_ohe_new].isna().sum().sum()\n",
    "# no NAs!\n",
    "\n",
    "# drop the old versions\n",
    "meyer.drop(columns = diy_ohe, inplace = True)\n",
    "\n",
    "# check the dtypes\n",
    "pd.set_option('display.max_rows', None)\n",
    "meyer.dtypes\n",
    "\n",
    "# should be VERY short now!\n",
    "value_list_3 = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list_3 += a\n",
    "value_list_3 = list(set(value_list_3))\n",
    "sorted(value_list_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 3 in full:\n",
    "2024-06-04_imputation_misc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuesday, June 4, 2024\n",
    "# More Imputation!\n",
    "\n",
    "# w1q64_t_verb\n",
    "meyer['w1q64_t_verb'].value_counts(dropna = False)\n",
    "meyer.shape # 250 columns - gonna add one then delete the original\n",
    "# meyer['w1q64_t_num'] = meyer['w1q64_t_verb'].fillna('0')\n",
    "# I got a weird warning here ^, so for future runs V\n",
    "meyer[['w1q64_t_num']] = meyer[['w1q64_t_verb']].fillna('0')\n",
    "# for right now V\n",
    "meyer_c = meyer.copy(deep = True)\n",
    "meyer = meyer_c.copy(deep = True)\n",
    "meyer.shape\n",
    "# thanks to this SO article for advice\n",
    "# https://stackoverflow.com/questions/68292862/performancewarning-dataframe-is-highly-fragmented-this-is-usually-the-result-o\n",
    "\n",
    "# reduce to 0s and 1s\n",
    "meyer.loc[:, 'w1q64_t_num'] = np.where(meyer.loc[:, 'w1q64_t_num']!='0', 1, 0)\n",
    "\n",
    "# check\n",
    "meyer['w1q64_t_num'].value_counts(dropna = False)\n",
    "meyer['w1q64_t_verb'].value_counts(dropna = False)\n",
    "\n",
    "# drop the original\n",
    "meyer.drop(columns = ['w1q64_t_verb'], inplace = True)\n",
    "\n",
    "\n",
    "# gmilesaway2\n",
    "meyer['gmilesaway2'].value_counts(dropna = False)\n",
    "\n",
    "meyer[['gmilesaway2_ei']] = meyer[['gmilesaway2']].fillna(0)\n",
    "meyer['gmilesaway2'].value_counts(dropna = False)\n",
    "meyer['gmilesaway2_ei'].value_counts(dropna = False)\n",
    "\n",
    "# reverse code so 1=close\n",
    "meyer[['gmilesaway2_ei_r']] = 1-meyer[['gmilesaway2_ei']]\n",
    "meyer['gmilesaway2'].value_counts(dropna = False)\n",
    "meyer['gmilesaway2_ei'].value_counts(dropna = False)\n",
    "meyer['gmilesaway2_ei_r'].value_counts(dropna = False)\n",
    "\n",
    "# drop\n",
    "meyer.drop(columns = ['gmilesaway2', 'gmilesaway2_ei'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "\n",
    "# w1q123d & w1q123c\n",
    "# These guys had some missing values, but also had an option (5)\n",
    "# for \"don't know/doesn't apply.\"  I don't know what the truth\n",
    "# is for these missing values, so I'm recoding them to \"don't know.\"\n",
    "\n",
    "# What to do with the 5s \n",
    "# will be a question for the next round of cleaning.\n",
    "\n",
    "meyer[['w1q123d_ei', 'w1q123c_ei']] = meyer[['w1q123d', 'w1q123c']].fillna(5)\n",
    "meyer[['w1q123d_ei', 'w1q123c_ei', 'w1q123d', 'w1q123c']]\n",
    "\n",
    "meyer.drop(columns = ['w1q123d', 'w1q123c'], inplace = True)\n",
    "\n",
    "\n",
    "# w1q179\n",
    "# impute as \"nothing in particular\", seems logical\n",
    "meyer[['w1q179_ei']] = meyer[['w1q179']].fillna(13)\n",
    "meyer[['w1q179', 'w1q179_ei']]\n",
    "\n",
    "meyer.drop(columns = 'w1q179', inplace = True)\n",
    "\n",
    "\n",
    "# all right!  real imputer time!!\n",
    "\n",
    "# I'm going to go ahead and do this on the full dataset, \n",
    "# because screw it.\n",
    "# If I have time, I can go back and re-run it with\n",
    "# a train test split.  But for now I'm just going to do it\n",
    "# all at once, with the justification that for an \n",
    "# inferential model, the TTS isn't that important.\n",
    "# This data was gathered at a specific moment in history,\n",
    "# and it covers some sensitive topics.  There is no \n",
    "# reason to design this process to accommodate for novel\n",
    "# data extending into the future, because there is no\n",
    "# reason to assume that these values wouldn't change\n",
    "# over time.  (Which does kind of invalidate everything\n",
    "# I'm doing on this 2016 dataset but SHHHHHHHHH.)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "si_med = SimpleImputer(strategy = 'median')\n",
    "si_mode = SimpleImputer(strategy = 'most_frequent')\n",
    "\n",
    "s_median = '''\n",
    "w1q50\n",
    "w1q48\n",
    "w1q51\n",
    "w1q112\n",
    "w1q49\n",
    "w1q47\n",
    "w1q46\n",
    "w1q45\n",
    "w1q146c\n",
    "w1q162\n",
    "w1q181\n",
    "w1q146f\n",
    "w1q146g\n",
    "w1q146e'''\n",
    "\n",
    "s_mode = '''\n",
    "w1q175\n",
    "w1q72\n",
    "w1q142d\n",
    "w1q65\n",
    "w1q52\n",
    "w1q142i\n",
    "w1q142g\n",
    "w1q180\n",
    "w1q69\n",
    "w1q135d\n",
    "w1q142h\n",
    "w1q135b\n",
    "w1q135c\n",
    "w1q135e\n",
    "w1q142b\n",
    "w1q142e\n",
    "w1q142f\n",
    "w1q142j\n",
    "w1q142a\n",
    "w1q142k\n",
    "w1q19c\n",
    "w1q19d\n",
    "w1q19b\n",
    "w1q19a\n",
    "w1q75\n",
    "w1q78\n",
    "w1q79\n",
    "w1q76'''\n",
    "\n",
    "print(s_median.replace('\\n', '\", \"'))\n",
    "print(s_mode.replace('\\n', '\", \"'))\n",
    "\n",
    "s_median = [\"w1q50\", \"w1q48\", \"w1q51\", \"w1q112\", \n",
    "  \"w1q49\", \"w1q47\", \"w1q46\", \"w1q45\", \"w1q146c\", \n",
    "  \"w1q162\", \"w1q181\", \"w1q146f\", \"w1q146g\", \"w1q146e\"]\n",
    "  \n",
    "s_mode = [\"w1q175\", \"w1q72\", \"w1q142d\", \"w1q65\", \"w1q52\", \n",
    "  \"w1q142i\", \"w1q142g\", \"w1q180\", \"w1q69\", \"w1q135d\", \n",
    "  \"w1q142h\", \"w1q135b\", \"w1q135c\", \"w1q135e\", \"w1q142b\", \n",
    "  \"w1q142e\", \"w1q142f\", \"w1q142j\", \"w1q142a\", \"w1q142k\", \n",
    "  \"w1q19c\", \"w1q19d\", \"w1q19b\", \"w1q19a\", \"w1q75\", \n",
    "  \"w1q78\", \"w1q79\", \"w1q76\"]\n",
    "\n",
    "s_median_new = [''.join([x, '_ei']) for x in s_median]\n",
    "s_mode_new = [''.join([x, '_ei']) for x in s_mode]\n",
    "\n",
    "# # Safety first\n",
    "# meyer.to_csv('meyer_much_imputation_done_but_not_si_yet.csv', index = False)\n",
    "# \n",
    "# # I think the cleanest way to do this without it breaking everything is to\n",
    "# # break the dataframe apart, use the 2 SIs separately, then re-merge.\n",
    "# # Let me check that the studyID will be suitable for merging on.\n",
    "# \n",
    "# meyer.shape # 1507 rows (250 columns)\n",
    "# meyer['studyid'].nunique() # 1507 unique values\n",
    "# \n",
    "# # split the df up for imputing\n",
    "# meyer_median = meyer[s_median+['studyid']]\n",
    "# meyer_mode = meyer[s_mode+['studyid']]\n",
    "# \n",
    "# # new column names\n",
    "# s_median_new.append('studyid')\n",
    "# s_mode_new.append('studyid')\n",
    "# meyer_median.columns = s_median_new\n",
    "# meyer_mode.columns = s_mode_new\n",
    "# \n",
    "# # do the imputation!\n",
    "# meyer_median = pd.DataFrame(\n",
    "#   si_med.fit_transform(meyer_median), columns = si_med.get_feature_names_out())\n",
    "# \n",
    "# meyer_mode = pd.DataFrame(\n",
    "#   si_mode.fit_transform(meyer_mode), columns = si_mode.get_feature_names_out())\n",
    "# \n",
    "# # is it really that easy?\n",
    "# meyer_median.isna().sum().sum()\n",
    "# meyer_mode.isna().sum().sum()\n",
    "# \n",
    "# mme = meyer_median.describe()\n",
    "# mmo = meyer_mode.describe()\n",
    "# \n",
    "# # No!  No, it is not!!\n",
    "# # The stupid 97s, 98s, and 99s are at it again!\n",
    "\n",
    "s = '''\n",
    "w1q119\n",
    "w1q109\n",
    "w1q105\n",
    "w1q113\n",
    "w1q101'''\n",
    "\n",
    "print(s.replace('\\n', '\", \"'))\n",
    "\n",
    "s = [\"w1q119\", \"w1q109\", \"w1q105\", \"w1q113\", \"w1q101\"]\n",
    "\n",
    "s_ei = [''.join([x, '_ei']) for x in s]\n",
    "\n",
    "meyer[s_ei] = meyer[s].fillna(0)\n",
    "\n",
    "for i, j in zip(s, s_ei):\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('-'*20)\n",
    "  meyer[j].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "  print('')\n",
    "\n",
    "meyer.drop(columns = s, inplace = True)\n",
    "\n",
    "# RIGHT NOW THESE ARE 0S\n",
    "# ONLY 1-3 IS DEFINED IN THE SCALE \n",
    "# (these are the suicide and nssh Qs)\n",
    "# TOMORROW, plot it, and see how the 0 column\n",
    "# compares to the others\n",
    "# and reassign accordingly\n",
    "\n",
    "\n",
    "# w1q32\n",
    "# right or wrong these guys were TREATED as \"no\" in the survey\n",
    "# so I'm going to code them that way.\n",
    "\n",
    "meyer['w1q32'].value_counts(dropna = False)\n",
    "meyer['w1q32'].fillna(2).value_counts(dropna = False)\n",
    "\n",
    "meyer[['w1q32_ei']] = meyer[['w1q32']].fillna(2)\n",
    "\n",
    "meyer[['w1q32_ei']].value_counts(dropna = False)\n",
    "\n",
    "meyer.drop(columns = ['w1q32_ei'], inplace = True)\n",
    "\n",
    "meyer.shape\n",
    "list(meyer.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 4 in full:\n",
    "2024-06-04_simple_imputer_stuff.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written Tuesday, June 4, 2024\n",
    "# FOR Wednesday, June 5, 2024\n",
    "\n",
    "# Much of this was copied from impt_misc\n",
    "\n",
    "# The simple imputer stuff\n",
    "\n",
    "s_median = [\"w1q50\", \"w1q48\", \"w1q51\", \"w1q112\", \n",
    "  \"w1q49\", \"w1q47\", \"w1q46\", \"w1q45\", \"w1q146c\", \n",
    "  \"w1q162\", \"w1q181\", \"w1q146f\", \"w1q146g\", \"w1q146e\"]\n",
    "  \n",
    "s_mode = [\"w1q175\", \"w1q72\", \"w1q142d\", \"w1q65\", \"w1q52\", \n",
    "  \"w1q142i\", \"w1q142g\", \"w1q180\", \"w1q69\", \"w1q135d\", \n",
    "  \"w1q142h\", \"w1q135b\", \"w1q135c\", \"w1q135e\", \"w1q142b\", \n",
    "  \"w1q142e\", \"w1q142f\", \"w1q142j\", \"w1q142a\", \"w1q142k\", \n",
    "  \"w1q19c\", \"w1q19d\", \"w1q19b\", \"w1q19a\", \"w1q75\", \n",
    "  \"w1q78\", \"w1q79\", \"w1q76\"]\n",
    "\n",
    "s_median_new = [''.join([x, '_ei']) for x in s_median]\n",
    "s_mode_new = [''.join([x, '_ei']) for x in s_mode]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Right here I need to figure out how to deal with the 97s-99s.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Safety first\n",
    "# meyer.to_csv('meyer_much_imputation_done_but_not_si_yet.csv', index = False)\n",
    "\n",
    "# I think the cleanest way to do this without it breaking everything is to\n",
    "# break the dataframe apart, use the 2 SIs separately, then re-merge.\n",
    "# Let me check that the studyID will be suitable for merging on.\n",
    "\n",
    "meyer.shape # 1507 rows (250 columns)\n",
    "meyer['studyid'].nunique() # 1507 unique values\n",
    "\n",
    "# split the df up for imputing\n",
    "meyer_median = meyer[s_median+['studyid']]\n",
    "meyer_mode = meyer[s_mode+['studyid']]\n",
    "\n",
    "# new column names\n",
    "s_median_new.append('studyid')\n",
    "s_mode_new.append('studyid')\n",
    "meyer_median.columns = s_median_new\n",
    "meyer_mode.columns = s_mode_new\n",
    "\n",
    "# do the imputation!\n",
    "meyer_median = pd.DataFrame(\n",
    "  si_med.fit_transform(meyer_median), columns = si_med.get_feature_names_out())\n",
    "\n",
    "meyer_mode = pd.DataFrame(\n",
    "  si_mode.fit_transform(meyer_mode), columns = si_mode.get_feature_names_out())\n",
    "\n",
    "# is it really that easy?\n",
    "meyer_median.isna().sum().sum()\n",
    "meyer_mode.isna().sum().sum()\n",
    "\n",
    "mme = meyer_median.describe()\n",
    "mmo = meyer_mode.describe()\n",
    "\n",
    "# No!  No, it is not!!\n",
    "# The stupid 97s, 98s, and 99s are at it again!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSVs are in the main project folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wednesday, June 5, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "I finally finished imputing my null values!  \n",
    "\n",
    "I think when this is all over, I should reload a clean, fresh copy of the dataframe and see\n",
    "- how many NAs there are per ROW -- I wonder if there are a handful of people who skipped a lot of questions and I've ended up imputing basically their whole row\n",
    "- how many cells I've altered.  it all seems so reasonable when I'm going one column at a time, but when I step back and watch every single column turn into an _ei column, it makes me wonder if I'm destroying my dataset.  If the final tally is staggering, then I should probably go with Hank's suggestion of only picking an handful of columns and ignoring the rest.\n",
    "\n",
    "In any case, today I finished doing the imputation as laid out in imputation_plan.xlsx.  Or, mostly.  I changed my mind on a few things as I went, and discovered things along the way.  Most memorably, some column that I meant to impute with the median populated value ended up just being 0s, because once I converted the 97s to 0s, that was the median.  I'm sure there's a way to tell SI to impute only from values in a range, but 0 was faster.  I also cleaned up a bunch more of those 97s et al., and did some subset investigations.  On some poverty and employment measures there were probably more columns that I could have used to predict the missing values, but using measures of central tendency or just the columns that came most readily to mind was fastest, and will have to do for this iteration.\n",
    "\n",
    "Once all the imputation of truly *missing* values was done, I looped back around to look for any of those 7/97/98/99 values that *should* be NAs but aren't.  I did this by looking through every W1 listing in the 831 page documentation, and making a list of which variables have defined values like that, and what they are.  This list is called `problem_children`, and is a list of tuples with structure `(column_name, [list of relevant value(s)])`.\n",
    "\n",
    "I heard back from the people at ICPSR, and they said no to posting my dataset on github.  They are fine with me posting a link though, and that should suffice.\n",
    "\n",
    "**I created new CSVs at various points throughout my process, so starting tomorrow I can load from one of those directly.  The most up to date one is `2024-06-05_all-imputation-done.csv`.**\n",
    "\n",
    "\n",
    "files created: \n",
    "- 2024-06-05_finishing-imputation.py\n",
    "- 2024-06-05_making-a-list-of-potential-7s-98s-etc-to-check.py\n",
    "- MAJOR modifications: 2024-06-04_simple_imputer_stuff.py\n",
    "- MINOR modifications:\n",
    "modified:   2024-06-04_imputation_misc.py  (this is the only one that warrants re-posting)\n",
    "modified:   ../../01_notebooks/notes_plus_all_code.ipynb\n",
    "modified:   ../01_notes/imputation_plan.xlsx\n",
    "modified:   2024-05-31_continuing_messing_with_meyer_columns.py\n",
    "modified:   2024-06-04_imputation_dealing-with-7s-and-97s-indiy-ohe-columns.py\n",
    "\n",
    "- all of the following CSVs:\n",
    "2024-06-05_all-imputation-done.csv\n",
    "2024-06-05_all-imputation-done_reordered.csv\n",
    "2024-06-05_all-imputation-except-poverty-done.csv\n",
    "2024-06-05_meyer_backup_diy_ohe_no_97s_7s.csv\n",
    "2024-06-05_meyer_much_imputation_done_but_not_si_yet.csv\n",
    "2024-06-05_most-imputation-done_midway-through-nativity.csv\n",
    "2024-06-05_pre-si-remerging.csv\n",
    "2024-06-05_si-imputation-done_remerged.csv\n",
    "2024-06-05_values-imputed-with-median.csv\n",
    "2024-06-05_values-imputed-with-mode.csv\n",
    "mid_2024-06-05_remaining_nas.csv  (the df under this csv was created in a script, but the .to_csv itself was only in the console)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 1 in full:\n",
    "2024-06-04_imputation_misc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuesday, June 4, 2024\n",
    "# More Imputation!\n",
    "\n",
    "# w1q64_t_verb\n",
    "meyer['w1q64_t_verb'].value_counts(dropna = False)\n",
    "meyer.shape # 250 columns - gonna add one then delete the original\n",
    "# meyer['w1q64_t_num'] = meyer['w1q64_t_verb'].fillna('0')\n",
    "# I got a weird warning here ^, so for future runs V\n",
    "meyer[['w1q64_t_num']] = meyer[['w1q64_t_verb']].fillna('0')\n",
    "# for right now V\n",
    "meyer_c = meyer.copy(deep = True)\n",
    "meyer = meyer_c.copy(deep = True)\n",
    "meyer.shape\n",
    "# thanks to this SO article for advice\n",
    "# https://stackoverflow.com/questions/68292862/performancewarning-dataframe-is-highly-fragmented-this-is-usually-the-result-o\n",
    "\n",
    "# reduce to 0s and 1s\n",
    "meyer.loc[:, 'w1q64_t_num'] = np.where(meyer.loc[:, 'w1q64_t_num']!='0', 1, 0)\n",
    "\n",
    "# check\n",
    "meyer['w1q64_t_num'].value_counts(dropna = False)\n",
    "meyer['w1q64_t_verb'].value_counts(dropna = False)\n",
    "\n",
    "# drop the original\n",
    "meyer.drop(columns = ['w1q64_t_verb'], inplace = True)\n",
    "\n",
    "\n",
    "# gmilesaway2\n",
    "meyer['gmilesaway2'].value_counts(dropna = False)\n",
    "\n",
    "meyer[['gmilesaway2_ei']] = meyer[['gmilesaway2']].fillna(0)\n",
    "meyer['gmilesaway2'].value_counts(dropna = False)\n",
    "meyer['gmilesaway2_ei'].value_counts(dropna = False)\n",
    "\n",
    "# reverse code so 1=close\n",
    "meyer[['gmilesaway2_ei_r']] = 1-meyer[['gmilesaway2_ei']]\n",
    "meyer['gmilesaway2'].value_counts(dropna = False)\n",
    "meyer['gmilesaway2_ei'].value_counts(dropna = False)\n",
    "meyer['gmilesaway2_ei_r'].value_counts(dropna = False)\n",
    "\n",
    "# drop\n",
    "meyer.drop(columns = ['gmilesaway2', 'gmilesaway2_ei'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "\n",
    "# w1q123d & w1q123c\n",
    "# These guys had some missing values, but also had an option (5)\n",
    "# for \"don't know/doesn't apply.\"  I don't know what the truth\n",
    "# is for these missing values, so I'm recoding them to \"don't know.\"\n",
    "\n",
    "# What to do with the 5s \n",
    "# will be a question for the next round of cleaning.\n",
    "\n",
    "meyer[['w1q123d_ei', 'w1q123c_ei']] = meyer[['w1q123d', 'w1q123c']].fillna(5)\n",
    "meyer[['w1q123d_ei', 'w1q123c_ei', 'w1q123d', 'w1q123c']]\n",
    "\n",
    "meyer.drop(columns = ['w1q123d', 'w1q123c'], inplace = True)\n",
    "\n",
    "\n",
    "# w1q179\n",
    "# impute as \"nothing in particular\", seems logical\n",
    "meyer[['w1q179_ei']] = meyer[['w1q179']].fillna(13)\n",
    "meyer[['w1q179', 'w1q179_ei']]\n",
    "\n",
    "meyer.drop(columns = 'w1q179', inplace = True)\n",
    "\n",
    "\n",
    "# all right!  real imputer time!!\n",
    "\n",
    "# I'm going to go ahead and do this on the full dataset, \n",
    "# because screw it.\n",
    "# If I have time, I can go back and re-run it with\n",
    "# a train test split.  But for now I'm just going to do it\n",
    "# all at once, with the justification that for an \n",
    "# inferential model, the TTS isn't that important.\n",
    "# This data was gathered at a specific moment in history,\n",
    "# and it covers some sensitive topics.  There is no \n",
    "# reason to design this process to accommodate for novel\n",
    "# data extending into the future, because there is no\n",
    "# reason to assume that these values wouldn't change\n",
    "# over time.  (Which does kind of invalidate everything\n",
    "# I'm doing on this 2016 dataset but SHHHHHHHHH.)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "si_med = SimpleImputer(strategy = 'median')\n",
    "si_mode = SimpleImputer(strategy = 'most_frequent')\n",
    "\n",
    "s_median = '''\n",
    "w1q50\n",
    "w1q48\n",
    "w1q51\n",
    "w1q112\n",
    "w1q49\n",
    "w1q47\n",
    "w1q46\n",
    "w1q45\n",
    "w1q146c\n",
    "w1q162\n",
    "w1q181\n",
    "w1q146f\n",
    "w1q146g\n",
    "w1q146e'''\n",
    "\n",
    "s_mode = '''\n",
    "w1q175\n",
    "w1q72\n",
    "w1q142d\n",
    "w1q65\n",
    "w1q52\n",
    "w1q142i\n",
    "w1q142g\n",
    "w1q180\n",
    "w1q69\n",
    "w1q135d\n",
    "w1q142h\n",
    "w1q135b\n",
    "w1q135c\n",
    "w1q135e\n",
    "w1q142b\n",
    "w1q142e\n",
    "w1q142f\n",
    "w1q142j\n",
    "w1q142a\n",
    "w1q142k\n",
    "w1q19c\n",
    "w1q19d\n",
    "w1q19b\n",
    "w1q19a\n",
    "w1q75\n",
    "w1q78\n",
    "w1q79\n",
    "w1q76'''\n",
    "\n",
    "print(s_median.replace('\\n', '\", \"'))\n",
    "print(s_mode.replace('\\n', '\", \"'))\n",
    "\n",
    "s_median = [\"w1q50\", \"w1q48\", \"w1q51\", \"w1q112\", \n",
    "  \"w1q49\", \"w1q47\", \"w1q46\", \"w1q45\", \"w1q146c\", \n",
    "  \"w1q162\", \"w1q181\", \"w1q146f\", \"w1q146g\", \"w1q146e\"]\n",
    "  \n",
    "s_mode = [\"w1q175\", \"w1q72\", \"w1q142d\", \"w1q65\", \"w1q52\", \n",
    "  \"w1q142i\", \"w1q142g\", \"w1q180\", \"w1q69\", \"w1q135d\", \n",
    "  \"w1q142h\", \"w1q135b\", \"w1q135c\", \"w1q135e\", \"w1q142b\", \n",
    "  \"w1q142e\", \"w1q142f\", \"w1q142j\", \"w1q142a\", \"w1q142k\", \n",
    "  \"w1q19c\", \"w1q19d\", \"w1q19b\", \"w1q19a\", \"w1q75\", \n",
    "  \"w1q78\", \"w1q79\", \"w1q76\"]\n",
    "\n",
    "s_median_new = [''.join([x, '_ei']) for x in s_median]\n",
    "s_mode_new = [''.join([x, '_ei']) for x in s_mode]\n",
    "\n",
    "# # Safety first\n",
    "# meyer.to_csv('meyer_much_imputation_done_but_not_si_yet.csv', index = False)\n",
    "# \n",
    "# # I think the cleanest way to do this without it breaking everything is to\n",
    "# # break the dataframe apart, use the 2 SIs separately, then re-merge.\n",
    "# # Let me check that the studyID will be suitable for merging on.\n",
    "# \n",
    "# meyer.shape # 1507 rows (250 columns)\n",
    "# meyer['studyid'].nunique() # 1507 unique values\n",
    "# \n",
    "# # split the df up for imputing\n",
    "# meyer_median = meyer[s_median+['studyid']]\n",
    "# meyer_mode = meyer[s_mode+['studyid']]\n",
    "# \n",
    "# # new column names\n",
    "# s_median_new.append('studyid')\n",
    "# s_mode_new.append('studyid')\n",
    "# meyer_median.columns = s_median_new\n",
    "# meyer_mode.columns = s_mode_new\n",
    "# \n",
    "# # do the imputation!\n",
    "# meyer_median = pd.DataFrame(\n",
    "#   si_med.fit_transform(meyer_median), columns = si_med.get_feature_names_out())\n",
    "# \n",
    "# meyer_mode = pd.DataFrame(\n",
    "#   si_mode.fit_transform(meyer_mode), columns = si_mode.get_feature_names_out())\n",
    "# \n",
    "# # is it really that easy?\n",
    "# meyer_median.isna().sum().sum()\n",
    "# meyer_mode.isna().sum().sum()\n",
    "# \n",
    "# mme = meyer_median.describe()\n",
    "# mmo = meyer_mode.describe()\n",
    "# \n",
    "# # No!  No, it is not!!\n",
    "# # The stupid 97s, 98s, and 99s are at it again!\n",
    "\n",
    "\n",
    "# Suicide and NSSH Qs\n",
    "\n",
    "s = '''\n",
    "w1q119\n",
    "w1q109\n",
    "w1q105\n",
    "w1q113\n",
    "w1q101'''\n",
    "\n",
    "print(s.replace('\\n', '\", \"'))\n",
    "\n",
    "s = [\"w1q119\", \"w1q109\", \"w1q105\", \"w1q113\", \"w1q101\"]\n",
    "\n",
    "s_ei = [''.join([x, '_ei']) for x in s]\n",
    "\n",
    "meyer[s_ei] = meyer[s].fillna(0)\n",
    "\n",
    "for i, j in zip(s, s_ei):\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('-'*20)\n",
    "  meyer[j].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "  print('')\n",
    "\n",
    "meyer.drop(columns = s, inplace = True)\n",
    "\n",
    "# RIGHT NOW THESE ARE 0S\n",
    "# ONLY 1-3 IS DEFINED IN THE SCALE \n",
    "# (these are the suicide and nssh Qs)\n",
    "# TOMORROW, plot it, and see how the 0 column\n",
    "# compares to the others\n",
    "# and reassign accordingly\n",
    "\n",
    "\n",
    "# w1q32\n",
    "# right or wrong these guys were TREATED as \"no\" in the survey\n",
    "# so I'm going to code them that way.\n",
    "\n",
    "meyer['w1q32'].value_counts(dropna = False)\n",
    "meyer['w1q32'].fillna(2).value_counts(dropna = False)\n",
    "\n",
    "meyer[['w1q32_ei']] = meyer[['w1q32']].fillna(2)\n",
    "\n",
    "meyer[['w1q32_ei']].value_counts(dropna = False)\n",
    "\n",
    "meyer.drop(columns = ['w1q32'], inplace = True)\n",
    "\n",
    "meyer.shape\n",
    "list(meyer.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 2 in full:\n",
    "2024-06-04_simple_imputer_stuff.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written Tuesday, June 4, 2024\n",
    "# FOR Wednesday, June 5, 2024\n",
    "\n",
    "# from the backup\n",
    "os.getcwd()\n",
    "meyer = pd.read_csv('2024-06-05_meyer_much_imputation_done_but_not_si_yet.csv',\n",
    "  sep = ',', low_memory=False, na_values = ' ')\n",
    "  \n",
    "# THERE WAS MORE STUFF ON THAT SCRIPT AFTER SAVING THE CSV\n",
    "\n",
    "# Much of this was copied from impt_misc\n",
    "\n",
    "# The simple imputer stuff\n",
    "\n",
    "s_median = [\"w1q50\", \"w1q48\", \"w1q51\", \"w1q112\", \n",
    "  \"w1q49\", \"w1q47\", \"w1q46\", \"w1q45\", \"w1q146c\", \n",
    "  \"w1q162\", \"w1q181\", \"w1q146f\", \"w1q146g\", \"w1q146e\", # original ^ added V\n",
    "  \"w1q103\", \"w1q104\", \"w1q106\", \"w1q107\", \"w1q108\", \n",
    "  \"w1q111\", \"w1q115\", \"w1q116\", \"w1q117\", \"w1q134\", \n",
    "  \"w1q146a\", \"w1q146h\", \"w1q146i\", \"w1q146j\", \"w1q146k\", \n",
    "  \"w1q146l\", \"w1q169\"]\n",
    "\n",
    "s_mode = [\"w1q175\", \"w1q72\", \"w1q142d\", \"w1q65\", \"w1q52\", \n",
    "  \"w1q142i\", \"w1q142g\", \"w1q180\", \"w1q69\", \"w1q135d\", \n",
    "  \"w1q142h\", \"w1q135b\", \"w1q135c\", \"w1q135e\", \"w1q142b\", \n",
    "  \"w1q142e\", \"w1q142f\", \"w1q142j\", \"w1q142a\", \"w1q142k\", \n",
    "  \"w1q19c\", \"w1q19d\", \"w1q19b\", \"w1q19a\", \"w1q75\", \n",
    "  \"w1q78\", \"w1q79\", \"w1q76\",                    # original ^ added V\n",
    "  \"w1q135a\", \"w1q135f\", \"w1q142c\", \"w1q35\", \"w1q36\", \n",
    "  \"w1q37\", \"w1q38\", \"w1q124\", \"w1q89\"]\n",
    "\n",
    "# agh I found more\n",
    "\n",
    "s_median_2 = '''\n",
    "w1q103\n",
    "w1q104\n",
    "w1q106\n",
    "w1q107\n",
    "w1q108\n",
    "w1q111\n",
    "w1q115\n",
    "w1q116\n",
    "w1q117\n",
    "w1q134\n",
    "w1q146a\n",
    "w1q146h\n",
    "w1q146i\n",
    "w1q146j\n",
    "w1q146k\n",
    "w1q146l\n",
    "w1q169'''\n",
    "\n",
    "s_mode_2 = '''\n",
    "w1q135a\n",
    "w1q135f\n",
    "w1q142c\n",
    "w1q35\n",
    "w1q36\n",
    "w1q37\n",
    "w1q38\n",
    "w1q124\n",
    "w1q89'''\n",
    "\n",
    "print(s_median_2.replace('\\n', '\", \"'))\n",
    "print(s_mode_2.replace('\\n', '\", \"'))\n",
    "\n",
    "s_97s = []\n",
    "s_98s = []\n",
    "s_99s = []\n",
    "for i in s_median:\n",
    "  a = list(meyer[i].unique())\n",
    "  if ((97 in a) | (97. in a)):\n",
    "    s_97s.append(i)\n",
    "  if ((98 in a) | (98. in a)):\n",
    "    s_98s.append(i)\n",
    "  if ((99 in a) | (99. in a)):\n",
    "    s_99s.append(i)\n",
    "    \n",
    "s_97s.sort()\n",
    "s_98s.sort()\n",
    "s_99s.sort()\n",
    "\n",
    "for i in s_mode:\n",
    "  a = list(meyer[i].unique())\n",
    "  if ((97 in a) | (97. in a)):\n",
    "    s_97s.append(i)\n",
    "  if ((98 in a) | (98. in a)):\n",
    "    s_98s.append(i)\n",
    "  if ((99 in a) | (99. in a)):\n",
    "    s_99s.append(i)\n",
    "\n",
    "s_97s = ['w1q102', 'w1q103', 'w1q104', \n",
    "  'w1q106', 'w1q107', 'w1q108', 'w1q110', 'w1q111', 'w1q112',   # impute to 0s; \n",
    "  'w1q114', 'w1q115', 'w1q116', 'w1q117', 'w1q134']    # these are (mostly) age Qs\n",
    "s_98s = ['w1q45', 'w1q46', 'w1q47', 'w1q48', 'w1q49', 'w1q50', 'w1q51']  # NA\n",
    "s_99s = ['w1q45', 'w1q46', 'w1q47', 'w1q48', 'w1q49', 'w1q50', 'w1q51']  # age + 2\n",
    "\n",
    "s_median_new = [''.join([x, '_ei']) for x in s_median]\n",
    "s_mode_new = [''.join([x, '_ei']) for x in s_mode]\n",
    "\n",
    "s_97s_new = [''.join([x, '_ei']) for x in s_97s]\n",
    "s_98s_new = [''.join([x, '_ei']) for x in s_98s]\n",
    "s_99s_new = [''.join([x, '_ei']) for x in s_99s]\n",
    "\n",
    "# Replace these 97s with 0s\n",
    "meyer.loc[:, s_97s_new] = np.where(meyer.loc[:, s_97s]==97, 0, meyer.loc[:, s_97s])\n",
    "\n",
    "# Take a peek\n",
    "meyer[['w1q102', 'w1q103', 'w1q102_ei', 'w1q103_ei']]\n",
    "\n",
    "\n",
    "# Replace these 98s with NAs\n",
    "meyer.loc[:, s_98s_new] = np.where(meyer.loc[:, s_98s]==98, np.nan, meyer.loc[:, s_98s])\n",
    "\n",
    "# Take a peek\n",
    "meyer[['w1q45', 'w1q46', 'w1q45_ei', 'w1q46_ei']]\n",
    "\n",
    "\n",
    "# Replace these 99s with NAs\n",
    "meyer.loc[:, s_99s_new] = np.where(meyer.loc[:, s_99s]==99, np.nan, meyer.loc[:, s_99s_new])\n",
    "\n",
    "# Take a peek\n",
    "meyer[['w1q45', 'w1q46', 'w1q45_ei', 'w1q46_ei']]\n",
    "\n",
    "\n",
    "xs_97s = []\n",
    "xs_98s = []\n",
    "xs_99s = []\n",
    "\n",
    "# for i in s_97s_new:\n",
    "# for i in s_98s_new:\n",
    "for i in s_99s_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  if ((97 in a) | (97. in a)):\n",
    "    xs_97s.append(i)\n",
    "  if ((98 in a) | (98. in a)):\n",
    "    xs_98s.append(i)\n",
    "  if ((99 in a) | (99. in a)):\n",
    "    xs_99s.append(i)\n",
    "# none left!\n",
    "\n",
    "# Right here I need to figure out how to deal with the 97s-99s.\n",
    "# Ok here's hoping that's settled!\n",
    "\n",
    "# put them in order.  checking them made me crazy otherwise\n",
    "s_mode.sort()\n",
    "s_mode_new.sort()\n",
    "s_median.sort()\n",
    "s_median_new.sort()\n",
    "\n",
    "# make sure they're in order\n",
    "for i, j in list(zip(s_mode, s_mode_new)):\n",
    "  print(i, j)\n",
    "for i, j in list(zip(s_median, s_median_new)):\n",
    "  print(i, j)\n",
    "\n",
    "# They are!  Ok!  Let's go again!!\n",
    "\n",
    "# Safety first\n",
    "# meyer.to_csv('2024-06-05_meyer_much_imputation_done_but_not_si_yet.csv', index = False)\n",
    "\n",
    "# I think the cleanest way to do this without it breaking everything is to\n",
    "# break the dataframe apart, use the 2 SIs separately, then re-merge.\n",
    "# Let me check that the studyID will be suitable for merging on.\n",
    "\n",
    "meyer.shape # (1507, 271)\n",
    "# ah crap gotta drop the original 97s-99s\n",
    "\n",
    "drop_90s = s_97s + s_98s # s_99s = s_98s\n",
    "meyer.drop(columns = drop_90s, inplace = True)\n",
    "\n",
    "\n",
    "meyer.shape # 1507 rows (250 columns)\n",
    "meyer['studyid'].nunique() # 1507 unique values\n",
    "\n",
    "# update the lists of columns\n",
    "med_cols = [c if c in list(meyer.columns) else ''.join([c, '_ei']) for c in s_median]\n",
    "mode_cols = [c if c in list(meyer.columns) else ''.join([c, '_ei']) for c in s_mode]\n",
    "\n",
    "# split the df up for imputing\n",
    "meyer_median = meyer[med_cols+['studyid']]\n",
    "meyer_mode = meyer[mode_cols+['studyid']]\n",
    "\n",
    "# new column names\n",
    "s_median_new.append('studyid')\n",
    "s_mode_new.append('studyid')\n",
    "meyer_median.columns = s_median_new\n",
    "meyer_mode.columns = s_mode_new\n",
    "\n",
    "# Instantiate\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "si_med = SimpleImputer(strategy = 'median')\n",
    "si_mode = SimpleImputer(strategy = 'most_frequent')\n",
    "\n",
    "# do the imputation!\n",
    "meyer_median = pd.DataFrame(\n",
    "  si_med.fit_transform(meyer_median), columns = si_med.get_feature_names_out())\n",
    "\n",
    "meyer_mode = pd.DataFrame(\n",
    "  si_mode.fit_transform(meyer_mode), columns = si_mode.get_feature_names_out())\n",
    "\n",
    "# is it really that easy?\n",
    "meyer_median.isna().sum().sum()\n",
    "meyer_mode.isna().sum().sum()\n",
    "\n",
    "mme = meyer_median.describe()\n",
    "mmo = meyer_mode.describe()\n",
    "\n",
    "# Yes!  It is!  (This time.)\n",
    "\n",
    "# BUT because I imputed the 97s to be 0s, and there were so many 97s, that made the \n",
    "# median and mode 0.  So most of these guys just ended up imputed with 0s.\n",
    "# It's not exactly what I wanted (the true missings are people who DID have the \n",
    "# relevant thoughts, but just wouldn't say when), but given that there are so few \n",
    "# true missings, and that this has already eaten up a ton of time, I think it'll\n",
    "# be fine to leave for this iteration.  Put it in the next steps.\n",
    "\n",
    "# OK LAST THING RE-MERGE WOOHOO!\n",
    "meyer.shape # full thing has 250 columns\n",
    "len(s_median) # 31, including ID\n",
    "len(s_mode) # 37, including ID\n",
    "\n",
    "# also let me check some stuff\n",
    "# are the column names what I think they are?\n",
    "s_median_new==list(meyer_median.columns) # True\n",
    "s_mode_new==list(meyer_mode.columns) # True\n",
    "# is studyid the only column name that doesn't end in _ei\n",
    "[c for c in list(meyer_median.columns) if c.split('_')[-1]!='ei']\n",
    "[c for c in list(meyer_mode.columns) if c.split('_')[-1]!='ei']\n",
    "\n",
    "# ok let's GOOOO\n",
    "# I'm just going to combine all the possible columns that I might\n",
    "# now need to drop, rather than trying to remember which is which.\n",
    "# And then I'll filter it down to only the ones actually in meyer.columns.\n",
    "\n",
    "# drop_cols = s_median + s_mode + s_median_new + s_mode_new\n",
    "# len(drop_cols) # 138\n",
    "# \n",
    "# drop_cols = [c for c in drop_cols if c in list(meyer.columns)]\n",
    "# drop_cols = list(set(drop_cols))\n",
    "# drop_cols.remove('studyid')\n",
    "# len(drop_cols) # 68\n",
    "# ((len(s_median_new)-1) + (len(s_mode_new)-1)) # 66\n",
    "# # ooooh right I added some other suicide questions in and fixed their 97s\n",
    "# x = [c for c in drop_cols if c not in s_mode]\n",
    "# x = [c for c in x if c not in s_mode_new]\n",
    "# x = [c for c in x if c not in s_median_new]\n",
    "# x = [c for c in x if c not in s_median]\n",
    "# x # empty\n",
    "# x = [c for c in drop_cols if c not in s_mode_new]\n",
    "# x = [c for c in x if c not in s_median_new]\n",
    "# x\n",
    "\n",
    "# oh. duh. new approach\n",
    "drop_cols = med_cols + mode_cols\n",
    "len(drop_cols) # 68\n",
    "compare = [c for c in drop_cols if c in list(meyer.columns)]\n",
    "len(compare) # 68\n",
    "len(list(meyer_median.columns) + list(meyer_mode.columns)) # 70\n",
    "\n",
    "# safety first\n",
    "# meyer.to_csv('2024-06-05_pre-si-remerging.csv', index = False)\n",
    "# meyer_median.to_csv('2024-06-05_values-imputed-with-median.csv', index = False)\n",
    "# meyer_mode.to_csv('2024-06-05_values-imputed-with-mode.csv', index = False)\n",
    "\n",
    "# drop cols\n",
    "meyer.drop(columns = drop_cols, inplace = True) # [1507 rows x 182 columns]\n",
    "\n",
    "# merge\n",
    "meyer_m = meyer.merge(\n",
    "  meyer_median, left_on = 'studyid', right_on = 'studyid', how = 'left').merge(\n",
    "    meyer_mode, left_on = 'studyid', right_on = 'studyid', how = 'left')\n",
    "\n",
    "meyer_m.shape # (1507, 250)\n",
    "\n",
    "meyer_m.isna().sum()\n",
    "\n",
    "meyer = meyer_m.copy(deep = True)\n",
    "\n",
    "del meyer_m\n",
    "\n",
    "# meyer.to_csv('2024-06-05_si-imputation-done_remerged.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 3 in full:\n",
    "2024-06-05_finishing-imputation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wednesday, June 5, 2024\n",
    "# Imputation Regrouping\n",
    "\n",
    "# Most of my imputation is now done!  \n",
    "# I really messed up my spreadsheet though, so I'm\n",
    "# just going to make a new one.\n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "meyer = pd.read_csv('2024-06-05_si-imputation-done_remerged.csv')\n",
    "\n",
    "remaining_nas = pd.DataFrame(meyer.isna().sum())\n",
    "original_nas = pd.read_excel('./dsb318-capstone/04_scratch_work/01_notes/imputation_plan.xlsx')\n",
    "  \n",
    "remaining_nas.shape\n",
    "original_nas.shape\n",
    "\n",
    "nas = remaining_nas.merge(\n",
    "  original_nas, left_on = remaining_nas.index, right_on = 'column_name', how = 'left')\n",
    "\n",
    "nas.index = list(remaining_nas.index)\n",
    "\n",
    "nas = nas[nas[0]!=0]  # that column is the most up to date count\n",
    "# if it's currently 0, drop it\n",
    "\n",
    "\n",
    "# my spreadsheet got messed up, so I'm going back around on some of these\n",
    "\n",
    "for i in nas['column_name']:\n",
    "  print(f\"meyer[['{i}_ei']] = meyer[['{i}']].fillna(_)\")\n",
    "\n",
    "\n",
    "# The easy-er ones\n",
    "\n",
    "meyer[['w1q01_ei']] = meyer[['w1q01']].fillna(7)\n",
    "meyer[['w1q02_ei']] = meyer[['w1q02']].fillna(8)\n",
    "meyer[['w1q33_ei']] = meyer[['w1q33']].fillna(0)\n",
    "meyer[['w1q123a_ei']] = meyer[['w1q123a']].fillna(5)\n",
    "meyer[['w1q123b_ei']] = meyer[['w1q123b']].fillna(5)\n",
    "meyer[['w1q114_ei']] = meyer[['w1q114_ei']].fillna(0) # note that this guy already existed\n",
    "meyer[['w1q34_ei']] = meyer[['w1q34']].fillna(0) \n",
    "# I was originally going to impute this ^ from other info, but a 0 is fine and faster\n",
    "meyer[['w1q121_ei']] = meyer[['w1q121']].fillna(0) # nssh age\n",
    "meyer[['w1q122_ei']] = meyer[['w1q122']].fillna(0) # nssh age\n",
    "meyer.shape # (1507, 258)\n",
    "\n",
    "drop_cols = ['w1q01', 'w1q02', 'w1q33', 'w1q123a', \n",
    "  'w1q123b', 'w1q34', 'w1q121', 'w1q122']\n",
    "\n",
    "meyer.drop(columns = drop_cols, inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "# ooh boy here we go\n",
    "\n",
    "# impute 2 if sum(139)>0, else mode\n",
    "s = ''\n",
    "for i in list(range(1, 11)):\n",
    "  s += f'meyer[\"w1q139_{i}_ei\"] + '\n",
    "\n",
    "meyer['sum_w1q139'] = meyer[\"w1q139_1_ei\"] + meyer[\n",
    "  \"w1q139_2_ei\"] + meyer[\"w1q139_3_ei\"] + meyer[\n",
    "  \"w1q139_4_ei\"] + meyer[\"w1q139_5_ei\"] + meyer[\n",
    "  \"w1q139_6_ei\"] + meyer[\"w1q139_7_ei\"] + meyer[\n",
    "  \"w1q139_8_ei\"] + meyer[\"w1q139_9_ei\"] + meyer[\"w1q139_10_ei\"]\n",
    "\n",
    "meyer['sum_w1q139'].value_counts(dropna = False)\n",
    "\n",
    "meyer[['w1q137_ei']] = meyer[['w1q137']]\n",
    "meyer[['w1q138_ei']] = meyer[['w1q138']]\n",
    "\n",
    "cond1 = meyer['w1q137'].isna()\n",
    "cond2 = meyer['w1q138'].isna()\n",
    "\n",
    "meyer.loc[cond1, 'w1q137_ei'] = np.where(meyer.loc[cond1, 'sum_w1q139']>0, 2, 1)\n",
    "meyer.loc[cond2, 'w1q138_ei'] = np.where(meyer.loc[cond2, 'sum_w1q139']>0, 2, 1)\n",
    "\n",
    "meyer['w1q137'].value_counts(dropna = False)\n",
    "meyer['w1q137_ei'].value_counts(dropna = False)\n",
    "meyer['w1q138'].value_counts(dropna = False)\n",
    "meyer['w1q138_ei'].value_counts(dropna = False)\n",
    "\n",
    "# nice\n",
    "meyer.drop(columns = ['w1q137', 'w1q138', 'sum_w1q139'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "\n",
    "# impute 2 if sum(141)>0, else mode\n",
    "meyer['sum_w1q141'] = meyer[\"w1q141_1_ei\"] + meyer[\n",
    "  \"w1q141_2_ei\"] + meyer[\"w1q141_3_ei\"] + meyer[\n",
    "  \"w1q141_4_ei\"] + meyer[\"w1q141_5_ei\"] + meyer[\n",
    "  \"w1q141_6_ei\"] + meyer[\"w1q141_7_ei\"] + meyer[\n",
    "  \"w1q141_8_ei\"] + meyer[\"w1q141_9_ei\"] + meyer[\"w1q141_10_ei\"]\n",
    "\n",
    "meyer['sum_w1q141'].value_counts(dropna = False)\n",
    "\n",
    "meyer[['w1q140_ei']] = meyer[['w1q140']]\n",
    "\n",
    "cond1 = meyer['w1q140'].isna()\n",
    "\n",
    "meyer.loc[cond1, 'w1q140_ei'] = np.where(meyer.loc[cond1, 'sum_w1q141']>0, 2, 1)\n",
    "\n",
    "meyer['w1q140'].value_counts(dropna = False)\n",
    "meyer['w1q140_ei'].value_counts(dropna = False)\n",
    "\n",
    "# nice\n",
    "meyer.drop(columns = ['w1q140', 'sum_w1q141'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "\n",
    "# subset investigation; there were exactly 48 do not knows on q2\n",
    "\n",
    "cond1 = meyer['w1q03'].isna()\n",
    "\n",
    "test = meyer.loc[cond1, ['w1q02_ei', 'w1q03']]\n",
    "test['w1q02_ei'].value_counts(dropna = False, normalize = True) # why so many 8s?\n",
    "meyer['w1q02_ei'].value_counts(dropna = False, normalize = True)  # close enough for now\n",
    "\n",
    "# I conducted that test to see if the 48 NAs in Q3 were the same\n",
    "# people as the 48 \"don't knows\" in  Q2, or any other pattern.\n",
    "# I did not find one, and therefore imputed the mode, which is 2.\n",
    "\n",
    "meyer[['w1q03_ei']] = meyer[['w1q03']].fillna(2)\n",
    "\n",
    "meyer.shape\n",
    "meyer.drop(columns = 'w1q03', inplace = True)\n",
    "\n",
    "\n",
    "# impute from 142b or .c=True and 142e=False; and/or w1q171_x\n",
    "meyer['w1q142b_ei'].value_counts(dropna = False)\n",
    "meyer['w1q142c_ei'].value_counts(dropna = False)\n",
    "meyer['w1q146d'].value_counts(dropna = False)\n",
    "\n",
    "jobless_142s = ((meyer['w1q142b_ei']==1) | (meyer['w1q142c_ei']==1))\n",
    "cond1 = meyer['w1q146d'].isna()\n",
    "\n",
    "meyer[['w1q146d_ei']] = meyer[['w1q146d']]\n",
    "\n",
    "# meyer.loc[cond1, 'w1q146d_ei'] = np.where(meyer.loc[cond1, 'sum_w1q141']>0, 2, 1)\n",
    "\n",
    "# trying to do the np.where is making my head hurt, so\n",
    "for i in list(range(1507)):\n",
    "  if pd.notna(meyer.loc[i, 'w1q146d_ei'])==True:\n",
    "    continue\n",
    "  elif meyer.loc[i, 'w1q142c_ei']==1:\n",
    "    meyer.loc[i, 'w1q146d_ei']=1\n",
    "  elif meyer.loc[i, 'w1q142b_ei']==1:\n",
    "    meyer.loc[i, 'w1q146d_ei']=1\n",
    "  else:\n",
    "    meyer.loc[i, 'w1q146d_ei']=0\n",
    "\n",
    "meyer['w1q142b_ei'].value_counts(dropna = False)\n",
    "meyer['w1q142c_ei'].value_counts(dropna = False)\n",
    "meyer['w1q146d'].value_counts(dropna = False)\n",
    "meyer['w1q146d_ei'].value_counts(dropna = False)\n",
    "\n",
    "meyer.drop(columns = 'w1q146d', inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "\n",
    "# impute from poverty\n",
    "meyer[['w1q146b_ei']] = meyer[['w1q146b']]\n",
    "\n",
    "money_cols = ['w1q146b', 'w1poverty_i', # 'w1povertycat_i', \n",
    "  'w1q142h_ei', 'w1age', 'geducation', 'w1q175_ei']\n",
    "\n",
    "[c for c in list(meyer.columns) if c.split('_')[-1] not in ['ei', 'i']]\n",
    "\n",
    "# 142h: During the last year have you experienced a major financial crisis; 1=y, 2=n\n",
    "# 175: under water w/ debt; 1=n, 2=y  --- student loans?  maybe but that's getting too bespoke\n",
    "\n",
    "meyer['w1q175_ei'].value_counts()\n",
    "meyer['w1q146b'].value_counts()\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "meyer.loc[(meyer['w1q146b'].isna()), money_cols]\n",
    "\n",
    "a = sorted(list(meyer.columns))\n",
    "\n",
    "# I checked several columns that also have to do with money.\n",
    "# The census questions can address most of these NAs. \n",
    "# I imputed a bunch of values for w1q175, so I'm hesitant\n",
    "# to use it to impute others.  w1q142h_ei (major financial\n",
    "# crisis) is a 2 (no) for all of the remaining NAs.\n",
    "# For the rest I'm going to impute 0.  It's a close tie\n",
    "# between 0 (not true that they don't have enough money\n",
    "# to make ends meet) and 1+2 (somewhat or very true), and\n",
    "# I'm tempted to impute a 1 (somewhat true that they don't\n",
    "# have enough money to make ends meet), but it feels less\n",
    "# presumptuous to impute a 0.\n",
    "\n",
    "# trying to do the np.where is making my head hurt, so\n",
    "for i in list(range(1507)):\n",
    "  if pd.notna(meyer.loc[i, 'w1q146b_ei'])==True:\n",
    "    continue\n",
    "  elif meyer.loc[i, 'w1poverty_i']==1:  # census poverty yes\n",
    "    meyer.loc[i, 'w1q146b_ei']=1\n",
    "  else:\n",
    "    meyer.loc[i, 'w1q146b_ei']=0\n",
    "\n",
    "meyer['w1q146b'].value_counts(dropna = False)\n",
    "meyer['w1q146b_ei'].value_counts(dropna = False)\n",
    "\n",
    "meyer.drop(columns = 'w1q146b', inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "\n",
    "\n",
    "# subset investigation\n",
    "# these are the Qs about US nativity\n",
    "\n",
    "meyer = pd.read_csv('2024-06-05_most-imputation-done_midway-through-nativity.csv')\n",
    "\n",
    "meyer[['w1q166_ei']] = meyer[['w1q166']]\n",
    "meyer[['w1q167_ei']] = meyer[['w1q167']]\n",
    "meyer[['w1q168_ei']] = meyer[['w1q168']]\n",
    "\n",
    "test = meyer[['w1q166', 'w1q167', 'w1q168']]\n",
    "test1 = test[test['w1q166'].isna()] # u born here?\n",
    "test2 = test[test['w1q167'].isna()] # u live here 6-13?\n",
    "test3 = test[test['w1q168'].isna()] # parents NOT born here?\n",
    "\n",
    "# if they left all 3 blank, I'm imputing the mode\n",
    "a = meyer['w1q166'].isna()\n",
    "b = meyer['w1q167'].isna()\n",
    "c = meyer['w1q168'].isna()\n",
    "\n",
    "meyer.loc[(a & b & c), 'w1q166_ei'] = 1\n",
    "meyer.loc[(a & b & c), 'w1q167_ei'] = 1\n",
    "meyer.loc[(a & b & c), 'w1q168_ei'] = 3\n",
    "\n",
    "# look again\n",
    "test = meyer[['w1q166_ei', 'w1q167_ei', 'w1q168_ei']]\n",
    "test1 = test[test['w1q166_ei'].isna()] # u born here?\n",
    "test2 = test[test['w1q167_ei'].isna()] # u live here 6-13?\n",
    "test3 = test[test['w1q168_ei'].isna()] # parents NOT born here?\n",
    "\n",
    "# ok now the rest\n",
    "# looking at the pattern of associated 167s and 168s, I think \n",
    "# it's safe to impute the remaining (n=5) NAs in 166 as 1 \n",
    "\n",
    "meyer[['w1q166_ei']] = meyer[['w1q166_ei']].fillna(1)\n",
    "\n",
    "# looking at the pattern of associated 166s and 168s, I think \n",
    "# it's safe to impute the remaining (n=5) NAs in 167 thusly:\n",
    "a = meyer['w1q166']==2 # I wasn't born here\n",
    "b = meyer['w1q167'].isna()\n",
    "c = ((meyer['w1q168'].notna()) & (meyer['w1q168']<3)) # 1+ parent not born here\n",
    "\n",
    "meyer.loc[(a & b & c), 'w1q167_ei'] = 2 # under those conditions, impute NO\n",
    "meyer[['w1q167_ei']] = meyer[['w1q167_ei']].fillna(1) # otherwise yes\n",
    "\n",
    "# looking at the pattern of associated 166s and 167s, I think \n",
    "# it's safe to impute the remaining (n=5) NAs in 168 as 1 \n",
    "\n",
    "meyer[['w1q168_ei']] = meyer[['w1q168_ei']].fillna(1)\n",
    "\n",
    "meyer.drop(columns = ['w1q166', 'w1q167', 'w1q168'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "# let's back this guy up.  I'm getting nervous.\n",
    "# meyer.to_csv('2024-06-05_most-imputation-done_midway-through-nativity.csv', index = False)\n",
    "\n",
    "# let's back this guy up.  I'm getting nervous.\n",
    "# meyer.to_csv('2024-06-05_all-imputation-except-poverty-done.csv', index = False)\n",
    "\n",
    "# subset investigation\n",
    "meyer[['w1poverty_i_ei']] = meyer[['w1poverty_i']].fillna(0)\n",
    "meyer[['w1povertycat_i_ei']] = meyer[['w1povertycat_i']].fillna(4)\n",
    "\n",
    "meyer.drop(columns = ['w1poverty_i', 'w1povertycat_i'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "meyer.isna().sum().sum() # 0!\n",
    "\n",
    "meyer.to_csv('2024-06-05_all-imputation-done.csv', index = False)\n",
    "\n",
    "# Reorder the columns\n",
    "ordered_cols = sorted(list(meyer.columns))\n",
    "ordered_cols.remove('studyid')\n",
    "ordered_cols = ['studyid'] + ordered_cols\n",
    "len(ordered_cols) # 250\n",
    "\n",
    "meyer = meyer[ordered_cols]\n",
    "meyer.shape\n",
    "\n",
    "meyer.to_csv('2024-06-05_all-imputation-done_reordered.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 4 in full:\n",
    "2024-06-05_making-a-list-of-potential-7s-98s-etc-to-check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wednesday, June 5, 2024\n",
    "# Wrapping up loose values\n",
    "\n",
    "# All of my imputation is done, hooray!!\n",
    "# Now I'm just going back through and looking for all of the\n",
    "# \"non-NA NAs\" (e.g., 97, 98, 99) that I need to replace with \n",
    "# real imputations, lest they through the entire column off.\n",
    "\n",
    "for i in list(meyer.columns):\n",
    "  print(i)\n",
    "  \n",
    "problem_children = [('w1q01_ei', [98]), ('w1q02_ei', [98]), \n",
    "  ('w1q33_ei', [97]), ('w1q34_ei', [97]), ('w1q35_ei', [97]), \n",
    "  ('w1q36_ei', [97]), ('w1q45_ei', [98, 99]), ('w1q46_ei', [98, 99]), \n",
    "  ('w1q47_ei', [98, 99]), ('w1q48_ei', [98, 99]), ('w1q49_ei', [98, 99]), \n",
    "  ('w1q50_ei', [98, 99]), ('w1q51_ei', [98, 99]), ('w1q89_ei', [7]), \n",
    "  ('w1q102_ei', [97]), ('w1q103_ei', [97]), ('w1q104_ei', [97]), \n",
    "  ('w1q106_ei', [97]), ('w1q107_ei', [97]), ('w1q108_ei', [97]),\n",
    "  ('w1q110_ei', [97]), ('w1q111_ei', [0, 97]), ('w1q112_ei', [97]),\n",
    "  ('w1q114_ei', [6, 7, 8, 9, 97]), ('w1q115_ei', [97]), ('w1q116_ei', [97]),\n",
    "  ('w1q117_ei', [97]), ('w1q120_ei', [97]), ('w1q121_ei', [97]),\n",
    "  ('w1q122_ei', [97]), ('w1q123a_ei', [5]), ('w1q123b_ei', [5]), \n",
    "  ('w1q123c_ei', [5]), ('w1q123d_ei', [5]), ('w1q134_ei', [97]), \n",
    "  ('w1q168_ei', [97]), ('w1q136_10_ei', [7, 97]), ('w1q136_1_ei', [7, 97]), \n",
    "  ('w1q136_2_ei', [7, 97]), ('w1q136_3_ei', [7, 97]), \n",
    "  ('w1q136_4_ei', [7, 97]), ('w1q136_5_ei', [7, 97]), \n",
    "  ('w1q136_6_ei', [7, 97]), ('w1q136_7_ei', [7, 97]), \n",
    "  ('w1q136_8_ei', [7, 97]), ('w1q136_9_ei', [7, 97]), \n",
    "  ('w1q139_10_ei', [7, 97]), ('w1q139_1_ei', [7, 97]), \n",
    "  ('w1q139_2_ei', [7, 97]), ('w1q139_3_ei', [7, 97]), \n",
    "  ('w1q139_4_ei', [7, 97]), ('w1q139_5_ei', [7, 97]), \n",
    "  ('w1q139_6_ei', [7, 97]), ('w1q139_7_ei', [7, 97]), \n",
    "  ('w1q139_8_ei', [7, 97]), ('w1q139_9_ei', [7, 97]), \n",
    "  ('w1q141_10_ei', [7, 97]), ('w1q141_1_ei', [7, 97]), \n",
    "  ('w1q141_2_ei', [7, 97]), ('w1q141_3_ei', [7, 97]), \n",
    "  ('w1q141_4_ei', [7, 97]), ('w1q141_5_ei', [7, 97]), \n",
    "  ('w1q141_6_ei', [7, 97]), ('w1q141_7_ei', [7, 97]), \n",
    "  ('w1q141_8_ei', [7, 97]), ('w1q141_9_ei', [7, 97]), \n",
    "  ('w1q143_10_ei', [7, 97]), ('w1q143_1_ei', [7, 97]), \n",
    "  ('w1q143_2_ei', [7, 97]), ('w1q143_3_ei', [7, 97]), \n",
    "  ('w1q143_4_ei', [7, 97]), ('w1q143_5_ei', [7, 97]), \n",
    "  ('w1q143_6_ei', [7, 97]), ('w1q143_7_ei', [7, 97]), \n",
    "  ('w1q143_8_ei', [7, 97]), ('w1q143_9_ei', [7, 97]), \n",
    "  ('w1q145_10_ei', [7, 97]), ('w1q145_1_ei', [7, 97]), \n",
    "  ('w1q145_2_ei', [7, 97]), ('w1q145_3_ei', [7, 97]), \n",
    "  ('w1q145_4_ei', [7, 97]), ('w1q145_5_ei', [7, 97]), \n",
    "  ('w1q145_6_ei', [7, 97]), ('w1q145_7_ei', [7, 97]), \n",
    "  ('w1q145_8_ei', [7, 97]), ('w1q145_9_ei', [7, 97]), \n",
    "  ('w1q163_10_ei', [7, 97]), ('w1q163_1_ei', [7, 97]), \n",
    "  ('w1q163_2_ei', [7, 97]), ('w1q163_3_ei', [7, 97]), \n",
    "  ('w1q163_4_ei', [7, 97]), ('w1q163_5_ei', [7, 97]), \n",
    "  ('w1q163_6_ei', [7, 97]), ('w1q163_7_ei', [7, 97]), \n",
    "  ('w1q163_8_ei', [7, 97]), ('w1q163_9_ei', [7, 97])]\n",
    "\n",
    "\n",
    "s = '''\n",
    "w1q136_10_ei\n",
    "w1q136_1_ei\n",
    "w1q136_2_ei\n",
    "w1q136_3_ei\n",
    "w1q136_4_ei\n",
    "w1q136_5_ei\n",
    "w1q136_6_ei\n",
    "w1q136_7_ei\n",
    "w1q136_8_ei\n",
    "w1q136_9_ei\n",
    "w1q139_10_ei\n",
    "w1q139_1_ei\n",
    "w1q139_2_ei\n",
    "w1q139_3_ei\n",
    "w1q139_4_ei\n",
    "w1q139_5_ei\n",
    "w1q139_6_ei\n",
    "w1q139_7_ei\n",
    "w1q139_8_ei\n",
    "w1q139_9_ei\n",
    "w1q141_10_ei\n",
    "w1q141_1_ei\n",
    "w1q141_2_ei\n",
    "w1q141_3_ei\n",
    "w1q141_4_ei\n",
    "w1q141_5_ei\n",
    "w1q141_6_ei\n",
    "w1q141_7_ei\n",
    "w1q141_8_ei\n",
    "w1q141_9_ei\n",
    "w1q143_10_ei\n",
    "w1q143_1_ei\n",
    "w1q143_2_ei\n",
    "w1q143_3_ei\n",
    "w1q143_4_ei\n",
    "w1q143_5_ei\n",
    "w1q143_6_ei\n",
    "w1q143_7_ei\n",
    "w1q143_8_ei\n",
    "w1q143_9_ei\n",
    "w1q145_10_ei\n",
    "w1q145_1_ei\n",
    "w1q145_2_ei\n",
    "w1q145_3_ei\n",
    "w1q145_4_ei\n",
    "w1q145_5_ei\n",
    "w1q145_6_ei\n",
    "w1q145_7_ei\n",
    "w1q145_8_ei\n",
    "w1q145_9_ei\n",
    "w1q163_10_ei\n",
    "w1q163_1_ei\n",
    "w1q163_2_ei\n",
    "w1q163_3_ei\n",
    "w1q163_4_ei\n",
    "w1q163_5_ei\n",
    "w1q163_6_ei\n",
    "w1q163_7_ei\n",
    "w1q163_8_ei\n",
    "w1q163_9_ei'''\n",
    "\n",
    "k = s.replace('\\n', '\", \"')\n",
    "print(k)\n",
    "\n",
    "s = [\"w1q136_10_ei\", \"w1q136_1_ei\", \"w1q136_2_ei\", \"w1q136_3_ei\", \n",
    "  \"w1q136_4_ei\", \"w1q136_5_ei\", \"w1q136_6_ei\", \"w1q136_7_ei\", \n",
    "  \"w1q136_8_ei\", \"w1q136_9_ei\", \"w1q139_10_ei\", \"w1q139_1_ei\", \n",
    "  \"w1q139_2_ei\", \"w1q139_3_ei\", \"w1q139_4_ei\", \"w1q139_5_ei\", \n",
    "  \"w1q139_6_ei\", \"w1q139_7_ei\", \"w1q139_8_ei\", \"w1q139_9_ei\", \n",
    "  \"w1q141_10_ei\", \"w1q141_1_ei\", \"w1q141_2_ei\", \"w1q141_3_ei\", \n",
    "  \"w1q141_4_ei\", \"w1q141_5_ei\", \"w1q141_6_ei\", \"w1q141_7_ei\", \n",
    "  \"w1q141_8_ei\", \"w1q141_9_ei\", \"w1q143_10_ei\", \"w1q143_1_ei\", \n",
    "  \"w1q143_2_ei\", \"w1q143_3_ei\", \"w1q143_4_ei\", \"w1q143_5_ei\", \n",
    "  \"w1q143_6_ei\", \"w1q143_7_ei\", \"w1q143_8_ei\", \"w1q143_9_ei\", \n",
    "  \"w1q145_10_ei\", \"w1q145_1_ei\", \"w1q145_2_ei\", \"w1q145_3_ei\", \n",
    "  \"w1q145_4_ei\", \"w1q145_5_ei\", \"w1q145_6_ei\", \"w1q145_7_ei\", \n",
    "  \"w1q145_8_ei\", \"w1q145_9_ei\", \"w1q163_10_ei\", \"w1q163_1_ei\", \n",
    "  \"w1q163_2_ei\", \"w1q163_3_ei\", \"w1q163_4_ei\", \"w1q163_5_ei\", \n",
    "  \"w1q163_6_ei\", \"w1q163_7_ei\", \"w1q163_8_ei\", \"w1q163_9_ei\"]\n",
    "\n",
    "g = ''\n",
    "for i in s:\n",
    "  g += f\"('{i}', [7, 97]), \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSVs are in the main project folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "\n",
    "\n",
    "files created: \n",
    "- x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 1 in full:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 2 in full:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 3 in full:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 4 in full:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
