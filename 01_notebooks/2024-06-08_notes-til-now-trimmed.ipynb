{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 3 in full:\n",
    "2024-06-05_finishing-imputation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ooh boy here we go\n",
    "\n",
    "# impute 2 if sum(139)>0, else mode\n",
    "s = ''\n",
    "for i in list(range(1, 11)):\n",
    "  s += f'meyer[\"w1q139_{i}_ei\"] + '\n",
    "\n",
    "meyer['sum_w1q139'] = meyer[\"w1q139_1_ei\"] + meyer[\n",
    "  \"w1q139_2_ei\"] + meyer[\"w1q139_3_ei\"] + meyer[\n",
    "  \"w1q139_4_ei\"] + meyer[\"w1q139_5_ei\"] + meyer[\n",
    "  \"w1q139_6_ei\"] + meyer[\"w1q139_7_ei\"] + meyer[\n",
    "  \"w1q139_8_ei\"] + meyer[\"w1q139_9_ei\"] + meyer[\"w1q139_10_ei\"]\n",
    "\n",
    "meyer['sum_w1q139'].value_counts(dropna = False)\n",
    "\n",
    "meyer[['w1q137_ei']] = meyer[['w1q137']]\n",
    "meyer[['w1q138_ei']] = meyer[['w1q138']]\n",
    "\n",
    "cond1 = meyer['w1q137'].isna()\n",
    "cond2 = meyer['w1q138'].isna()\n",
    "\n",
    "meyer.loc[cond1, 'w1q137_ei'] = np.where(meyer.loc[cond1, 'sum_w1q139']>0, 2, 1)\n",
    "meyer.loc[cond2, 'w1q138_ei'] = np.where(meyer.loc[cond2, 'sum_w1q139']>0, 2, 1)\n",
    "\n",
    "meyer['w1q137'].value_counts(dropna = False)\n",
    "meyer['w1q137_ei'].value_counts(dropna = False)\n",
    "meyer['w1q138'].value_counts(dropna = False)\n",
    "meyer['w1q138_ei'].value_counts(dropna = False)\n",
    "\n",
    "# nice\n",
    "meyer.drop(columns = ['w1q137', 'w1q138', 'sum_w1q139'], inplace = True)\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute 2 if sum(141)>0, else mode\n",
    "meyer['sum_w1q141'] = meyer[\"w1q141_1_ei\"] + meyer[\n",
    "  \"w1q141_2_ei\"] + meyer[\"w1q141_3_ei\"] + meyer[\n",
    "  \"w1q141_4_ei\"] + meyer[\"w1q141_5_ei\"] + meyer[\n",
    "  \"w1q141_6_ei\"] + meyer[\"w1q141_7_ei\"] + meyer[\n",
    "  \"w1q141_8_ei\"] + meyer[\"w1q141_9_ei\"] + meyer[\"w1q141_10_ei\"]\n",
    "\n",
    "meyer['sum_w1q141'].value_counts(dropna = False)\n",
    "\n",
    "meyer[['w1q140_ei']] = meyer[['w1q140']]\n",
    "\n",
    "cond1 = meyer['w1q140'].isna()\n",
    "\n",
    "meyer.loc[cond1, 'w1q140_ei'] = np.where(meyer.loc[cond1, 'sum_w1q141']>0, 2, 1)\n",
    "\n",
    "meyer['w1q140'].value_counts(dropna = False)\n",
    "meyer['w1q140_ei'].value_counts(dropna = False)\n",
    "\n",
    "# nice\n",
    "meyer.drop(columns = ['w1q140', 'sum_w1q141'], inplace = True)\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute from 142b or .c=True and 142e=False; and/or w1q171_x\n",
    "meyer['w1q142b_ei'].value_counts(dropna = False)\n",
    "meyer['w1q142c_ei'].value_counts(dropna = False)\n",
    "meyer['w1q146d'].value_counts(dropna = False)\n",
    "\n",
    "jobless_142s = ((meyer['w1q142b_ei']==1) | (meyer['w1q142c_ei']==1))\n",
    "cond1 = meyer['w1q146d'].isna()\n",
    "\n",
    "meyer[['w1q146d_ei']] = meyer[['w1q146d']]\n",
    "\n",
    "# meyer.loc[cond1, 'w1q146d_ei'] = np.where(meyer.loc[cond1, 'sum_w1q141']>0, 2, 1)\n",
    "\n",
    "# trying to do the np.where is making my head hurt, so\n",
    "for i in list(range(1507)):\n",
    "  if pd.notna(meyer.loc[i, 'w1q146d_ei'])==True:\n",
    "    continue\n",
    "  elif meyer.loc[i, 'w1q142c_ei']==1:\n",
    "    meyer.loc[i, 'w1q146d_ei']=1\n",
    "  elif meyer.loc[i, 'w1q142b_ei']==1:\n",
    "    meyer.loc[i, 'w1q146d_ei']=1\n",
    "  else:\n",
    "    meyer.loc[i, 'w1q146d_ei']=0\n",
    "\n",
    "meyer['w1q142b_ei'].value_counts(dropna = False)\n",
    "meyer['w1q142c_ei'].value_counts(dropna = False)\n",
    "meyer['w1q146d'].value_counts(dropna = False)\n",
    "meyer['w1q146d_ei'].value_counts(dropna = False)\n",
    "\n",
    "meyer.drop(columns = 'w1q146d', inplace = True)\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute from poverty\n",
    "meyer[['w1q146b_ei']] = meyer[['w1q146b']]\n",
    "\n",
    "money_cols = ['w1q146b', 'w1poverty_i', # 'w1povertycat_i', \n",
    "  'w1q142h_ei', 'w1age', 'geducation', 'w1q175_ei']\n",
    "\n",
    "[c for c in list(meyer.columns) if c.split('_')[-1] not in ['ei', 'i']]\n",
    "\n",
    "# 142h: During the last year have you experienced a major financial crisis; 1=y, 2=n\n",
    "# 175: under water w/ debt; 1=n, 2=y  --- student loans?  maybe but that's getting too bespoke\n",
    "\n",
    "meyer['w1q175_ei'].value_counts()\n",
    "meyer['w1q146b'].value_counts()\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "meyer.loc[(meyer['w1q146b'].isna()), money_cols]\n",
    "\n",
    "a = sorted(list(meyer.columns))\n",
    "\n",
    "# I checked several columns that also have to do with money.\n",
    "# The census questions can address most of these NAs. \n",
    "# I imputed a bunch of values for w1q175, so I'm hesitant\n",
    "# to use it to impute others.  w1q142h_ei (major financial\n",
    "# crisis) is a 2 (no) for all of the remaining NAs.\n",
    "# For the rest I'm going to impute 0.  It's a close tie\n",
    "# between 0 (not true that they don't have enough money\n",
    "# to make ends meet) and 1+2 (somewhat or very true), and\n",
    "# I'm tempted to impute a 1 (somewhat true that they don't\n",
    "# have enough money to make ends meet), but it feels less\n",
    "# presumptuous to impute a 0.\n",
    "\n",
    "# trying to do the np.where is making my head hurt, so\n",
    "for i in list(range(1507)):\n",
    "  if pd.notna(meyer.loc[i, 'w1q146b_ei'])==True:\n",
    "    continue\n",
    "  elif meyer.loc[i, 'w1poverty_i']==1:  # census poverty yes\n",
    "    meyer.loc[i, 'w1q146b_ei']=1\n",
    "  else:\n",
    "    meyer.loc[i, 'w1q146b_ei']=0\n",
    "\n",
    "meyer['w1q146b'].value_counts(dropna = False)\n",
    "meyer['w1q146b_ei'].value_counts(dropna = False)\n",
    "\n",
    "meyer.drop(columns = 'w1q146b', inplace = True)\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset investigation\n",
    "# these are the Qs about US nativity\n",
    "\n",
    "meyer[['w1q166_ei']] = meyer[['w1q166']]\n",
    "meyer[['w1q167_ei']] = meyer[['w1q167']]\n",
    "meyer[['w1q168_ei']] = meyer[['w1q168']]\n",
    "\n",
    "test = meyer[['w1q166', 'w1q167', 'w1q168']]\n",
    "test1 = test[test['w1q166'].isna()] # u born here?\n",
    "test2 = test[test['w1q167'].isna()] # u live here 6-13?\n",
    "test3 = test[test['w1q168'].isna()] # parents NOT born here?\n",
    "\n",
    "# if they left all 3 blank, I'm imputing the mode\n",
    "a = meyer['w1q166'].isna()\n",
    "b = meyer['w1q167'].isna()\n",
    "c = meyer['w1q168'].isna()\n",
    "\n",
    "meyer.loc[(a & b & c), 'w1q166_ei'] = 1\n",
    "meyer.loc[(a & b & c), 'w1q167_ei'] = 1\n",
    "meyer.loc[(a & b & c), 'w1q168_ei'] = 3\n",
    "\n",
    "# look again\n",
    "test = meyer[['w1q166_ei', 'w1q167_ei', 'w1q168_ei']]\n",
    "test1 = test[test['w1q166_ei'].isna()] # u born here?\n",
    "test2 = test[test['w1q167_ei'].isna()] # u live here 6-13?\n",
    "test3 = test[test['w1q168_ei'].isna()] # parents NOT born here?\n",
    "\n",
    "# ok now the rest\n",
    "# looking at the pattern of associated 167s and 168s, I think \n",
    "# it's safe to impute the remaining (n=5) NAs in 166 as 1 \n",
    "\n",
    "meyer[['w1q166_ei']] = meyer[['w1q166_ei']].fillna(1)\n",
    "\n",
    "# looking at the pattern of associated 166s and 168s, I think \n",
    "# it's safe to impute the remaining (n=5) NAs in 167 thusly:\n",
    "a = meyer['w1q166']==2 # I wasn't born here\n",
    "b = meyer['w1q167'].isna()\n",
    "c = ((meyer['w1q168'].notna()) & (meyer['w1q168']<3)) # 1+ parent not born here\n",
    "\n",
    "meyer.loc[(a & b & c), 'w1q167_ei'] = 2 # under those conditions, impute NO\n",
    "meyer[['w1q167_ei']] = meyer[['w1q167_ei']].fillna(1) # otherwise yes\n",
    "\n",
    "# looking at the pattern of associated 166s and 167s, I think \n",
    "# it's safe to impute the remaining (n=5) NAs in 168 as 1 \n",
    "\n",
    "meyer[['w1q168_ei']] = meyer[['w1q168_ei']].fillna(1)\n",
    "\n",
    "meyer.drop(columns = ['w1q166', 'w1q167', 'w1q168'], inplace = True)\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's back this guy up.  I'm getting nervous.\n",
    "# meyer.to_csv('2024-06-05_most-imputation-done_midway-through-nativity.csv', index = False)\n",
    "\n",
    "# let's back this guy up.  I'm getting nervous.\n",
    "# meyer.to_csv('2024-06-05_all-imputation-except-poverty-done.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset investigation\n",
    "meyer[['w1poverty_i_ei']] = meyer[['w1poverty_i']].fillna(0)\n",
    "meyer[['w1povertycat_i_ei']] = meyer[['w1povertycat_i']].fillna(4)\n",
    "\n",
    "meyer.drop(columns = ['w1poverty_i', 'w1povertycat_i'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "meyer.isna().sum().sum() # 0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meyer.to_csv('2024-06-05_all-imputation-done.csv', index = False)\n",
    "\n",
    "# Reorder the columns\n",
    "ordered_cols = sorted(list(meyer.columns))\n",
    "ordered_cols.remove('studyid')\n",
    "ordered_cols = ['studyid'] + ordered_cols\n",
    "len(ordered_cols) # 250\n",
    "\n",
    "meyer = meyer[ordered_cols]\n",
    "meyer.shape\n",
    "\n",
    "meyer.to_csv('2024-06-05_all-imputation-done_reordered.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thursday, June 6, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "Today I finished tracking down and handling all the non-NA NAs (7, 97, 98, 99), dropped some more columns, and did all the recoding and transformations I could think of in preparation to combine the columns via `feat_eng_dict`.  I even created a function today!\n",
    "\n",
    "I created a new set-up file today, in which I imported the most up to date version of the CSV that I saved yesterday.  I plan to do the same tomorrow.\n",
    "\n",
    "Today I finally chose a y variable - **kessler6**.  This is a general mental health measure.  I dropped a few rows because they had more than 1 missing value in the features that comprise this scale.  The original authors imputed values for everyone, but I didn't want to have y values with more than 1 component part that had to be imputed.  Interestingly, in doing so, I think (although I'm not sure) that I may have dropped the rows from which many of the other missing values came, too.  \n",
    "\n",
    "Because I was already well underway with my cleaning, I had to go about dropping these rows in a rather roundabout way.  (This would probably be best to whitewash in the final report.)  I reimported a clean copy of the dataset, located all the rows with more than one NA in that set of columns,and then extracted the corresponding `studyid`.  I then dropped the rows with that `studyid` in my working copy of the dataframe.  At the end of the day, my dataset is **(1494, 228)**.\n",
    "\n",
    "**I discovered that the kessler values *are not normally distributed*.**  I tried a few different things, and taking the square root of them showed the most improvement, although the qqplot still looks not-great (not as bad as project 4 though!).  **I created a column for the square root of these values, but *did not delete the original values*.  It will be important to not include them, or the `studyid` in my X matrix!**\n",
    "\n",
    "All of the non-NA NAs were handled a little differently, but (I think) I sufficiently commented the code to explain what I did and why.  Where possible/necessary/I remembered, I tried to mirror what I did with the actual NAs when dealing with these values.\n",
    "\n",
    "I then went through all the columns indicated in my `feat_eng_dict`, and recoded or transformed them as necessary.  I also updated the names of the columns in the dictionary, because all but literally 1 of them have had `'_ei'` and sometimes `'_r'` tacked onto them by now.  What I did with each batch of columns is marked in comments in the code.  This is also the script where I defined and used a function.  Also in this script, I OHE'd the columns about what religion someone was raised in or practices now.  I first collapsed these categories into fewer bins, then OHE'd them.  I chose to drop the resulting column that corresponded to, more or less, \"not religious,\" because it seemed most intuitive.  The other 2 categories are for Christian-ish religions, and other.  Despite it being somewhat disrespectful, I chose to collapse all other religious practices into \"other\" because these categories were all so small on their own.  Even combined, they are dwarfed by the other two options.\n",
    "\n",
    "Finally, when all that was done, I updated my dictionary and saved a fresh copy of it to use tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.py` files created: \n",
    "1. 2024-06-06_set-up.py\n",
    "2. 2024-06-06_non-NA-NAs.py\n",
    "3. 2024-06-06_y-variable.py\n",
    "4. 2024-06-06_column_recoding_for_feat_eng_dict_combinations.py\n",
    "5. 2024-06-06_updated_feat_eng_dict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 2 in full:\n",
    "2024-06-06_non-NA-NAs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thursday, June 6, 2024\n",
    "# \"Imputation\" of non-NA NAs (97s, etc.)\n",
    "\n",
    "# w1q120 is the only one without an _ei\n",
    "problem_children = {'w1q01_ei': [98], 'w1q02_ei': [98],  'w1q33_ei': [97], \n",
    "  'w1q34_ei': [97], 'w1q35_ei': [97],  'w1q36_ei': [97], 'w1q45_ei': [98, 99], \n",
    "  'w1q46_ei': [98, 99],  'w1q47_ei': [98, 99], 'w1q48_ei': [98, 99], \n",
    "  'w1q49_ei': [98, 99],  'w1q50_ei': [98, 99], 'w1q51_ei': [98, 99], \n",
    "  'w1q89_ei': [7],  'w1q102_ei': [97], 'w1q103_ei': [97], 'w1q104_ei': [97],  \n",
    "  'w1q106_ei': [97], 'w1q107_ei': [97], 'w1q108_ei': [97],  'w1q110_ei': [97], \n",
    "  'w1q111_ei': [0, 97], 'w1q112_ei': [97],  'w1q114_ei': [6, 7, 8, 9, 97], \n",
    "  'w1q115_ei': [97], 'w1q116_ei': [97],  'w1q117_ei': [97], 'w1q120': [97], \n",
    "  'w1q121_ei': [97],  'w1q122_ei': [97], 'w1q123a_ei': [5], 'w1q123b_ei': [5],  \n",
    "  'w1q123c_ei': [5], 'w1q123d_ei': [5], 'w1q134_ei': [97],  'w1q168_ei': [97], \n",
    "  'w1q136_10_ei': [7, 97], 'w1q136_1_ei': [7, 97],  'w1q136_2_ei': [7, 97], \n",
    "  'w1q136_3_ei': [7, 97],  'w1q136_4_ei': [7, 97], 'w1q136_5_ei': [7, 97],  \n",
    "  'w1q136_6_ei': [7, 97], 'w1q136_7_ei': [7, 97],  'w1q136_8_ei': [7, 97], \n",
    "  'w1q136_9_ei': [7, 97],  'w1q139_10_ei': [7, 97], 'w1q139_1_ei': [7, 97],  \n",
    "  'w1q139_2_ei': [7, 97], 'w1q139_3_ei': [7, 97],  'w1q139_4_ei': [7, 97], \n",
    "  'w1q139_5_ei': [7, 97],  'w1q139_6_ei': [7, 97], 'w1q139_7_ei': [7, 97],  \n",
    "  'w1q139_8_ei': [7, 97], 'w1q139_9_ei': [7, 97],  'w1q141_10_ei': [7, 97], \n",
    "  'w1q141_1_ei': [7, 97],  'w1q141_2_ei': [7, 97], 'w1q141_3_ei': [7, 97],  \n",
    "  'w1q141_4_ei': [7, 97], 'w1q141_5_ei': [7, 97],  'w1q141_6_ei': [7, 97], \n",
    "  'w1q141_7_ei': [7, 97],  'w1q141_8_ei': [7, 97], 'w1q141_9_ei': [7, 97],  \n",
    "  'w1q143_10_ei': [7, 97], 'w1q143_1_ei': [7, 97],  'w1q143_2_ei': [7, 97], \n",
    "  'w1q143_3_ei': [7, 97],  'w1q143_4_ei': [7, 97], 'w1q143_5_ei': [7, 97],  \n",
    "  'w1q143_6_ei': [7, 97], 'w1q143_7_ei': [7, 97],  'w1q143_8_ei': [7, 97], \n",
    "  'w1q143_9_ei': [7, 97],  'w1q145_10_ei': [7, 97], 'w1q145_1_ei': [7, 97],  \n",
    "  'w1q145_2_ei': [7, 97], 'w1q145_3_ei': [7, 97],  'w1q145_4_ei': [7, 97], \n",
    "  'w1q145_5_ei': [7, 97],  'w1q145_6_ei': [7, 97], 'w1q145_7_ei': [7, 97],  \n",
    "  'w1q145_8_ei': [7, 97], 'w1q145_9_ei': [7, 97],  'w1q163_10_ei': [7, 97], \n",
    "  'w1q163_1_ei': [7, 97],  'w1q163_2_ei': [7, 97], 'w1q163_3_ei': [7, 97],  \n",
    "  'w1q163_4_ei': [7, 97], 'w1q163_5_ei': [7, 97],  'w1q163_6_ei': [7, 97], \n",
    "  'w1q163_7_ei': [7, 97],  'w1q163_8_ei': [7, 97], 'w1q163_9_ei': [7, 97]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_jail = []\n",
    "for c, v in problem_children.items():\n",
    "  a = list(meyer[c].unique())\n",
    "  for i in v:\n",
    "    if i in a:\n",
    "      column_jail.append((c, v, i))\n",
    "      break\n",
    "\n",
    "for i in column_jail:\n",
    "  if i[0]=='w1q02_ei':\n",
    "    continue\n",
    "  meyer[i[0]].value_counts(dropna = False)\n",
    "  meyer[i[0]].value_counts(dropna = False, normalize = True)\n",
    "  print('')\n",
    "  print('='*20)\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('w1q89_ei', [7], 7), \n",
    "# This is the question about how often people smoke cigarettes, and the 7s are people\n",
    "# who said on the previous question that they do not smoke (or failed to answer the \n",
    "# question, whom I (think I) imputed as no). Therefore, I'll impute \"not at all\" here.\n",
    "cond = meyer['w1q89_ei']==7.0\n",
    "meyer.loc[cond, 'w1q89_ei']=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the 123 questions are about outness, but they're coded weird. It's are you out to...\n",
    "# 1 = All, 2 = Most, 3 = Some, 4 = None, 5 = don't know/does not apply/[missing value]\n",
    "# So firstly, it's counter-intuitive.  Higher numbers = less out.  Sort of.  Secondly, 5 is\n",
    "# in a weird place on the scale  I think I'm going to recode it like this:\n",
    "# 4 -> 0 = I can confidently say I am out to \"None\" of these people. (LOWEST OUTNESS)\n",
    "# 5 -> 1 = I'm being wishy-washy about how out I am to these people.\n",
    "# 3 -> 2 = Some\n",
    "# 2 -> 3 = Most\n",
    "# 1 -> 4 = All (HIGHEST OUTNESS)\n",
    "# ('w1q123a_ei', [5], 5), ('w1q123b_ei', [5], 5), ('w1q123c_ei', [5], 5), ('w1q123d_ei', [5], 5)]\n",
    "\n",
    "meyer[['w1q123a_ei_r', 'w1q123b_ei_r', 'w1q123c_ei_r', 'w1q123d_ei_r']] = meyer[[\n",
    "  'w1q123a_ei', 'w1q123b_ei', 'w1q123c_ei', 'w1q123d_ei']]\n",
    "\n",
    "recode_123s = {4: 0, 5: 1, 3: 2, 2: 3, 1: 4}\n",
    "\n",
    "old_cols = ['w1q123a_ei', 'w1q123b_ei', 'w1q123c_ei', 'w1q123d_ei']\n",
    "new_cols = ['w1q123a_ei_r', 'w1q123b_ei_r', 'w1q123c_ei_r', 'w1q123d_ei_r']\n",
    "\n",
    "for old, new in list(zip(old_cols, new_cols)):\n",
    "  meyer[new] = meyer[old].map(recode_123s)\n",
    "\n",
    "check = meyer[['w1q123a_ei', 'w1q123a_ei_r', 'w1q123b_ei', 'w1q123b_ei_r', \n",
    "  'w1q123c_ei', 'w1q123c_ei_r', 'w1q123d_ei', 'w1q123d_ei_r']]\n",
    "# Yeah checks out!\n",
    "\n",
    "meyer.drop(columns = old_cols, inplace = True)\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't remember if I actually *executed* all that code, so let me test it again. I reran \n",
    "# the code to define problem_children just to be safe, and now I'm going to modify the loop.\n",
    "column_jail = []\n",
    "columns_done = []\n",
    "z = 0 # I'm a little hazy on `continue` so this is just a check\n",
    "for c, v in problem_children.items():\n",
    "\n",
    "  # if that column name isn't in the list, find out why\n",
    "  if c not in list(meyer.columns):\n",
    "    if c=='w1q02_ei':\n",
    "      columns_done.append(c)\n",
    "      z+=1\n",
    "      continue\n",
    "  \n",
    "    elif c in age_cols:\n",
    "      columns_done.append(c)\n",
    "      z+=1\n",
    "      continue\n",
    "    \n",
    "    # if neither of those work, try renaming it\n",
    "    c_r = ''.join([c, '_r'])\n",
    "    if c_r not in list(meyer.columns):\n",
    "      print(c)\n",
    "      columns_done.append(c)\n",
    "      z+=1\n",
    "      continue\n",
    "    elif c_r in list(meyer.columns): \n",
    "      c = c_r # rename it and continue thru the loop\n",
    "\n",
    "  # if that column name is in the list, or if c_r is, do this\n",
    "  a = list(meyer[c].unique())\n",
    "  for i in v:\n",
    "    if i in a:\n",
    "      column_jail.append((c, v, i))\n",
    "      break\n",
    "  columns_done.append(c)\n",
    "  z+=1\n",
    "z==len(problem_children.keys())\n",
    "\n",
    "column_jail\n",
    "# oh it's just [('w1q114_ei', [6, 7, 8, 9, 97], 6)]\n",
    "meyer['w1q114_ei'].value_counts(dropna = False)\n",
    "# w1q114_ei\n",
    "# 0.0     1146\n",
    "# 1.0      251\n",
    "# 3.0       32\n",
    "# 2.0       30\n",
    "# 6.0       17\n",
    "# 4.0       14\n",
    "# 5.0        9\n",
    "# 11.0       6\n",
    "# 16.0       1\n",
    "# 21.0       1\n",
    "# Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's totally fine, I fixed the thing I wanted to fix with 6+\n",
    "# I should probably indicate that I changed stuff though.\n",
    "\n",
    "meyer.shape # (1507, 233)\n",
    "meyer[['w1q114_ei_r']] = meyer[['w1q114_ei']]\n",
    "meyer.shape\n",
    "meyer.drop(columns = ['w1q114_ei'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "\n",
    "# ok!  Woohoo!  Let's save another copy.\n",
    "meyer.to_csv('2024-06-06_non-NA-NAs-fixed.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 3 in full:\n",
    "2024-06-06_y-variable.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thursday, June 6, 2024\n",
    "\n",
    "# I should probably also choose a y variable.  [several hours latere]  Let's go with\n",
    "# Kessler6!  It's a general mental health scale.  According to the documentation, there were\n",
    "# 1491 complete cases of this, and honestly I'd be fine just dropping the rest.  Let me see\n",
    "# how many missing values there were per row in the imputed versions.  If it's only 1, I \n",
    "# might let it slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oh no is it skewed\n",
    "# Plot a histogram of it\n",
    "plt.figure(figsize = (16, 9));\n",
    "plt.hist(meyer['w1kessler6_i'], bins = 'auto', color = 'purple');\n",
    "plt.suptitle(f'Distribution of {Y}', size = 24)\n",
    "# plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "plt.xlabel('w1kessler6_i', size = 20);\n",
    "plt.ylabel('Frequency', size = 20);\n",
    "plt.xticks(size = 16, rotation = 60);\n",
    "plt.yticks(size = 16)\n",
    "#plt.tight_layout()\n",
    "# plt.savefig(f'./{a}/{i}_histogram.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot a histogram of it\n",
    "plt.figure(figsize = (16, 9));\n",
    "plt.hist(meyer['w1kessler6_i']**(1/2), bins = 'auto', color = 'purple');\n",
    "plt.suptitle(f'Distribution of {Y}', size = 24)\n",
    "# plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "plt.xlabel('w1kessler6_i', size = 20);\n",
    "plt.ylabel('Frequency', size = 20);\n",
    "plt.xticks(size = 16, rotation = 60);\n",
    "plt.yticks(size = 16)\n",
    "#plt.tight_layout()\n",
    "# plt.savefig(f'./{a}/{i}_histogram.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot a histogram of it\n",
    "plt.figure(figsize = (16, 9));\n",
    "plt.hist(meyer['w1kessler6_i']**(1/3), bins = 'auto', color = 'purple');\n",
    "plt.suptitle(f'Distribution of {Y}', size = 24)\n",
    "# plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "plt.xlabel('w1kessler6_i', size = 20);\n",
    "plt.ylabel('Frequency', size = 20);\n",
    "plt.xticks(size = 16, rotation = 60);\n",
    "plt.yticks(size = 16)\n",
    "#plt.tight_layout()\n",
    "# plt.savefig(f'./{a}/{i}_histogram.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot a histogram of it\n",
    "\n",
    "# log transformation gets a div by 0 warning\n",
    "# plt.figure(figsize = (16, 9));\n",
    "# plt.hist(np.log(meyer['w1kessler6_i']), bins = 'auto', color = 'purple');\n",
    "# plt.suptitle(f'Distribution of {Y}', size = 24)\n",
    "# # plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "# plt.xlabel('w1kessler6_i', size = 20);\n",
    "# plt.ylabel('Frequency', size = 20);\n",
    "# plt.xticks(size = 16, rotation = 60);\n",
    "# plt.yticks(size = 16)\n",
    "# #plt.tight_layout()\n",
    "# # plt.savefig(f'./{a}/{i}_histogram.png')\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# QQplot of raw kessler\n",
    "sm.qqplot(meyer['w1kessler6_i'], line='45');\n",
    "plt.suptitle(f'QQ-Plot of kessler6_i', size = 20);\n",
    "plt.xticks(size = 14, rotation = 60);\n",
    "plt.yticks(size = 14);\n",
    "plt.tight_layout();\n",
    "# plt.savefig('./03_images/output/dropout_rate_qqplot.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# QQplot of sqrt kessler\n",
    "sm.qqplot((meyer['w1kessler6_i']**0.5), line='45');\n",
    "plt.suptitle(f'QQ-Plot of kessler6_i', size = 20);\n",
    "plt.xticks(size = 14, rotation = 60);\n",
    "plt.yticks(size = 14);\n",
    "plt.tight_layout();\n",
    "# plt.savefig('./03_images/output/dropout_rate_qqplot.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# QQplot of cbrt kessler\n",
    "sm.qqplot((meyer['w1kessler6_i']**(1/3)), line='45');\n",
    "plt.suptitle(f'QQ-Plot of kessler6_i', size = 20);\n",
    "plt.xticks(size = 14, rotation = 60);\n",
    "plt.yticks(size = 14);\n",
    "plt.tight_layout();\n",
    "# plt.savefig('./03_images/output/dropout_rate_qqplot.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Ok, I'll square root it.\n",
    "\n",
    "meyer['kessler6_sqrt'] = (meyer['w1kessler6_i']**0.5)\n",
    "meyer.shape\n",
    "\n",
    "# let's get a copy of that too\n",
    "meyer.to_csv('2024-06-06_sqrt-kessler.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 4 in full:\n",
    "2024-06-06_column_recoding_for_feat_eng_dict_combinations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite Scale Calculations\n",
    "\n",
    "# In this script, I need to go back to `feat_eng_dict`, that list I made in \n",
    "# `2024-05-23_thru_2024-05-30_exploring_Meyer_2023_dataset.py`, and use it to \n",
    "# combine some of these columns into composite columns and drop the rest.\n",
    "\n",
    "unaltered_col_names = [\n",
    "  c.replace('_r', '').replace('_ei', '') for c in list(meyer.columns)]\n",
    "  \n",
    "for k, v in feat_eng_dict.items():\n",
    "  a = v[1:]\n",
    "  for j in a:\n",
    "    if ((j not in unaltered_col_names) & (j not in list(meyer.columns))):\n",
    "      print(k, j)\n",
    "\n",
    "# redefine it here without those\n",
    "feat_eng_dict = {'pers_well_being': ['sum', 'w1q01'],\n",
    "  'neighb_welcoming': ['mean', 'w1q19a', 'w1q19b', 'w1q19c', 'w1q19d'],\n",
    "  'age_awakening': ['min','w1q45', 'w1q46', 'w1q47', 'w1q48'],\n",
    "  'age_out': ['min', 'w1q49', 'w1q50', 'w1q51'],\n",
    "  'health_insurance': ['binarize', 'w1q64_1', 'w1q64_2', 'w1q64_3', 'w1q64_4', \n",
    "    'w1q64_5', 'w1q64_6', 'w1q64_7', 'w1q64_8', 'w1q64_9', 'w1q64_10', \n",
    "    'w1q64_11', 'w1q64_12', 'w1q64_13', 'w1q64_t_num'], \n",
    "  'serious_health_cond': ['binarize', 'w1q74_5', 'w1q74_6', 'w1q74_10', \n",
    "    'w1q74_11', 'w1q74_14', 'w1q74_17', 'w1q74_18', 'w1q74_20'], \n",
    "  'disabled': ['binarize', 'w1q75', 'w1q76'],\n",
    "  'suicidal_ideation': ['sum', 'w1q101', 'w1q105', 'w1q109'], \n",
    "  'suicide_attempts': ['recode', 'w1q113', 'w1q114'],\n",
    "  'outness': ['sum', 'w1q123a', 'w1q123b', 'w1q123c', 'w1q123d', 'w1q124'],\n",
    "  'abusive_treatment': ['sum', 'w1q135a', 'w1q135b', \n",
    "    'w1q135c', 'w1q135d', 'w1q135e', 'w1q135f'],\n",
    "  'work_neg_outcomes': ['recode', 'w1q137', 'w1q138'], # account for age\n",
    "  'abus_treat_non_queer': ['binarize', 'w1q136_1', 'w1q136_5', 'w1q136_6', \n",
    "    'w1q136_8', 'w1q136_9', 'w1q136_10'],\n",
    "  'stress_past_year_gen': ['recode', 'w1q142a', 'w1q142h', 'w1q142i'],\n",
    "  'stress_past_year_work': ['recode', 'w1q142b', 'w1q142c', 'w1q142e'],\n",
    "  'stress_past_year_interpersonal': ['recode', 'w1q142d', 'w1q142f', 'w1q142g'],\n",
    "  'stress_past_year_crime': ['recode', 'w1q142j', 'w1q142k'],\n",
    "  'work_disc_non_queer': ['binarize', 'w1q139_1', 'w1q139_5', 'w1q139_6', \n",
    "    'w1q139_8', 'w1q139_9', 'w1q139_10'],\n",
    "  'housing_disc_non_queer': ['binarize', 'w1q141_1', 'w1q141_5', 'w1q141_6', \n",
    "    'w1q141_8', 'w1q141_9', 'w1q141_10'],\n",
    "  'stress_past_year_non_queer': ['binarize', 'w1q143_1', 'w1q143_5', 'w1q143_6', \n",
    "    'w1q143_8', 'w1q143_9', 'w1q143_10'],\n",
    "  'daily_discr_non_queer': ['binarize', 'w1q145_1', 'w1q145_5', 'w1q145_6'],\n",
    "  'childhd_bullying_non_queer': ['binarize', 'w1q163_1', 'w1q163_5', 'w1q163_6', \n",
    "    'w1q163_8', 'w1q163_9', 'w1q163_10'],\n",
    "  'abus_treat_sex_gender': ['binarize', 'w1q136_2', 'w1q136_3', 'w1q136_4'],\n",
    "  'work_disc_sex_gender': ['binarize', 'w1q139_2', 'w1q139_3', 'w1q139_4'],\n",
    "  'housing_disc_sex_gender': ['binarize', 'w1q141_2', 'w1q141_3', 'w1q141_4'],\n",
    "  'stress_past_year_sex_gender': ['binarize', 'w1q143_2', 'w1q143_3', 'w1q143_4'],\n",
    "  'daily_discr_sex_gender': ['binarize', 'w1q145_2', 'w1q145_3', 'w1q145_4', \n",
    "    'w1q145_8', 'w1q145_9', 'w1q145_10'],\n",
    "  'childhd_bullying_sex_gender': ['binarize', 'w1q163_2', 'w1q163_3', 'w1q163_4'],\n",
    "  'religiosity': ['recode', 'w1q179', 'w1q180', 'w1q181'], \n",
    "  'chronic_strain': ['sum', 'w1q146a', 'w1q146b', 'w1q146c', 'w1q146d', 'w1q146e', \n",
    "    'w1q146f', 'w1q146g', 'w1q146h', 'w1q146i', 'w1q146j', 'w1q146k', 'w1q146l']}\n",
    "\n",
    "# then go back up and check again\n",
    "# got them all!\n",
    "\n",
    "# Now I'm going to go through and find which ones have NOT been altered\n",
    "for k, v in feat_eng_dict.items():\n",
    "  a = v[1:]\n",
    "  for j in a:\n",
    "    if j in list(meyer.columns):\n",
    "      print(k, j)\n",
    "\n",
    "# oh dear just one\n",
    "for k, v in feat_eng_dict.items():\n",
    "  a = v[:1]\n",
    "  b = ['_'.join([x, 'ei']) for x in v[1:]]\n",
    "  c = a + b\n",
    "  feat_eng_dict[k] = c\n",
    "  \n",
    "# manually fix this one\n",
    "feat_eng_dict['health_insurance'] = ['binarize', 'w1q64_1_ei', \n",
    "  'w1q64_2_ei', 'w1q64_3_ei', 'w1q64_4_ei', 'w1q64_5_ei', 'w1q64_6_ei', \n",
    "  'w1q64_7_ei', 'w1q64_8_ei', 'w1q64_9_ei', 'w1q64_10_ei', \n",
    "  'w1q64_11_ei', 'w1q64_12_ei', 'w1q64_13_ei', 'w1q64_t_num']\n",
    "  \n",
    "# and these\n",
    "feat_eng_dict['suicide_attempts'] = ['recode', 'w1q113_ei', 'w1q114_ei_r']\n",
    "feat_eng_dict['outness'] = ['sum', 'w1q123a_ei_r', \n",
    "  'w1q123b_ei_r', 'w1q123c_ei_r', 'w1q123d_ei_r', 'w1q124_ei']\n",
    "  \n",
    "# check again\n",
    "for k, v in feat_eng_dict.items():\n",
    "  a = v[1:]\n",
    "  for j in a:\n",
    "    if j not in list(meyer.columns):\n",
    "      print(k, j)\n",
    "      \n",
    "      \n",
    "for k, v in feat_eng_dict.items():\n",
    "  print(k, v)\n",
    "  print('')\n",
    "\n",
    "feat_eng_dict['health_insurance'] = ['binarize', 'w1q64_2_ei', \n",
    "  'w1q64_3_ei', 'w1q64_4_ei', 'w1q64_5_ei', 'w1q64_6_ei', \n",
    "  'w1q64_7_ei', 'w1q64_8_ei', 'w1q64_9_ei', 'w1q64_10_ei', \n",
    "  'w1q64_11_ei', 'w1q64_12_ei', 'w1q64_13_ei', 'w1q64_t_num']\n",
    "\n",
    "# cut\n",
    "drop_items = ['pers_well_being', 'age_awakening', 'age_out', 'neighb_welcoming']\n",
    "drop_cols = ['w1q45_ei', 'w1q46_ei', 'w1q47_ei', 'w1q48_ei', 'w1q49_ei', 'w1q50_ei', \n",
    "  'w1q51_ei']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mess with the columns before computing\n",
    "\n",
    "def recode(dictry, name, cols):\n",
    "  '''cols is a list of columns that need to be recoded.  \n",
    "  name is the name I want to give the composite column based on cols\n",
    "  dict is the STRING name of the dictionary (I need that for the func to modify it)\n",
    "    this function will spit out the appropriate syntax, but only for THIS df.'''\n",
    "  for i in cols[1:]:\n",
    "    print(f\"meyer[['{i}_r']] = meyer[['{i}']]\")\n",
    "  print('')\n",
    "  for i in cols[1:]:\n",
    "    print(f\"meyer['{i}'].value_counts(dropna = False).sort_index()\")\n",
    "    print(f\"meyer['{i}_r'].value_counts(dropna = False).sort_index()\")\n",
    "  print('')\n",
    "  print(f\"meyer.drop(columns = {cols[1:]}).shape\")\n",
    "  print(f\"meyer.drop(columns = {cols[1:]}, inplace = True)\")\n",
    "  print('')\n",
    "  cols_r = cols[:1] + [''.join([x, '_r']) for x in cols[1:]]\n",
    "  print(f\"{dictry}['{name}'] = {cols_r}\")\n",
    "# I then ran this a bunch of times to generate the code seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse code these so that 1=bad neighborhood and 0=fine\n",
    "meyer[['w1q19a_ei_r']] = meyer[['w1q19a_ei']]-1\n",
    "meyer[['w1q19b_ei_r']] = meyer[['w1q19b_ei']]-1\n",
    "meyer[['w1q19c_ei_r']] = meyer[['w1q19c_ei']]-1\n",
    "meyer[['w1q19d_ei_r']] = meyer[['w1q19d_ei']]-1\n",
    "meyer.drop(columns = ['w1q19a_ei', 'w1q19b_ei', 'w1q19c_ei', 'w1q19d_ei']).shape\n",
    "meyer.drop(columns = ['w1q19a_ei', 'w1q19b_ei', 'w1q19c_ei', 'w1q19d_ei'], inplace = True)\n",
    "feat_eng_dict['bad_neighbhd'] = ['sum', 'w1q19a_ei_r', 'w1q19b_ei_r', 'w1q19c_ei_r', 'w1q19d_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse code these so that 1=disabled, 0=non-disabled\n",
    "# currently it's 1=disabled, 2=non-disabled\n",
    "# (1-2)*(-1)==1, (2-2)*(-1)==0\n",
    "meyer[['w1q75_ei_r']] = abs(meyer[['w1q75_ei']]-2)\n",
    "meyer[['w1q76_ei_r']] = abs(meyer[['w1q76_ei']]-2)\n",
    "\n",
    "meyer['w1q75_ei'].value_counts(dropna = False, sort = True, ascending = True)\n",
    "meyer['w1q75_ei_r'].value_counts(dropna = False, sort = True, ascending = True)\n",
    "meyer['w1q76_ei'].value_counts(dropna = False, sort = True, ascending = True)\n",
    "meyer['w1q76_ei_r'].value_counts(dropna = False, sort = True, ascending = True)\n",
    "\n",
    "meyer.drop(columns = ['w1q75_ei', 'w1q76_ei']).shape\n",
    "meyer.drop(columns = ['w1q75_ei', 'w1q76_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['disabled'] = ['binarize', 'w1q75_ei_r', 'w1q76_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-base\n",
    "meyer[['w1q101_ei_r']] = meyer[['w1q101_ei']]-1\n",
    "meyer[['w1q105_ei_r']] = meyer[['w1q105_ei']]-1\n",
    "meyer[['w1q109_ei_r']] = meyer[['w1q109_ei']]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And with that done, the first column is redundant!\n",
    "# This entry in the dictionary now only has one column in its\n",
    "# composite, and I actually think it would be better to combine\n",
    "# it (w1q114_ei) with the previous one, for an overall 'suicidality' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_eng_dict['suicide_attempts'] = ['recode', 'w1q113_ei_r', 'w1q114_ei_r_r']\n",
    "drop_items.append('suicidal_ideation')\n",
    "drop_items.append('suicide_attempts')\n",
    "feat_eng_dict['suicidality'] = ['sum', 'w1q101_ei_r', 'w1q105_ei_r', 'w1q109_ei_r', 'w1q114_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I already got the 123s, but I need to reverse code 124\n",
    "# here's what it is now -> what I want it to be\n",
    "# MOST VISIBLE\n",
    "# 1 -> \n",
    "# 2 -> \n",
    "# 3 -> \n",
    "# 4 -> 1\n",
    "# 5 -> 0\n",
    "# LEAST VISIBLE\n",
    "# oh it's just 5 minus thing\n",
    "\n",
    "meyer[['w1q124_ei_r']] = 5-meyer[['w1q124_ei']]\n",
    "\n",
    "meyer['w1q123a_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q123b_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q123c_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q123d_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q124_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q124_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q124_ei']).shape\n",
    "meyer.drop(columns = ['w1q124_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['outness'] = ['mean', 'w1q123a_ei_r', 'w1q123b_ei_r', \n",
    "  'w1q123c_ei_r', 'w1q123d_ei_r', 'w1q124_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these guys I just need to 0-base\n",
    "meyer[['w1q135a_ei_r']] = meyer[['w1q135a_ei']]-1\n",
    "meyer[['w1q135b_ei_r']] = meyer[['w1q135b_ei']]-1\n",
    "meyer[['w1q135c_ei_r']] = meyer[['w1q135c_ei']]-1\n",
    "meyer[['w1q135d_ei_r']] = meyer[['w1q135d_ei']]-1\n",
    "meyer[['w1q135e_ei_r']] = meyer[['w1q135e_ei']]-1\n",
    "meyer[['w1q135f_ei_r']] = meyer[['w1q135f_ei']]-1\n",
    "\n",
    "meyer['w1q135a_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135a_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135b_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135b_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135c_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135c_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135d_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135d_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135e_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135e_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135f_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135f_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q135a_ei', 'w1q135b_ei', 'w1q135c_ei', 'w1q135d_ei', 'w1q135e_ei', 'w1q135f_ei']).shape\n",
    "meyer.drop(columns = ['w1q135a_ei', 'w1q135b_ei', 'w1q135c_ei', 'w1q135d_ei', 'w1q135e_ei', 'w1q135f_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['abusive_treatment'] = ['sum', 'w1q135a_ei_r', 'w1q135b_ei_r', 'w1q135c_ei_r', 'w1q135d_ei_r', 'w1q135e_ei_r', 'w1q135f_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same \n",
    "meyer[['w1q137_ei_r']] = meyer[['w1q137_ei']]-1\n",
    "meyer[['w1q138_ei_r']] = meyer[['w1q138_ei']]-1\n",
    "\n",
    "meyer['w1q137_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q137_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q138_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q138_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q137_ei', 'w1q138_ei']).shape\n",
    "meyer.drop(columns = ['w1q137_ei', 'w1q138_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['work_neg_outcomes'] = ['sum', 'w1q137_ei_r', 'w1q138_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode code these guys so 1=stress and 0=not\n",
    "meyer[['w1q142a_ei_r']] = abs(meyer[['w1q142a_ei']]-2)\n",
    "meyer[['w1q142h_ei_r']] = abs(meyer[['w1q142h_ei']]-2)\n",
    "meyer[['w1q142i_ei_r']] = abs(meyer[['w1q142i_ei']]-2)\n",
    "\n",
    "meyer['w1q142a_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142a_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142h_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142h_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142i_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142i_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q142a_ei', 'w1q142h_ei', 'w1q142i_ei']).shape\n",
    "meyer.drop(columns = ['w1q142a_ei', 'w1q142h_ei', 'w1q142i_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['stress_past_year_gen'] = ['sum', 'w1q142a_ei_r', 'w1q142h_ei_r', 'w1q142i_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these too\n",
    "meyer[['w1q142b_ei_r']] = abs(meyer[['w1q142b_ei']]-2)\n",
    "meyer[['w1q142c_ei_r']] = abs(meyer[['w1q142c_ei']]-2)\n",
    "meyer[['w1q142e_ei_r']] = abs(meyer[['w1q142e_ei']]-2)\n",
    "\n",
    "meyer['w1q142b_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142b_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142c_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142c_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142e_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142e_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q142b_ei', 'w1q142c_ei', 'w1q142e_ei']).shape\n",
    "meyer.drop(columns = ['w1q142b_ei', 'w1q142c_ei', 'w1q142e_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['stress_past_year_work'] = ['sum', 'w1q142b_ei_r', 'w1q142c_ei_r', 'w1q142e_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and these\n",
    "meyer[['w1q142d_ei_r']] = abs(meyer[['w1q142d_ei']]-2)\n",
    "meyer[['w1q142f_ei_r']] = abs(meyer[['w1q142f_ei']]-2)\n",
    "meyer[['w1q142g_ei_r']] = abs(meyer[['w1q142g_ei']]-2)\n",
    "\n",
    "meyer['w1q142d_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142d_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142f_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142f_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142g_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142g_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q142d_ei', 'w1q142f_ei', 'w1q142g_ei']).shape\n",
    "meyer.drop(columns = ['w1q142d_ei', 'w1q142f_ei', 'w1q142g_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['stress_past_year_interpersonal'] = ['sum', 'w1q142d_ei_r', 'w1q142f_ei_r', 'w1q142g_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and these\n",
    "meyer[['w1q142j_ei_r']] = abs(meyer[['w1q142j_ei']]-2)\n",
    "meyer[['w1q142k_ei_r']] = abs(meyer[['w1q142k_ei']]-2)\n",
    "\n",
    "print(meyer['w1q142j_ei'].value_counts(dropna = False).sort_index())\n",
    "print(meyer['w1q142j_ei_r'].value_counts(dropna = False).sort_index())\n",
    "print(meyer['w1q142k_ei'].value_counts(dropna = False).sort_index())\n",
    "print(meyer['w1q142k_ei_r'].value_counts(dropna = False).sort_index())\n",
    "\n",
    "meyer.drop(columns = ['w1q142j_ei', 'w1q142k_ei']).shape\n",
    "meyer.drop(columns = ['w1q142j_ei', 'w1q142k_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['stress_past_year_crime'] = ['sum', 'w1q142j_ei_r', 'w1q142k_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# religiosity \n",
    "\n",
    "# here's what 179 is now; 1 and 2 are massive in 180\n",
    "# 1 Protestant (for example, Baptist, Methodist) 295 19.4 %\n",
    "# 2 Roman Catholic 133 8.8 %\n",
    "# 3 Mormon (Church of Jesus Christ of Latter-day Saints or LDS) 10 0.7 %\n",
    "# 4 Orthodox (Greek, Russian, or another Orthodox church) 6 0.4 %\n",
    "\n",
    "# 5 Jewish 38 2.5 %\n",
    "# 6 Muslim 3 0.2 %\n",
    "# 7 Buddhist 30 2.0 %\n",
    "# 8 Hindu 1 0.1 %\n",
    "# 11 Spiritual 262 17.3 %\n",
    "# 12 Something else 95 6.3 %\n",
    "\n",
    "# 9 Atheist (do not believe in God) 192 12.6 %\n",
    "# 10 Agnostic (not sure if there is a God) 156 10.3 %\n",
    "# 13 Nothing in particular 273 18.0 %\n",
    "\n",
    "# collapse 3-8 into \"other organized\"\n",
    "# .... actually collapse more, bcz this needs to be OHE'd\n",
    "\n",
    "# 1-4 christian-influenced religious     1\n",
    "# 5-8, 11-12  non-christian religious    5\n",
    "# 9-10, 13 not religious                 make this 0, because it's literally none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode the religion columns\n",
    "relig_recode = {1: 1, 2: 1, 3: 1, 4: 1, # vaguely christian\n",
    "  5: 5, 6: 5, 7: 5, 8: 5, 11: 5, 12: 5, # religious but not christian\n",
    "  9: 0, 10: 0, 13: 0} # not religious\n",
    "\n",
    "# 179 and 180 are the \"are you religious / were you raised religious\" Qs\n",
    "meyer['w1q179_ei_r'] = meyer['w1q179_ei'].map(relig_recode)\n",
    "meyer['w1q180_ei_r'] = meyer['w1q180_ei'].map(relig_recode)\n",
    "\n",
    "print(meyer['w1q179_ei'].value_counts(dropna = False).sort_index())\n",
    "print(meyer['w1q179_ei_r'].value_counts(dropna = False).sort_index())\n",
    "print(meyer['w1q180_ei'].value_counts(dropna = False).sort_index())\n",
    "print(meyer['w1q180_ei_r'].value_counts(dropna = False).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "meyer.drop(columns = ['w1q179_ei', 'w1q180_ei']).shape\n",
    "meyer.drop(columns = ['w1q179_ei', 'w1q180_ei'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "# I actually cannot think of a meaningful way to combine these.I think the \n",
    "# thing to do is just leave them all in alone, then see what pops as meaningful.\n",
    "drop_items.append('religiosity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 181 is about how often you attend religious services\n",
    "meyer[['w1q181_ei_r']] = 6-(meyer[['w1q181_ei']])\n",
    "\n",
    "meyer['w1q181_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q181_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q181_ei']).shape\n",
    "meyer.drop(columns = ['w1q181_ei'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "# woohoo!  save it to a csv\n",
    "meyer.to_csv('2024-06-06_recodes-for-feat-eng-dict-done_not-ohe-yet.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just ohe them now while I'm thinking about it\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(drop = None, # I want to manually drop a specific one\n",
    "  handle_unknown = 'ignore', sparse_output = False) \n",
    "\n",
    "ctx = ColumnTransformer(transformers=[('one_hot', ohe, ['w1q179_ei_r', 'w1q180_ei_r'])],\n",
    "    remainder = 'passthrough', verbose_feature_names_out=False)\n",
    "\n",
    "meyer_ohe = pd.DataFrame(data = ctx.fit_transform(meyer), \n",
    "  columns = ctx.get_feature_names_out())\n",
    "meyer_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the non-religious ones\n",
    "meyer_ohe.drop(columns = ['w1q179_ei_r_0', 'w1q180_ei_r_0']).shape\n",
    "meyer_ohe.drop(columns = ['w1q179_ei_r_0', 'w1q180_ei_r_0'], inplace = True)\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the new columns more semantic names\n",
    "relig_rename = {'w1q179_ei_r_1': 'w1q179_ei_r_relig_christ', \n",
    "  'w1q179_ei_r_5': 'w1q179_ei_r_relig_other', \n",
    "  'w1q180_ei_r_1': 'w1q180_ei_r_relig_christ', \n",
    "  'w1q180_ei_r_5': 'w1q180_ei_r_relig_other'}\n",
    "\n",
    "meyer_ohe.rename(columns = relig_rename, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it back in the right name\n",
    "meyer = meyer_ohe.copy(deep = True)\n",
    "\n",
    "# drop the columns I set aside before\n",
    "meyer.drop(columns = drop_cols, inplace = True)\n",
    "\n",
    "# save to csv\n",
    "meyer.to_csv('2024-06-06_recodes-for-feat-eng-dict-done_religious-ohe-done.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finish updating the feat_eng_list\n",
    "\n",
    "# safety first\n",
    "feat_eng_backup = feat_eng_dict.copy()\n",
    "\n",
    "for i in drop_items:\n",
    "  del feat_eng_dict[i]\n",
    "# thanks to stack overflow for the knowledge that that's how to del dict entries\n",
    "# https://stackoverflow.com/questions/5844672/delete-an-element-from-a-dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oh one more thing\n",
    "# Reorder the columns\n",
    "ordered_cols = sorted(list(meyer.columns))\n",
    "ordered_cols.remove('studyid')\n",
    "ordered_cols = ['studyid'] + ordered_cols\n",
    "len(ordered_cols) # 228\n",
    "len(list(meyer.columns)) # 228\n",
    "meyer.shape # (1494, 228)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meyer = meyer[ordered_cols]\n",
    "meyer.shape\n",
    "\n",
    "# save to csv\n",
    "meyer.to_csv('2024-06-06_recodes-for-feat-eng-dict-done_religious-ohe-done_reordered.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 5 in full:\n",
    "2024-06-06_updated_feat_eng_dict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thursday, June 6, 2024 - End of day\n",
    "# updated version of my feat_eng_dict\n",
    "\n",
    "feat_eng_dict = {\n",
    "  'health_insurance': [\n",
    "    'binarize', 'w1q64_2_ei', 'w1q64_3_ei', 'w1q64_4_ei', 'w1q64_5_ei', \n",
    "    'w1q64_6_ei', 'w1q64_7_ei', 'w1q64_8_ei', 'w1q64_9_ei', 'w1q64_10_ei', \n",
    "    'w1q64_11_ei', 'w1q64_12_ei', 'w1q64_13_ei', 'w1q64_t_num'], \n",
    "  'serious_health_cond': [\n",
    "    'binarize', 'w1q74_5_ei', 'w1q74_6_ei', 'w1q74_10_ei', 'w1q74_11_ei', \n",
    "    'w1q74_14_ei', 'w1q74_17_ei', 'w1q74_18_ei', 'w1q74_20_ei'], \n",
    "  'disabled': [\n",
    "    'binarize', 'w1q75_ei_r', 'w1q76_ei_r'], \n",
    "  'outness': [\n",
    "    'mean', 'w1q123a_ei_r', 'w1q123b_ei_r', 'w1q123c_ei_r', 'w1q123d_ei_r',\n",
    "    'w1q124_ei_r'], \n",
    "  'abusive_treatment': [\n",
    "    'sum', 'w1q135a_ei_r', 'w1q135b_ei_r', 'w1q135c_ei_r', 'w1q135d_ei_r', \n",
    "    'w1q135e_ei_r', 'w1q135f_ei_r'], \n",
    "  'work_neg_outcomes': [\n",
    "    'sum', 'w1q137_ei_r', 'w1q138_ei_r'], \n",
    "  'abus_treat_non_queer': [\n",
    "    'binarize', 'w1q136_1_ei', 'w1q136_5_ei', 'w1q136_6_ei', 'w1q136_8_ei', \n",
    "    'w1q136_9_ei', 'w1q136_10_ei'], \n",
    "  'stress_past_year_gen': [\n",
    "    'sum', 'w1q142a_ei_r', 'w1q142h_ei_r', 'w1q142i_ei_r'], \n",
    "  'stress_past_year_work': [\n",
    "    'sum', 'w1q142b_ei_r', 'w1q142c_ei_r', 'w1q142e_ei_r'], \n",
    "  'stress_past_year_interpersonal': [\n",
    "    'sum', 'w1q142d_ei_r', 'w1q142f_ei_r', 'w1q142g_ei_r'], \n",
    "  'stress_past_year_crime': [\n",
    "    'sum', 'w1q142j_ei_r', 'w1q142k_ei_r'], \n",
    "  'work_disc_non_queer': [\n",
    "    'binarize', 'w1q139_1_ei', 'w1q139_5_ei', 'w1q139_6_ei', 'w1q139_8_ei', \n",
    "    'w1q139_9_ei', 'w1q139_10_ei'], \n",
    "  'housing_disc_non_queer': [\n",
    "    'binarize', 'w1q141_1_ei', 'w1q141_5_ei', 'w1q141_6_ei', 'w1q141_8_ei', \n",
    "    'w1q141_9_ei', 'w1q141_10_ei'], \n",
    "  'stress_past_year_non_queer': [\n",
    "    'binarize', 'w1q143_1_ei', 'w1q143_5_ei', 'w1q143_6_ei', 'w1q143_8_ei', \n",
    "    'w1q143_9_ei', 'w1q143_10_ei'], \n",
    "  'daily_discr_non_queer': [\n",
    "    'binarize', 'w1q145_1_ei', 'w1q145_5_ei', 'w1q145_6_ei'], \n",
    "  'childhd_bullying_non_queer': [\n",
    "    'binarize', 'w1q163_1_ei', 'w1q163_5_ei', 'w1q163_6_ei', 'w1q163_8_ei', \n",
    "    'w1q163_9_ei', 'w1q163_10_ei'], \n",
    "  'abus_treat_sex_gender': [\n",
    "    'binarize', 'w1q136_2_ei', 'w1q136_3_ei', 'w1q136_4_ei'], \n",
    "  'work_disc_sex_gender': [\n",
    "    'binarize', 'w1q139_2_ei', 'w1q139_3_ei', 'w1q139_4_ei'], \n",
    "  'housing_disc_sex_gender': [\n",
    "    'binarize', 'w1q141_2_ei', 'w1q141_3_ei', 'w1q141_4_ei'], \n",
    "  'stress_past_year_sex_gender': [\n",
    "    'binarize', 'w1q143_2_ei', 'w1q143_3_ei', 'w1q143_4_ei'], \n",
    "  'daily_discr_sex_gender': [\n",
    "    'binarize', 'w1q145_2_ei', 'w1q145_3_ei', 'w1q145_4_ei', 'w1q145_8_ei', \n",
    "    'w1q145_9_ei', 'w1q145_10_ei'], \n",
    "  'childhd_bullying_sex_gender': [\n",
    "    'binarize', 'w1q163_2_ei', 'w1q163_3_ei', 'w1q163_4_ei'], \n",
    "  'chronic_strain': [\n",
    "    'sum', 'w1q146a_ei', 'w1q146b_ei', 'w1q146c_ei', 'w1q146d_ei', 'w1q146e_ei', \n",
    "    'w1q146f_ei', 'w1q146g_ei', 'w1q146h_ei', 'w1q146i_ei', 'w1q146j_ei', \n",
    "    'w1q146k_ei', 'w1q146l_ei'], \n",
    "  'bad_neighbhd': [\n",
    "    'sum', 'w1q19a_ei_r', 'w1q19b_ei_r', 'w1q19c_ei_r', 'w1q19d_ei_r'], \n",
    "  'suicidality': ['sum', 'w1q101_ei_r', 'w1q105_ei_r', 'w1q109_ei_r', 'w1q114_ei_r']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Friday, June 7, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today I finally got to run my `feat_eng_dict`!  I made some changes to it, and just summed up a lot of the things I was originally going to binarize.  I wanted to get a good look at them to see if there were differences based on the extent to which whatever the thing is has happened to someone, and in a handful of cases there were, but for the most part I think I would have been better off binarizing them.  I will try to do that in the future.  **Throughout the course of this evening, I have been thinking that I have to treat all of these columns the same way.  At this moment, I no longer think that's true.**  I think it's probably fine to say, \"I looked at these guys, and some of them had magnitude effects and some of them didn't.  I binarized the ones that didn't, and kept the others.\"\n",
    "\n",
    "I edited my autoplots function (`autoplots_v2.py`) to include qqplots.  This makes it take even longer to run and the output folder even more comically large, but it is handy.\n",
    "\n",
    "I then conducted some exploratory data analysis, mostly through the production of graphs.  These graphs can be found in the images folder.  In addition to the frequency distributions of each variable and their relationship (shown through scatterplots) with the target variable, I also created qqplots for each variable, and for 3 transformed versions of each variable.  The purpose of this was to assess each variable's normality, and determine whether any of them would benefit from a linear transformation.  I ultimately decided that this was not likely to useful for any variable other than the target, which is the square root of the Kessler scores.\n",
    "\n",
    "Look at these plots also revealed a number of variables that still need to be one-hot encoded, 0-based, or otherwise modified.  I would also like to rename the columns to be more intuitive, so that I don't have to contiually refer to the documentation.  I opted not to do all that before running my first model, but it would be good to do it before including any of those variables in the model.  I started several lists to do these cleaning tasks, but did not finish any of them.  These lists can be found in `2024-06-07-eda.py`, `2024-06-07_NEW_feat_eng_dict.py`, and `2024-06-07_columns-i-want-to-screw-with-some-more-and-how.py`.  When I have time to address these cleaning issues, I intend to rename and modify these scripts.\n",
    "\n",
    "Finally, I created my first model!  I flipped through the scatterplots and just eyeballed some predictors that seemed reasonable, and put them all into a linear regression.  I conducted a train-test split for a little bit of a check on the model, to get a sense of how overfit it might be, but I kept the test size small because this model is mostly for inference.  I am not concerned about its applicability to new data in the future, because newer data is likely to be legitimately different than this data from 2016.  I do, however, want to keep an eye on overfitting, especially as I add more predictors to my model.  Dimensionality has been a concern throughout this entire process, and I am wary of allowing my model to coil itself too tightly around the data.  Although I am not concerned about this model generalizing perfectly to future data, I also do not want it to be so bespoke to the current data that it becomes unsuitable for interpretation.  In other words, if it becomes so overfit that it cannot even generalize to testing data from the same dataset, then its coefficients are unlikely to be meaningful for any level of analysis.\n",
    "\n",
    "I did some cross validation because it seemed like a good idea to demonstrate that I know how to do that, although without gridsearching over anything, I'm not sure it's really necessary.\n",
    "\n",
    "I used several metrics to evaluate my model, and I think it's a decent model for the area.  $R^2$ was around 0.6 for both training and testing, and all of the loss functions were <1.  (This makes sense, because the y-variable itself is a square root.  Squaring a float <1 will make it smaller, not larger.)  This implies that my model is usually within a range of +/-1 with its predictions of the square root of Kessler scores (range: 0-4.9), and within +/-0.5 for the re-squared values (range: 0-24).\n",
    "\n",
    "I honestly can't believe how good this thing is for being so quickly slapped together.\n",
    "\n",
    "The most up to date CSV is **2024-06-07_h21-m19-s52_end-of-day.csv**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other files created:\n",
    "- 2 folders worth of plots in `dsb318-capstone/03_images/02_all_graphs/`\n",
    "\n",
    "`.py` files created: \n",
    "1. 2024-06-07_set-up.py\n",
    "2. 2024-06-07_updated_feat_eng_dict.py\n",
    "3. 2024-06-07_column-combination-at-last.py\n",
    "4. 2024-06-07-eda.py\n",
    "5. 2024-06-07_NEW_feat_eng_dict.py\n",
    "6. 2024-06-07_columns-i-want-to-screw-with-some-more-and-how.py\n",
    "7. 2024-06-07_modeling-part-1.py\n",
    "8. autoplots_v2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 1 in full:\n",
    "1. 2024-06-07_set-up.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capstone Setup\n",
    "\n",
    "# Module Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, date, time\n",
    "from string import capwords\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working Directory\n",
    "os.getcwd()\n",
    "#os.chdir('C:/Users/emily/Git_Stuff/General_Assembly/04_Projects/project-capstone')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these right off the bat\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "\n",
    "# THIS IS THE ORIGINAL\n",
    "# meyer = pd.read_csv(\n",
    "#   './potential_datasets/2024_05_23_download_ICPSR_Meyer_2023_generations_data_attempt_2/ICPSR_37166/DS0007/37166-0007-Data.tsv', \n",
    "#   sep = '\\t', low_memory=False, na_values = ' ') # Thanks to ibrahim rupawala for highlighting the na_values argument\n",
    "#   # https://stackoverflow.com/questions/13445241/replacing-blank-values-white-space-with-nan-in-pandas/47105408#47105408\n",
    "\n",
    "# THIS IS THE MOST UP TO DATE VERSION FOR WORKING ON\n",
    "meyer = pd.read_csv('2024-06-06_recodes-for-feat-eng-dict-done_religious-ohe-done_reordered.csv')\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 2 in full:\n",
    "2. 2024-06-07_updated_feat_eng_dict.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thursday, June 6, 2024 - End of day\n",
    "# updated version of my feat_eng_dict\n",
    "\n",
    "feat_eng_dict = {\n",
    "  'health_insurance': [\n",
    "    'binarize', 'w1q64_2_ei', 'w1q64_3_ei', 'w1q64_4_ei', 'w1q64_5_ei', \n",
    "    'w1q64_6_ei', 'w1q64_7_ei', 'w1q64_8_ei', 'w1q64_9_ei', 'w1q64_10_ei', \n",
    "    'w1q64_11_ei', 'w1q64_12_ei', 'w1q64_13_ei', 'w1q64_t_num'], \n",
    "  'serious_health_cond': [\n",
    "    'binarize', 'w1q74_5_ei', 'w1q74_6_ei', 'w1q74_10_ei', 'w1q74_11_ei', \n",
    "    'w1q74_14_ei', 'w1q74_17_ei', 'w1q74_18_ei', 'w1q74_20_ei'], \n",
    "  'disabled': [\n",
    "    'binarize', 'w1q75_ei_r', 'w1q76_ei_r'], \n",
    "  'outness': [\n",
    "    'mean', 'w1q123a_ei_r', 'w1q123b_ei_r', 'w1q123c_ei_r', 'w1q123d_ei_r',\n",
    "    'w1q124_ei_r'], \n",
    "  'abusive_treatment': [\n",
    "    'sum', 'w1q135a_ei_r', 'w1q135b_ei_r', 'w1q135c_ei_r', 'w1q135d_ei_r', \n",
    "    'w1q135e_ei_r', 'w1q135f_ei_r'], \n",
    "  'work_neg_outcomes': [\n",
    "    'sum', 'w1q137_ei_r', 'w1q138_ei_r'], \n",
    "  'abus_treat_non_queer': [\n",
    "    'sum', 'w1q136_1_ei', 'w1q136_5_ei', 'w1q136_6_ei', 'w1q136_8_ei', \n",
    "    'w1q136_9_ei', 'w1q136_10_ei'], \n",
    "  'stress_past_year_gen': [\n",
    "    'sum', 'w1q142a_ei_r', 'w1q142h_ei_r', 'w1q142i_ei_r'], \n",
    "  'stress_past_year_work': [\n",
    "    'sum', 'w1q142b_ei_r', 'w1q142c_ei_r', 'w1q142e_ei_r'], \n",
    "  'stress_past_year_interpersonal': [\n",
    "    'sum', 'w1q142d_ei_r', 'w1q142f_ei_r', 'w1q142g_ei_r'], \n",
    "  'stress_past_year_crime': [\n",
    "    'sum', 'w1q142j_ei_r', 'w1q142k_ei_r'], \n",
    "  'work_disc_non_queer': [\n",
    "    'sum', 'w1q139_1_ei', 'w1q139_5_ei', 'w1q139_6_ei', 'w1q139_8_ei', \n",
    "    'w1q139_9_ei', 'w1q139_10_ei'], \n",
    "  'housing_disc_non_queer': [\n",
    "    'sum', 'w1q141_1_ei', 'w1q141_5_ei', 'w1q141_6_ei', 'w1q141_8_ei', \n",
    "    'w1q141_9_ei', 'w1q141_10_ei'], \n",
    "  'stress_past_year_non_queer': [\n",
    "    'sum', 'w1q143_1_ei', 'w1q143_5_ei', 'w1q143_6_ei', 'w1q143_8_ei', \n",
    "    'w1q143_9_ei', 'w1q143_10_ei'], \n",
    "  'daily_discr_non_queer': [\n",
    "    'sum', 'w1q145_1_ei', 'w1q145_5_ei', 'w1q145_6_ei'], \n",
    "  'childhd_bullying_non_queer': [\n",
    "    'sum', 'w1q163_1_ei', 'w1q163_5_ei', 'w1q163_6_ei', 'w1q163_8_ei', \n",
    "    'w1q163_9_ei', 'w1q163_10_ei'], \n",
    "  'abus_treat_sex_gender': [\n",
    "    'sum', 'w1q136_2_ei', 'w1q136_3_ei', 'w1q136_4_ei'], \n",
    "  'work_disc_sex_gender': [\n",
    "    'sum', 'w1q139_2_ei', 'w1q139_3_ei', 'w1q139_4_ei'], \n",
    "  'housing_disc_sex_gender': [\n",
    "    'sum', 'w1q141_2_ei', 'w1q141_3_ei', 'w1q141_4_ei'], \n",
    "  'stress_past_year_sex_gender': [\n",
    "    'sum', 'w1q143_2_ei', 'w1q143_3_ei', 'w1q143_4_ei'], \n",
    "  'daily_discr_sex_gender': [\n",
    "    'sum', 'w1q145_2_ei', 'w1q145_3_ei', 'w1q145_4_ei', 'w1q145_8_ei', \n",
    "    'w1q145_9_ei', 'w1q145_10_ei'], \n",
    "  'childhd_bullying_sex_gender': [\n",
    "    'sum', 'w1q163_2_ei', 'w1q163_3_ei', 'w1q163_4_ei'], \n",
    "  'chronic_strain': [\n",
    "    'sum', 'w1q146a_ei', 'w1q146b_ei', 'w1q146c_ei', 'w1q146d_ei', 'w1q146e_ei', \n",
    "    'w1q146f_ei', 'w1q146g_ei', 'w1q146h_ei', 'w1q146i_ei', 'w1q146j_ei', \n",
    "    'w1q146k_ei', 'w1q146l_ei'], \n",
    "  'bad_neighbhd': [\n",
    "    'sum', 'w1q19a_ei_r', 'w1q19b_ei_r', 'w1q19c_ei_r', 'w1q19d_ei_r'], \n",
    "  'suicidality': ['sum', 'w1q101_ei_r', 'w1q105_ei_r', 'w1q109_ei_r', 'w1q114_ei_r']}\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 3 in full:\n",
    "3. 2024-06-07_column-combination-at-last.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friday, June 7, 2024\n",
    "# Feature Engineering!  Whoo!\n",
    "\n",
    "# In this script, I will finally execute my feat_eng_dict to create\n",
    "# composite score columns and drop the individual parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to look at the values in each variable and triple check \n",
    "# that they're ready to go and that the method is appropriate.\n",
    "for k, v in feat_eng_dict.items():\n",
    "  print('='*20)\n",
    "  print(f'{k}: {v[0]}')\n",
    "  print('-'*20)\n",
    "  for i in v[1:]:\n",
    "    if i not in list(meyer.columns):\n",
    "      print(f'{i} not in columns')\n",
    "    else:\n",
    "      # print(f'Dtype: {meyer[[i]].dytpe}')\n",
    "      meyer[i].value_counts(dropna = False).sort_index()\n",
    "    print('-'*20)\n",
    "  print('='*20, '\\n'*2)\n",
    "# All good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I want to see a list of all the methods of combination.\n",
    "methods = []\n",
    "for v in feat_eng_dict.values():\n",
    "  if v[0] not in methods:\n",
    "    methods.append(v[0])\n",
    "methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok let's run this bad boy!!\n",
    "# (I'm so freaking excited I'm so proud of myself for \n",
    "# coming up with this and I'm so clever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meyer.shape # (1494, 228)\n",
    "cols_added = 0\n",
    "cols_done = []\n",
    "for k, v in feat_eng_dict.items():\n",
    "  # print('')\n",
    "  # print(k)\n",
    "  meyer[k] = meyer[v[1]]\n",
    "  cols_done.append(v[1])\n",
    "  cols_added += 1\n",
    "  for i in v[2:]:\n",
    "    # print('doing the sum')\n",
    "    meyer[k] += meyer[i]\n",
    "    cols_done.append(i)\n",
    "  if v[0]=='mean':\n",
    "    # print('doing the mean')\n",
    "    meyer[k] = meyer[k]/len(v[1:])\n",
    "  elif v[0]=='binarize': \n",
    "    # print('binarizing')\n",
    "    meyer[k] = np.where(meyer[k]>1, 1, meyer[k])\n",
    "  \n",
    "cols_added + 228 # 253\n",
    "meyer.shape # (1494, 253)\n",
    "len(cols_done) # 121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks\n",
    "\n",
    "# Binarization\n",
    "meyer['check_disabled'] = (meyer['w1q75_ei_r'] + meyer['w1q76_ei_r'])\n",
    "meyer.loc[(meyer['check_disabled']==2), 'check_disabled'] = 1 \n",
    "print(sum((meyer['check_disabled']-meyer['disabled'])!=0))\n",
    "print(sum(meyer['check_disabled']!=meyer['disabled']))\n",
    "\n",
    "# Sum\n",
    "meyer['check_suicidality'] = (meyer['w1q101_ei_r'] + meyer['w1q105_ei_r'] + meyer['w1q109_ei_r'] + meyer['w1q114_ei_r'])\n",
    "print(sum(meyer['check_suicidality']==meyer['suicidality']))\n",
    "print(sum(meyer['check_suicidality']-meyer['suicidality'])==0))\n",
    "\n",
    "# Mean\n",
    "meyer['check_outness'] = (meyer['w1q123a_ei_r'] + meyer['w1q123b_ei_r'] + meyer[\n",
    "  'w1q123c_ei_r'] + meyer['w1q123d_ei_r'] + meyer['w1q124_ei_r'])/len([\n",
    "  'mean', 'w1q123a_ei_r', 'w1q123b_ei_r', 'w1q123c_ei_r', 'w1q123d_ei_r', 'w1q124_ei_r'][1:])\n",
    "print(sum(meyer['check_outness']==meyer['outness']))\n",
    "print(sum(meyer['check_outness']-meyer['outness'])==0)\n",
    "\n",
    "# save a backup\n",
    "meyer.to_csv(f'{my_date()}_column-combination-done_no-drops_not-reordered_FIXED.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the component columns\n",
    "cols_done.append('check_outness')\n",
    "cols_done.append('check_suicidality')\n",
    "cols_done.append('check_disabled')\n",
    "228-121+25\n",
    "meyer.drop(columns = cols_done).shape # goal is 132\n",
    "meyer.drop(columns = cols_done, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the columns\n",
    "ordered_cols = sorted(list(meyer.columns))\n",
    "ordered_cols.remove('studyid')\n",
    "ordered_cols.remove('kessler6_sqrt')\n",
    "ordered_cols.remove('w1kessler6_i')\n",
    "ordered_cols = ['studyid', 'w1kessler6_i', 'kessler6_sqrt'] + ordered_cols\n",
    "ordered_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ordered_cols) # 132\n",
    "meyer.shape # (1494, 132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meyer = meyer[ordered_cols]\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save again\n",
    "meyer.to_csv(f'{my_date()}_column-combination-done_drops-done_reordered_FIXED.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 4 in full:\n",
    "4. 2024-06-07-eda.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friday, June 7, 2024\n",
    "# EDA\n",
    "\n",
    "# As usual, we're starting with my beloved autoplots - now with new arguments!\n",
    "\n",
    "autoplots(meyer, 'w1kessler6_i', qqplots=True, transform=True, line=False, verbose=False)\n",
    "autoplots(meyer, 'kessler6_sqrt', qqplots=True, transform=True, line=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a few more that I need to OHE, 0-base, or otherwise tinker with, but I'm going\n",
    "# to try to resist the urge to do that until I have SOME kind of model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meyer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meyer_corr = meyer.corr()\n",
    "\n",
    "meyer.shape\n",
    "meyer.to_csv(f'{my_date()}_end-of-day.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 5 in full:\n",
    "5. 2024-06-07_NEW_feat_eng_dict.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thursday, June 7, 2024\n",
    "# NEW feat_eng_dict\n",
    "\n",
    "# I thought I was so smart before, just getting a sum from these guys.\n",
    "# Fool.  It dilutes the power of the \"yes\"s! There aren't enough \"yes\"s \n",
    "# in any one level of frequency to compete with the \"no\"s.\n",
    "\n",
    "# I'm going to create this list with the first one I found and\n",
    "# then append to it in the console.\n",
    "columns_to_binarize = ['abus_treat_non_queer']\n",
    "# make a shorter name, dummy\n",
    "col_bi = columns_to_binarize.copy()\n",
    "\n",
    "# these are some visually very promising ones\n",
    "good_ones = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 6 in full:\n",
    "6. 2024-06-07_columns-i-want-to-screw-with-some-more-and-how.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 7 in full:\n",
    "7. 2024-06-07_modeling-part-1.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friday, June 7, 2024\n",
    "# Modeling!\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm just going to eyeball some features from the scatterplots\n",
    "good_ones = ['chronic_strain', 'suicidality', 'w1age', 'w1auditc_i', \n",
    "    'w1connectedness_i', 'w1conversion', 'w1dudit_i', 'w1everyday_i', \n",
    "  'w1feltstigma_i', 'w1idcentral_i', 'w1internalized_i', 'w1lifesat_i', \n",
    "  'w1meim_i', 'w1pinc_i', 'w1poverty_i_ei', 'w1q03_ei', 'w1q33_ei', \n",
    "  'w1q52_ei', 'w1q72_ei', 'w1q74_22_ei', 'w1q74_21_ei', 'w1q140_ei', \n",
    "  'w1q166_ei', 'w1q167_ei', 'w1q171_8_ei', 'w1q181_ei_r', 'w1socialwb_i', \n",
    "  'w1socsupport_i']\n",
    "\n",
    "# Make X and y\n",
    "X = meyer[good_ones]\n",
    "y = meyer['kessler6_sqrt']\n",
    "\n",
    "# Very small testing set because this is an inferential model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "  test_size = 0.1, random_state = 6)\n",
    "  \n",
    "for i in [X_train, X_test, y_train, y_test]:\n",
    "  print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Cross validation just for funsies\n",
    "cross_val_score(lr, X_train, y_train)\n",
    "\n",
    "# Fit the model\n",
    "model_1 = lr.fit(X_train, y_train)\n",
    "\n",
    "# Make some predictions\n",
    "model_1_train_preds = model_1.predict(X_train)\n",
    "model_1_test_preds = model_1.predict(X_test)\n",
    "\n",
    "# Score the model\n",
    "print('Training Set')\n",
    "model_1.score(X_train, y_train)\n",
    "mean_squared_error(y_train, model_1_train_preds)\n",
    "mean_squared_error(y_train, model_1_train_preds, squared = False)\n",
    "mean_absolute_error(y_train, model_1_train_preds)\n",
    "print('='*20)\n",
    "print('Testing Set')\n",
    "model_1.score(X_test, y_test)\n",
    "mean_squared_error(y_test, model_1_test_preds)\n",
    "mean_squared_error(y_test, model_1_test_preds, squared = False)\n",
    "mean_absolute_error(y_test, model_1_test_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
