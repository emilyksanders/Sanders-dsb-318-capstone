{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thursday, May 23, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it's a study of queer people across several age groups and across several years. so what I decided to do is look only at the first wave (before the attrition effects kicked in, which were sizable).  it's only n=1518\n",
    "\n",
    "so, I dropped all the features that were exclusive to wave 2 and/or 3, leaving me with 505 features.  then I went thru the codebook and found all the scales they used.  I tracked down the columns where they had calculated the scores, and dropped all the component parts.  \n",
    "I then investigated their imputation process a little bit.  It is very sound in theory and, more importantly, I don't have enough data to drop NAs nor do I have any better ideas on how to impute them.  They *did* the imputation I would have done - regress the missing value on related, populated values, then randomly select a value from the nearest 5 extant values based on something that sounds a lot like KNN.  it seems very robust.  after looking into that, I dropped the non-imputed scale score features, and only kept the imputed ones.\n",
    "\n",
    "after that, I had a little under 400 columns left.  having finally figured out how the scales work, and that they did in fact calculate a lot of these things, I went back to the demographics section of the codebook and looked to see if they did similar stuff there.  it seems like they may have, but I'm not sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "files created: \n",
    "- 2024-05-23_exploring_Meyer_2023_dataset.py\n",
    "- 2024-05-23_set_up.py **NEW SET UP SCRIPT FOR NEW DF!!!!**\n",
    "- 2024-05-23_cols_to_check.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very grim and not at all surprising that column with the most missings is IPV\n",
    "# (from the ACE survey - so IPV among parents, not participants themselves)\n",
    "# >>> meyer['w1ace_ipv'].value_counts(dropna = False)\n",
    "# w1ace_ipv\n",
    "# 0    950\n",
    "# 1    429\n",
    "#      139\n",
    "# Name: count, dtype: int64\n",
    "# >>> meyer['w1ace_ipv_i'].value_counts(dropna = False)\n",
    "# w1ace_ipv_i\n",
    "# 0    1024\n",
    "# 1     494\n",
    "# Name: count, dtype: int64\n",
    "# >>> 1024-950\n",
    "# 74\n",
    "# >>> 139-74\n",
    "# 65\n",
    "\n",
    "# ok.  their imputation method seems really solid, but that is sus af.\n",
    "# of the people who declined to answer about IPV, you think the majority \n",
    "# of them would have said NO!?!??!??????\n",
    "# I'd be tempted to put every one of those guys down as a yes!!!!\n",
    "# let me see the overlap with other stigmatized stuff\n",
    "len(meyer[(meyer['w1ace_ipv']==' ') & (meyer['w1ace_sex']==' ')])\n",
    "# 30 huh damn I would have thought like all of them\n",
    "# wait.\n",
    "len(meyer[(meyer['w1ace_ipv']=='1') & (meyer['w1ace_sex']==' ')])  # 19\n",
    "len(meyer[(meyer['w1ace_ipv']==' ') & (meyer['w1ace_sex']=='1')])  # 51\n",
    "# ok! that actually lends credibility to their imputation though\n",
    "# if people were too ashamed to admit IPV, they'd probably decline to answer\n",
    "# about CSA too.  these people, at least, really could go either way on IPV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is all actually moot \n",
    "# because I don't have a better idea for imputation than what they did.\n",
    "# I think the thing to do, if I can, is to model it on the imputed data\n",
    "# then model it again on the subsample where I've dropped all these NAs\n",
    "# at least the ACE NAs, where there's really a lot.\n",
    "# could also try modeling it just without those columns\n",
    "# a random forest would probably be good, \n",
    "# make sure the whole thing isn't hinging on imputed data\n",
    "# Meyer also probably ran some tests in his paper to make sure this is ok\n",
    "\n",
    "\n",
    "# I've gotta reduce my dimensions\n",
    "# the imputation methods they used seem very reasonable\n",
    "# (see p. 37 of 37166-Documentation-methodology.pdf)\n",
    "# so I'm going to drop the non-imputed ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wednesday, May 29, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I finished all the scales that were defined in the documentation, and dropped columns accordingly.  at that point i was down to 337 columns.  I then started going through the 831 page documenation to piece together the rest of what's what.  i started writing a dictionary of how I want to combine these columns, after which I plan to drop most of them.  \n",
    "\n",
    "there are a few I want to keep to see if they perform better alone than they do as a composite, but I want to be careful of introducing too much colinearity.  because I'd really like to be able to do interpretation, I might have to sacrifice some features I think are interesting in order to be able to make sense of the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "files created: \n",
    "- 2024-05-23_AND_2024-05-29_exploring_Meyer_2023_dataset.py\n",
    "- 2024-05-29_cols_to_check.txt\n",
    "- 2024-05-29_notes_about_non-scale_columns.txt\n",
    "both of the first two files are actually just longer versions of the older ones, so most of their contents are redundant.  I used the same setup file as on 5-23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am looking thru the documentation and making 2 lists\n",
    "# the variables should ROUGHLY correspond to each other\n",
    "# maybe I'll do some line breaks or whatever.\n",
    "\n",
    "# drop_cols_2 = [    <-- already in the report\n",
    "\n",
    "keep_cols = ['studyid', 'waveparticipated', 'w1survey_yr', \n",
    "  'w1age', 'cohort', 'w1sex', 'w1gender', 'w1sex_gender', \n",
    "  'screen_race', 'w1race',\n",
    "  'w1sexualid', 'w1sexminid', \n",
    "  'geduc1', 'geduc2', 'geducation', \n",
    "  'gurban_i', 'gcendiv', 'gcenreg', 'gmilesaway', 'gmilesaway2', \n",
    "  'w1hinc_i', 'w1poverty_i', 'w1povertycat_i', \n",
    "  'w1conversion', 'w1conversionhc', 'w1conversionrel', \n",
    "  'w1weight_full', 'gemployment2010', \n",
    "]\n",
    "\n",
    "check_cols = [\n",
    "  # no age, sex stuff\n",
    "  'w1q20_t_verb', # I suspect this is part of q20 and got rolled into w1race, but I'm not sure off the top of my head\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I finished all the variables that were easy to parse out and assigned\n",
    "# them to a list above.  I am now going to drop the ones that I indicated,\n",
    "# and then I will go through the 831 page documentation to make sense \n",
    "# of what's left and try to drop more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suicidal_idea_beh = ['w1q101', 'w1q102', 'w1q103', 'w1q104', 'w1q105', 'w1q106', \n",
    "  'w1q107', 'w1q108', 'w1q109', 'w1q110', 'w1q111', 'w1q112', 'w1q113', 'w1q114', \n",
    "  'w1q115', 'w1q116', 'w1q117', 'w1q118', 'w1q119', 'w1q120', 'w1q121']\n",
    "\n",
    "# drop_cols_2_again = <--- already in report\n",
    "\n",
    "keep_cols_good_on_own = ['w1q01', 'w1q02', 'w1q03', 'w1q32', 'w1q33', 'w1q34', 'w1q35', 'w1q36', \n",
    "  'w1q37', 'w1q38', 'w1q52', 'w1q65', 'w1q69', 'w1q72', 'w1q74_21', 'w1q74_22', 'w1q74_23', 'w1q78', \n",
    "  'w1q79', 'w1q89', 'w1q119', 'w1q134', 'w1q136_7', 'w1q139_7', 'w1q140', 'w1q141_7']\n",
    "\n",
    "keep_cols_ohe_done = ['w1q30_1', 'w1q30_2', 'w1q30_3', 'w1q30_4', 'w1q30_5', \n",
    "  'w1q39_1', 'w1q39_2', 'w1q39_3', 'w1q39_4', 'w1q39_5', 'w1q39_6', 'w1q39_7', \n",
    "  'w1q39_8', 'w1q39_9', 'w1q39_10', 'w1q39_11', 'w1q39_12', 'w1q39_t_verb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes about non-scale columns\n",
    "\n",
    "- I've also kept Q3, which is about happiness, but I suspect it's very corr'd with WB.  check that, and then either combine or drop it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thursday, May 30, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "files created: \n",
    "- 2024-05-23_thru_2024-05-30_exploring_Meyer_2023_dataset.py\n",
    "- 2024-05-30_cols_to_check.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually combine columns in ways that *I* think make sense\n",
    "# here I am creating a dictionary where the keys are the new column names I want\n",
    "# and the values are lists, wherein the first value on the list is the method \n",
    "# of combination, and the remainder are the columns to be combined\n",
    "# if the method is 'recode', then I probably need to give it more direct attn\n",
    "# but all the others I am hoping to be able to automate\n",
    "# the end goal is to use this dictionary to either generate or directly run\n",
    "# the code to create the new columns and drop the ones no longer needed\n",
    "# before dropping the columns, I should check them against keep_cols\n",
    "# to make sure I'm not hoping to keep them individually in addition to combo'ing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monday, June 3, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I continued working on a plan for null values and imputations.  I made a list of every column with null values and made a plan for them one by one.  I really want to do fancy, high quality imputation, but I think mode and median are the way to go, just for expediency.  Once all the null values are gone, it will be time to do the combinations as defined in the dictionary, do some more column dropping, and write up a data cleaning notebook.\n",
    "\n",
    "I dropped 11 rows that had straight people in them.\n",
    "\n",
    "**I'm going to use _ei in my column names to indicate \"Emily imputed.\"  The original authors used _i for their imputations.**\n",
    "\n",
    "files created: \n",
    "- imputation_plan.xlsx -- a list of all columns with null values, how many, and what to do with them.\n",
    "- can-i-impute-0s.md was actually created on 5/31, but it's a good one.  i looked through all the columns with naturally occuring 0s, and decided whether or not I could impute 0s for the NAs without borking everything up\n",
    "- I created 04_scratch_work/ in the repo, and 01_notes/ and 02_scripts/ within it.  notes/ has copies of all my notes, and scripts/ has copies of the scripts\n",
    "- I updated my gitignore to allow .py files, for now at least\n",
    "- 2024-05-31_continuing_messing_with_meyer_columns.py was created on Friday with just a little bit of bridge work between the column stuff in the giant cleaning script and the imputation planning I did today.\n",
    "- 2024-06-03_imputation_groundwork.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script 1 in full:\n",
    "# 2024-05-31_continuing_messing_with_meyer_columns.py\n",
    "\n",
    "meyer.shape\n",
    "meyer.columns\n",
    "\n",
    "\n",
    "# Before we do this next bit, let's preserve these guys\n",
    "keep_cols = combo_imputed + keep_cols + keep_cols_good_on_own + keep_cols_ohe_done\n",
    "\n",
    "# Now I need to handle the columns in the dictionary\n",
    "\n",
    "# did I duplicate any names?\n",
    "for i in feat_eng_dict.keys():\n",
    "  if i in keep_cols:\n",
    "    print(i)\n",
    "\n",
    "# check again because I figured out how to handle the \"NAs\" as strings\n",
    "meyer.isna().sum().sum()\n",
    "for i in list(zip(list(meyer.dtypes), list(meyer.columns))):\n",
    "  print(i)\n",
    "\n",
    "# I want to fill all my NAs with 0s, so I can do math on things.\n",
    "# Can I do that?  Do any of these columns have natural 0s?\n",
    "\n",
    "for i in list(meyer.columns):\n",
    "  a = list(meyer[i].unique())\n",
    "  if 0 in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script 2 in full: \n",
    "# 2024-06-03_imputation_groundwork.py\n",
    "\n",
    "# June 3, 2024\n",
    "# Imputation\n",
    "\n",
    "# This is continuing some work I started last week.\n",
    "# I have defined a dictionary full of columns that I\n",
    "# want to combine one way or another.  I can't do that\n",
    "# if they're full of NAs.  Imputing 0s would be the \n",
    "# easiest way to do it, but I don't want to introduce\n",
    "# unreasonable amounts of error into my data.\n",
    "\n",
    "# I'm going to use _ei to indicate \"Emily imputed.\"\n",
    "# The original authors used _i for their imputations.\n",
    "\n",
    "# Last week I made a list of all the columns with \n",
    "# naturally occurring 0s in them. Today I want a \n",
    "# list of all remaining NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "meyer.isna().sum().sort_values(ascending = False)\n",
    "\n",
    "# Friends of gmilesaway2\n",
    "# It's actually not a very good assumption that they live\n",
    "# more than 60 miles away. 73.5% of people live w/i 60m!\n",
    "# And yet, most people have never gone.  \n",
    "# I think the thing to impute here is a 0 under the original \n",
    "# meaning, that is, within 60 miles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've made a list of the NAs and I'm looking at THOSE\n",
    "# for how to impute.  THAT'S what this next section is.\n",
    "\n",
    "# w1q101\n",
    "# subset investigation\n",
    "s = mey.loc[cond, suicidal_idea_beh]\n",
    "s = s[['w1q101',  'w1q105', 'w1q109', 'w1q118', 'w1q119', 'w1q121']]\n",
    "\n",
    "# row condition\n",
    "cond = mey['w1q101'].isna()\n",
    "\n",
    "# column conditions\n",
    "y1 = meyer[((meyer['w1q105'].notna()) & (meyer['w1q105']!=1))]\n",
    "y2 = meyer[((meyer['w1q109'].notna()) & (meyer['w1q109']!=1))]\n",
    "y3 = meyer[((meyer['w1q113'].notna()) & (meyer['w1q113']!=1))]\n",
    "\n",
    "# In another pass I'd love to match the numbers\n",
    "# but for now we're just going to impute \"once.\"\n",
    "# It is at least not overcounting.\n",
    "meyer.loc[[(y1 | y2 | y3) & cond], 'w1q101'] = 2\n",
    "len(meyer[y1])\n",
    "\n",
    "# check these age columns\n",
    "cond = mey['w1q102'].isna() # none!\n",
    "cond = mey['w1q103'].isna()\n",
    "# ok this would take forever. n<10 -> impute median\n",
    "cond = mey['w1q112'].isna()\n",
    "s = mey.loc[cond, suicidal_idea_beh]\n",
    "\n",
    "# nssh\n",
    "cond = mey['w1q119'].isna()\n",
    "s = mey.loc[cond, suicidal_idea_beh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hey, is it just a few rows with a ton of missing data?\n",
    "# I hate to lose data but if they truly didn't fill out anything...\n",
    "meyer.iloc[10,:].isna().sum()\n",
    "# oh right, there are all those OHE columns with planned missing\n",
    "# so I need to start imputing first, then recheck these\n",
    "# then decide whether to drop anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuesday, June 4, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I followed the guidelines I laid out in the spreadsheet yesterday.  It took several tries to get an `np.where()` statement working (I tried a whole function first, and then the humble `np.where()` is what actually got the job done!), but was then successful.  I checked and rechecked and rechecked very carefully to make sure I wasn't introducing error into my data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to complete imputation on the entire dataset, prior to the train test split, because my primary interest lies in an inferential model.  This means that it is less crucial that my workflow and model be generalizable to new data gathered in the future.  Furthermore, because this data concerns sensitive personal and political issues, the median or mode responses would likely change over time if the survey were to be readministered, certainly over a long enough timeframe.  (Note: The obvious implications of this temporal drift for a 2016 dataset will be further discussed in the limitations and recommendations sections.)  It would be entirely illogical to fit the imputer on a subset of data from 2016 and then continue projecting those values forward into new data indefinitely.  Therefore, I prioritized fidelity to the current data over adaptability to hypothetical future data, and fit the imputers on my entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUT the stupid 97s-99s threw off all the summary stats so the si fits I did today don't work.  I need to clean those columns up first and then re-fit them.  I coped the si syntax into its own script to make this easier, and so that I could continue the imputation I was doing today without making that script a mobius strip of time shenanigans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "files created: \n",
    "\n",
    "- 2024-06-04_imputation_ATTEMPT-1_dealing-with-7s-and-97s-indiy-ohe-columns.py\n",
    "- - first attempt at this nonsense, tried a ton of things, abandoned this script once one of them worked\n",
    "\n",
    "- 2024-06-04_imputation_dealing-with-7s-and-97s-indiy-ohe-columns.py\n",
    "- - copied just the thing that worked from the last script and continued from there\n",
    "\n",
    "- 2024-06-04_imputation_misc.py\n",
    "- - once I got that first batch of stuff worked out, I jumped over here to get a clean start on the rest of it\n",
    "\n",
    "- 2024-06-04_simple_imputer_stuff.py\n",
    "- - this is a COPY of the si stuff from 2024-06-04_imputation_misc.py, because I discovered more 97s-99s that I need to clear out before it actually makes sense to do si\n",
    "\n",
    "- meyer_backup_diy_ohe_no_97s_7s.csv\n",
    "- - safety first\n",
    "\n",
    "- meyer_much_imputation_done_but_not_si_yet.csv\n",
    "- - safety first\n",
    "\n",
    "- MODIFIED imputation_plan.xlsx\n",
    "- - notes and color coding as I go along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 1 in full:\n",
    "2024-06-04_imputation_ATTEMPT-1_dealing-with-7s-and-97s-indiy-ohe-columns.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, so yesterday I made an excel spreadsheet list of all of NAs\n",
    "# and decided what to do with them.  There are a bunch of different \n",
    "# methods (I chose what I thought was best for each type of data), \n",
    "# and I think the neatest way to handle all of them without making\n",
    "# a giant mess is to break up the dataframe (include the ID in \n",
    "# each subset so that they can be cleanly re-merged), use a \n",
    "# real, official imputer, and then put it back together again.\n",
    "\n",
    "# Oooh boy this could be dangerous.  I'd have to drop them from\n",
    "# the original to avoid overwriting or otherwise losing stuff.\n",
    "# Let me see what the shape is now.\n",
    "\n",
    "meyer.shape # (1507, 267)\n",
    "# So that it is the shape I want to get back at the end,\n",
    "# minus any that I consciously, deliberately drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok BASIC test\n",
    "meyer.loc[:, diy_ohe_new] = meyer.loc[:, diy_ohe]\n",
    "\n",
    "value_list = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list += a\n",
    "value_list = list(set(value_list))\n",
    "sorted(value_list)\n",
    "\n",
    "# I just checked the original columns and they're fine.\n",
    "\n",
    "# check this again, should be no more 97s, but nothing else weird\n",
    "value_list_2 = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list_2 += a\n",
    "value_list_2 = list(set(value_list_2))\n",
    "sorted(value_list_2)\n",
    "\n",
    "[x for x in value_list if x not in value_list_2]  # 97\n",
    "[x for x in value_list_2 if x not in value_list]  # 0\n",
    "\n",
    "# OH THANK GOODNESS.  FINALLY!\n",
    "\n",
    "# check this again because I'm losing my mind\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok!  on we go!\n",
    "meyer[diy_ohe_new] = meyer[diy_ohe_new].fillna(0)\n",
    "\n",
    "# should be short now!\n",
    "value_list_3 = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list_3 += a\n",
    "value_list_3 = list(set(value_list_3))\n",
    "sorted(value_list_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AHAAHAHAAAAA I'M SO EXCITED I'M HYPERVENTILATING!!!!!!!\n",
    "meyer.loc[:, diy_ohe_new] = np.where(meyer.loc[:, diy_ohe_new]!=1, 0, meyer.loc[:, diy_ohe_new])\n",
    "\n",
    "# one more time???\n",
    "value_list_4 = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list_4 += a\n",
    "value_list_4 = list(set(value_list_4))\n",
    "sorted(value_list_4)\n",
    "\n",
    "# YAAAAAAAAAAASSSSSSSSSSSS!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some of that ^ may need to be integrated in here V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 2 in full:\n",
    "2024-06-04_imputation_dealing-with-7s-and-97s-indiy-ohe-columns.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuesday, June 4, 2024\n",
    "# Imputation, Take 3\n",
    "\n",
    "# This script is a re-write of the other, earlier \n",
    "# script from today.  I tried a bunch of different\n",
    "# ways to deal with the 97s, and it took a while\n",
    "# to get one to work.\n",
    "\n",
    "# Alrighty! Now we're just imputing 0s!\n",
    "# Then changing everything that's not 1 to 0.\n",
    "diy_ohe = [\"w1q145_3\", \"w1q163_3\", \"w1q136_3\", \"w1q145_9\", \n",
    "  \"w1q143_3\", \"w1q136_10\", \"w1q145_10\", \"w1q136_9\", \"w1q163_10\", \n",
    "  \"w1q163_9\", \"w1q143_9\", \"w1q143_4\", \"w1q136_6\", \"w1q143_10\", \n",
    "  \"w1q143_5\", \"w1q145_4\", \"w1q136_5\", \"w1q143_8\", \"w1q163_5\", \n",
    "  \"w1q136_4\", \"w1q163_6\", \"w1q145_6\", \"w1q136_1\", \"w1q143_7\", \n",
    "  \"w1q163_1\", \"w1q143_2\", \"w1q143_1\", \"w1q145_5\", \"w1q143_6\", \n",
    "  \"w1q163_2\", \"w1q163_4\", \"w1q136_8\", \"w1q145_8\", \"w1q145_1\", \n",
    "  \"w1q145_7\", \"w1q163_7\", \"w1q136_2\", \"w1q145_2\", \"w1q139_3\", \n",
    "  \"w1q136_7\", \"w1q139_9\", \"w1q139_10\", \"w1q139_5\", \"w1q139_4\", \n",
    "  \"w1q139_8\", \"w1q139_6\", \"w1q139_2\", \"w1q139_1\", \"w1q139_7\", \n",
    "  \"w1q163_8\", \"w1q141_3\", \"w1q141_9\", \"w1q141_10\", \"w1q141_8\", \n",
    "  \"w1q141_4\", \"w1q141_1\", \"w1q141_5\", \"w1q141_2\", \"w1q141_7\", \n",
    "  \"w1q141_6\", \"w1q64_12\", \"w1q64_5\", \"w1q74_5\", \"w1q74_6\", \n",
    "  \"w1q74_20\", \"w1q64_11\", \"w1q64_10\", \"w1q74_18\", \"w1q74_14\", \n",
    "  \"w1q74_17\", \"w1q171_8\", \"w1q30_3\", \"w1q64_13\", \"w1q64_7\", \n",
    "  \"w1q30_4\", \"w1q171_6\", \"w1q171_4\", \"w1q64_8\", \"w1q74_10\", \n",
    "  \"w1q74_21\", \"w1q64_6\", \"w1q74_11\", \"w1q171_5\", \"w1q64_3\", \n",
    "  \"w1q64_1\", \"w1q171_9\", \"w1q30_5\", \"w1q171_3\", \"w1q74_22\", \n",
    "  \"w1q64_9\", \"w1q171_2\", \"w1q171_7\", \"w1q74_23\", \"w1q64_4\", \n",
    "  \"w1q64_2\", \"w1q30_1\", \"w1q171_1\", \"w1q30_2\"]\n",
    "\n",
    "value_list = []\n",
    "for i in diy_ohe:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list += a\n",
    "value_list = list(set(value_list))\n",
    "sorted(value_list)\n",
    "\n",
    "# create some new column names\n",
    "diy_ohe_new = [''.join([x, '_ei']) for x in diy_ohe]\n",
    "\n",
    "# let's see if this replaces the 97s right\n",
    "meyer.loc[:, diy_ohe_new] = np.where(meyer.loc[:, diy_ohe]==97, 0, meyer.loc[:, diy_ohe])\n",
    "\n",
    "meyer.shape # (1507, 250)\n",
    "len(diy_ohe) # 98\n",
    "len(diy_ohe_new) # 98\n",
    "\n",
    "# check this\n",
    "for i, j in list(zip(diy_ohe, diy_ohe_new))[50:]:\n",
    "  print(i)\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('-'*20)\n",
    "  print(j)\n",
    "  meyer[j].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "  print('')\n",
    "  print('='*20)\n",
    "\n",
    "# I'm looking for valid 7s.\n",
    "# It seems like 7 was also used as an NA code\n",
    "# because why be consistent lolololol!!!!!!!!\n",
    "\n",
    "nat_7s = ['w1q143_7', 'w1q145_7', 'w1q163_7', 'w1q136_7', \n",
    "  'w1q139_7', 'w1q141_7', 'w1q64_7', 'w1q171_7']\n",
    "  \n",
    "nat_7s == [c for c in diy_ohe if c.split('_')[-1]=='7']\n",
    "# True.\n",
    "\n",
    "nat_7s_ei = [''.join([x, '_ei']) for x in nat_7s]\n",
    "\n",
    "# let's turn those 7s into 1s, and then the rest into 0s\n",
    "meyer.loc[:, nat_7s_ei] = np.where(meyer.loc[:, nat_7s]==7, 1, meyer.loc[:, nat_7s])\n",
    "\n",
    "# check this\n",
    "for i in nat_7s_ei:\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "\n",
    "# check one against the documentation\n",
    "meyer['w1q141_7'].value_counts(dropna = False)\n",
    "\n",
    "# check this again, should be no more 97s, but nothing else weird\n",
    "remaining_7s = []\n",
    "value_list_2 = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  if 7 in a:\n",
    "    remaining_7s.append(i)\n",
    "  value_list_2 += a\n",
    "value_list_2 = list(set(value_list_2))\n",
    "sorted(value_list_2)\n",
    "\n",
    "remaining_7s_qs = [x.split('_')[0] for x in remaining_7s]\n",
    "sorted(set(remaining_7s_qs))\n",
    "# I went through VERY CAREFULLY and confirmed that all the \n",
    "# remaining 7s are \"planned missings.\"\n",
    "\n",
    "# let's see if this replaces the 97s right\n",
    "meyer.loc[:, diy_ohe_new] = np.where(meyer.loc[:, diy_ohe_new]==7, 0, meyer.loc[:, diy_ohe_new])\n",
    "\n",
    "# check this\n",
    "for i, j in list(zip(diy_ohe, diy_ohe_new))[:50]:\n",
    "  print(i)\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('-'*20)\n",
    "  print(j)\n",
    "  meyer[j].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "  print('')\n",
    "  print('='*20)\n",
    "\n",
    "meyer[diy_ohe_new] = meyer[diy_ohe_new].fillna(0)\n",
    "\n",
    "# check again!\n",
    "for i, j in list(zip(diy_ohe, diy_ohe_new))[:50]:\n",
    "  print(i)\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('-'*20)\n",
    "  print(j)\n",
    "  meyer[j].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "  print('')\n",
    "  print('='*20)\n",
    "\n",
    "# wait, some of them still have 97s??\n",
    "\n",
    "# let's see if this replaces the 97s right\n",
    "meyer.loc[:, diy_ohe_new] = np.where(meyer.loc[:, diy_ohe_new]==97.0, 0, meyer.loc[:, diy_ohe_new])\n",
    "\n",
    "for i, j in list(zip(diy_ohe, diy_ohe_new))[:50]:\n",
    "  print(i)\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('-'*20)\n",
    "  print(j)\n",
    "  meyer[j].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "  print('')\n",
    "  print('='*20)\n",
    "  \n",
    "# this again too\n",
    "remaining_7s_2 = []\n",
    "remaining_97s = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  if 7 in a:\n",
    "    remaining_7s_2.append(i)\n",
    "  if 97 in a:\n",
    "    remaining_97s.append(i)\n",
    "sorted(set(remaining_7s_2))\n",
    "sorted(set(remaining_97s))\n",
    "\n",
    "# check AGAIN\n",
    "value_list_x = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list_x += a\n",
    "value_list_x = list(set(value_list_x))\n",
    "sorted(value_list_x)\n",
    "\n",
    "# make a backup\n",
    "os.getcwd()\n",
    "meyer.to_csv('meyer_backup_diy_ohe_no_97s_7s.csv', index = False)\n",
    "\n",
    "\n",
    "# reduce to 0s and 1s\n",
    "meyer.loc[:, diy_ohe_new] = np.where(meyer.loc[:, diy_ohe_new]!=0, 1, meyer.loc[:, diy_ohe_new])\n",
    "\n",
    "for i, j in list(zip(diy_ohe, diy_ohe_new))[:50]:\n",
    "  print(i)\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('-'*20)\n",
    "  print(j)\n",
    "  meyer[j].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "  print('')\n",
    "  print('='*20)\n",
    "\n",
    "# no NAs?\n",
    "meyer[diy_ohe_new].isna().sum().sum()\n",
    "# no NAs!\n",
    "\n",
    "# drop the old versions\n",
    "meyer.drop(columns = diy_ohe, inplace = True)\n",
    "\n",
    "# check the dtypes\n",
    "pd.set_option('display.max_rows', None)\n",
    "meyer.dtypes\n",
    "\n",
    "# should be VERY short now!\n",
    "value_list_3 = []\n",
    "for i in diy_ohe_new:\n",
    "  a = list(meyer[i].unique())\n",
    "  value_list_3 += a\n",
    "value_list_3 = list(set(value_list_3))\n",
    "sorted(value_list_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 3 in full:\n",
    "2024-06-04_imputation_misc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuesday, June 4, 2024\n",
    "# More Imputation!\n",
    "\n",
    "# w1q64_t_verb\n",
    "meyer['w1q64_t_verb'].value_counts(dropna = False)\n",
    "meyer.shape # 250 columns - gonna add one then delete the original\n",
    "# meyer['w1q64_t_num'] = meyer['w1q64_t_verb'].fillna('0')\n",
    "# I got a weird warning here ^, so for future runs V\n",
    "meyer[['w1q64_t_num']] = meyer[['w1q64_t_verb']].fillna('0')\n",
    "# for right now V\n",
    "# meyer_c = meyer.copy(deep = True)\n",
    "# meyer = meyer_c.copy(deep = True)\n",
    "# meyer.shape\n",
    "# thanks to this SO article for advice\n",
    "# https://stackoverflow.com/questions/68292862/performancewarning-dataframe-is-highly-fragmented-this-is-usually-the-result-o\n",
    "\n",
    "# reduce to 0s and 1s\n",
    "meyer.loc[:, 'w1q64_t_num'] = np.where(meyer.loc[:, 'w1q64_t_num']!='0', 1, 0)\n",
    "\n",
    "# check\n",
    "meyer['w1q64_t_num'].value_counts(dropna = False)\n",
    "meyer['w1q64_t_verb'].value_counts(dropna = False)\n",
    "\n",
    "# drop the original\n",
    "meyer.drop(columns = ['w1q64_t_verb'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmilesaway2\n",
    "meyer['gmilesaway2'].value_counts(dropna = False)\n",
    "\n",
    "meyer[['gmilesaway2_ei']] = meyer[['gmilesaway2']].fillna(0)\n",
    "meyer['gmilesaway2'].value_counts(dropna = False)\n",
    "meyer['gmilesaway2_ei'].value_counts(dropna = False)\n",
    "\n",
    "# reverse code so 1=close\n",
    "meyer[['gmilesaway2_ei_r']] = 1-meyer[['gmilesaway2_ei']]\n",
    "meyer['gmilesaway2'].value_counts(dropna = False)\n",
    "meyer['gmilesaway2_ei'].value_counts(dropna = False)\n",
    "meyer['gmilesaway2_ei_r'].value_counts(dropna = False)\n",
    "\n",
    "# drop\n",
    "meyer.drop(columns = ['gmilesaway2', 'gmilesaway2_ei'], inplace = True)\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w1q123d & w1q123c\n",
    "# These guys had some missing values, but also had an option (5)\n",
    "# for \"don't know/doesn't apply.\"  I don't know what the truth\n",
    "# is for these missing values, so I'm recoding them to \"don't know.\"\n",
    "\n",
    "# What to do with the 5s \n",
    "# will be a question for the next round of cleaning.\n",
    "\n",
    "meyer[['w1q123d_ei', 'w1q123c_ei']] = meyer[['w1q123d', 'w1q123c']].fillna(5)\n",
    "meyer[['w1q123d_ei', 'w1q123c_ei', 'w1q123d', 'w1q123c']]\n",
    "\n",
    "meyer.drop(columns = ['w1q123d', 'w1q123c'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w1q179\n",
    "# impute as \"nothing in particular\", seems logical\n",
    "meyer[['w1q179_ei']] = meyer[['w1q179']].fillna(13)\n",
    "meyer[['w1q179', 'w1q179_ei']]\n",
    "\n",
    "meyer.drop(columns = 'w1q179', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all right!  real imputer time!!\n",
    "\n",
    "# There is no reason to design this process to accommodate for novel\n",
    "# data extending into the future, because there is no\n",
    "# reason to assume that these values wouldn't change over time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written Tuesday, June 4, 2024\n",
    "# FOR Wednesday, June 5, 2024\n",
    "\n",
    "# Much of this was copied from impt_misc\n",
    "\n",
    "# The simple imputer stuff\n",
    "\n",
    "s_median = [\"w1q50\", \"w1q48\", \"w1q51\", \"w1q112\", \n",
    "  \"w1q49\", \"w1q47\", \"w1q46\", \"w1q45\", \"w1q146c\", \n",
    "  \"w1q162\", \"w1q181\", \"w1q146f\", \"w1q146g\", \"w1q146e\", # original ^ added V\n",
    "  \"w1q103\", \"w1q104\", \"w1q106\", \"w1q107\", \"w1q108\", \n",
    "  \"w1q111\", \"w1q115\", \"w1q116\", \"w1q117\", \"w1q134\", \n",
    "  \"w1q146a\", \"w1q146h\", \"w1q146i\", \"w1q146j\", \"w1q146k\", \n",
    "  \"w1q146l\", \"w1q169\"]\n",
    "\n",
    "s_mode = [\"w1q175\", \"w1q72\", \"w1q142d\", \"w1q65\", \"w1q52\", \n",
    "  \"w1q142i\", \"w1q142g\", \"w1q180\", \"w1q69\", \"w1q135d\", \n",
    "  \"w1q142h\", \"w1q135b\", \"w1q135c\", \"w1q135e\", \"w1q142b\", \n",
    "  \"w1q142e\", \"w1q142f\", \"w1q142j\", \"w1q142a\", \"w1q142k\", \n",
    "  \"w1q19c\", \"w1q19d\", \"w1q19b\", \"w1q19a\", \"w1q75\", \n",
    "  \"w1q78\", \"w1q79\", \"w1q76\",                    # original ^ added V\n",
    "  \"w1q135a\", \"w1q135f\", \"w1q142c\", \"w1q35\", \"w1q36\", \n",
    "  \"w1q37\", \"w1q38\", \"w1q124\", \"w1q89\"]\n",
    "\n",
    "s_median_new = [''.join([x, '_ei']) for x in s_median]\n",
    "s_mode_new = [''.join([x, '_ei']) for x in s_mode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right here I need to figure out how to deal with the 97s-99s.\n",
    "\n",
    "s_97s = []\n",
    "s_98s = []\n",
    "s_99s = []\n",
    "for i in s_median:\n",
    "  a = list(meyer[i].unique())\n",
    "  if ((97 in a) | (97. in a)):\n",
    "    s_97s.append(i)\n",
    "  if ((98 in a) | (98. in a)):\n",
    "    s_98s.append(i)\n",
    "  if ((99 in a) | (99. in a)):\n",
    "    s_99s.append(i)\n",
    "    \n",
    "print(s_97s.sort())\n",
    "print(s_98s.sort())\n",
    "print(s_99s.sort())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in s_mode:\n",
    "  a = list(meyer[i].unique())\n",
    "  if ((97 in a) | (97. in a)):\n",
    "    s_97s.append(i)\n",
    "  if ((98 in a) | (98. in a)):\n",
    "    s_98s.append(i)\n",
    "  if ((99 in a) | (99. in a)):\n",
    "    s_99s.append(i)\n",
    "\n",
    "s_97s = ['w1q102', 'w1q103', 'w1q104', \n",
    "  'w1q106', 'w1q107', 'w1q108', 'w1q110', 'w1q111', 'w1q112',   # impute to 0s; \n",
    "  'w1q114', 'w1q115', 'w1q116', 'w1q117', 'w1q134']    # these are (mostly) age Qs\n",
    "s_98s = ['w1q45', 'w1q46', 'w1q47', 'w1q48', 'w1q49', 'w1q50', 'w1q51']  # NA\n",
    "s_99s = ['w1q45', 'w1q46', 'w1q47', 'w1q48', 'w1q49', 'w1q50', 'w1q51']  # age + 2\n",
    "\n",
    "s_97s_new = [''.join([x, '_ei']) for x in s_97s]\n",
    "s_98s_new = [''.join([x, '_ei']) for x in s_98s]\n",
    "s_99s_new = [''.join([x, '_ei']) for x in s_99s]\n",
    "\n",
    "# Replace these 97s with 0s\n",
    "meyer.loc[:, s_97s_new] = np.where(meyer.loc[:, s_97s]==97, 0, meyer.loc[:, s_97s])\n",
    "\n",
    "# Replace these 98s with NAs\n",
    "meyer.loc[:, s_98s_new] = np.where(meyer.loc[:, s_98s]==98, np.nan, meyer.loc[:, s_98s])\n",
    "\n",
    "# Replace these 99s with NAs\n",
    "meyer.loc[:, s_99s_new] = np.where(meyer.loc[:, s_99s]==99, np.nan, meyer.loc[:, s_99s_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_97s = []\n",
    "xs_98s = []\n",
    "xs_99s = []\n",
    "\n",
    "for k in [s_97s_new, s_98s_new, s_99s_new]:\n",
    "  for i in k:\n",
    "    a = list(meyer[i].unique())\n",
    "    if ((97 in a) | (97. in a)):\n",
    "      xs_97s.append(i)\n",
    "    if ((98 in a) | (98. in a)):\n",
    "      xs_98s.append(i)\n",
    "    if ((99 in a) | (99. in a)):\n",
    "      xs_99s.append(i)\n",
    "# none left!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put them in order.  checking them made me crazy otherwise\n",
    "s_mode.sort()\n",
    "s_mode_new.sort()\n",
    "s_median.sort()\n",
    "s_median_new.sort()\n",
    "\n",
    "# make sure they're in order\n",
    "for i, j in list(zip(s_mode, s_mode_new)):\n",
    "  print(i, j)\n",
    "for i, j in list(zip(s_median, s_median_new)):\n",
    "  print(i, j)\n",
    "\n",
    "# They are!  Ok!  Let's go again!!\n",
    "\n",
    "# Safety first\n",
    "# meyer.to_csv('2024-06-05_meyer_much_imputation_done_but_not_si_yet.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think the cleanest way to do this without it breaking everything is to\n",
    "# break the dataframe apart, use the 2 SIs separately, then re-merge.\n",
    "# Let me check that the studyID will be suitable for merging on.\n",
    "\n",
    "drop_90s = s_97s + s_98s # s_99s = s_98s\n",
    "meyer.drop(columns = drop_90s, inplace = True)\n",
    "\n",
    "meyer.shape # 1507 rows (250 columns)\n",
    "meyer['studyid'].nunique() # 1507 unique values\n",
    "\n",
    "# update the lists of columns\n",
    "med_cols = [c if c in list(meyer.columns) else ''.join([c, '_ei']) for c in s_median]\n",
    "mode_cols = [c if c in list(meyer.columns) else ''.join([c, '_ei']) for c in s_mode]\n",
    "\n",
    "# split the df up for imputing\n",
    "meyer_median = meyer[s_median+['studyid']]\n",
    "meyer_mode = meyer[s_mode+['studyid']]\n",
    "\n",
    "# new column names\n",
    "s_median_new.append('studyid')\n",
    "s_mode_new.append('studyid')\n",
    "meyer_median.columns = s_median_new\n",
    "meyer_mode.columns = s_mode_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_med = SimpleImputer(strategy = 'median')\n",
    "si_mode = SimpleImputer(strategy = 'most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the imputation!\n",
    "meyer_median = pd.DataFrame(\n",
    "  si_med.fit_transform(meyer_median), columns = si_med.get_feature_names_out())\n",
    "\n",
    "meyer_mode = pd.DataFrame(\n",
    "  si_mode.fit_transform(meyer_mode), columns = si_mode.get_feature_names_out())\n",
    "\n",
    "# is it really that easy?\n",
    "meyer_median.isna().sum().sum()\n",
    "meyer_mode.isna().sum().sum()\n",
    "\n",
    "mme = meyer_median.describe()\n",
    "mmo = meyer_mode.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suicide and NSSH Qs\n",
    "s = [\"w1q119\", \"w1q109\", \"w1q105\", \"w1q113\", \"w1q101\"]\n",
    "\n",
    "s_ei = [''.join([x, '_ei']) for x in s]\n",
    "\n",
    "meyer[s_ei] = meyer[s].fillna(0)\n",
    "\n",
    "for i, j in zip(s, s_ei):\n",
    "  meyer[i].value_counts(dropna = False)\n",
    "  print('-'*20)\n",
    "  meyer[j].value_counts(dropna = False)\n",
    "  print('='*20)\n",
    "  print('')\n",
    "\n",
    "meyer.drop(columns = s, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIGHT NOW THESE ARE 0S\n",
    "# ONLY 1-3 IS DEFINED IN THE SCALE \n",
    "# (these are the suicide and nssh Qs)\n",
    "# TOMORROW, plot it, and see how the 0 column\n",
    "# compares to the others\n",
    "# and reassign accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w1q32\n",
    "# right or wrong these guys were TREATED as \"no\" in the survey\n",
    "# so I'm going to code them that way.\n",
    "\n",
    "meyer['w1q32'].value_counts(dropna = False)\n",
    "meyer['w1q32'].fillna(2).value_counts(dropna = False)\n",
    "\n",
    "meyer[['w1q32_ei']] = meyer[['w1q32']].fillna(2)\n",
    "\n",
    "meyer[['w1q32_ei']].value_counts(dropna = False)\n",
    "\n",
    "meyer.drop(columns = ['w1q32_ei'], inplace = True)\n",
    "\n",
    "meyer.shape\n",
    "list(meyer.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wednesday, June 5, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think when this is all over, I should reload a clean, fresh copy of the dataframe and see\n",
    "- how many NAs there are per ROW -- I wonder if there are a handful of people who skipped a lot of questions and I've ended up imputing basically their whole row\n",
    "- how many cells I've altered.  it all seems so reasonable when I'm going one column at a time, but when I step back and watch every single column turn into an _ei column, it makes me wonder if I'm destroying my dataset.  If the final tally is staggering, then I should probably go with Hank's suggestion of only picking an handful of columns and ignoring the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case, today I finished doing the imputation as laid out in imputation_plan.xlsx.  Or, mostly.  I changed my mind on a few things as I went, and discovered things along the way.  Most memorably, some column that I meant to impute with the median populated value ended up just being 0s, because once I converted the 97s to 0s, that was the median.  I'm sure there's a way to tell SI to impute only from values in a range, but 0 was faster.  I also cleaned up a bunch more of those 97s et al., and did some subset investigations.  On some poverty and employment measures there were probably more columns that I could have used to predict the missing values, but using measures of central tendency or just the columns that came most readily to mind was fastest, and will have to do for this iteration.\n",
    "\n",
    "Once all the imputation of truly *missing* values was done, I looped back around to look for any of those 7/97/98/99 values that *should* be NAs but aren't.  I did this by looking through every W1 listing in the 831 page documentation, and making a list of which variables have defined values like that, and what they are.  This list is called `problem_children`, and is a list of tuples with structure `(column_name, [list of relevant value(s)])`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "files created: \n",
    "- 2024-06-05_finishing-imputation.py\n",
    "- 2024-06-05_making-a-list-of-potential-7s-98s-etc-to-check.py\n",
    "- MAJOR modifications: 2024-06-04_simple_imputer_stuff.py\n",
    "- MINOR modifications:\n",
    "modified:   2024-06-04_imputation_misc.py  (this is the only one that warrants re-posting)\n",
    "modified:   ../../01_notebooks/notes_plus_all_code.ipynb\n",
    "modified:   ../01_notes/imputation_plan.xlsx\n",
    "modified:   2024-05-31_continuing_messing_with_meyer_columns.py\n",
    "modified:   2024-06-04_imputation_dealing-with-7s-and-97s-indiy-ohe-columns.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 2 in full:\n",
    "2024-06-04_simple_imputer_stuff.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUT because I imputed the 97s to be 0s, and there were so many 97s, that made the \n",
    "# median and mode 0.  So most of these guys just ended up imputed with 0s.\n",
    "# It's not exactly what I wanted (the true missings are people who DID have the \n",
    "# relevant thoughts, but just wouldn't say when), but given that there are so few \n",
    "# true missings, and that this has already eaten up a ton of time, I think it'll\n",
    "# be fine to leave for this iteration.  Put it in the next steps.\n",
    "\n",
    "# OK LAST THING RE-MERGE WOOHOO!\n",
    "meyer.shape # full thing has 250 columns\n",
    "len(s_median) # 31, including ID\n",
    "len(s_mode) # 37, including ID\n",
    "\n",
    "# also let me check some stuff\n",
    "# are the column names what I think they are?\n",
    "s_median_new==list(meyer_median.columns) # True\n",
    "s_mode_new==list(meyer_mode.columns) # True\n",
    "# is studyid the only column name that doesn't end in _ei\n",
    "[c for c in list(meyer_median.columns) if c.split('_')[-1]!='ei']\n",
    "[c for c in list(meyer_mode.columns) if c.split('_')[-1]!='ei']\n",
    "\n",
    "drop_cols = med_cols + mode_cols\n",
    "len(drop_cols) # 68\n",
    "compare = [c for c in drop_cols if c in list(meyer.columns)]\n",
    "len(compare) # 68\n",
    "len(list(meyer_median.columns) + list(meyer_mode.columns)) # 70\n",
    "\n",
    "# safety first\n",
    "# meyer.to_csv('2024-06-05_pre-si-remerging.csv', index = False)\n",
    "# meyer_median.to_csv('2024-06-05_values-imputed-with-median.csv', index = False)\n",
    "# meyer_mode.to_csv('2024-06-05_values-imputed-with-mode.csv', index = False)\n",
    "\n",
    "# drop cols\n",
    "meyer.drop(columns = drop_cols, inplace = True) # [1507 rows x 182 columns]\n",
    "\n",
    "# merge\n",
    "meyer_m = meyer.merge(\n",
    "  meyer_median, left_on = 'studyid', right_on = 'studyid', how = 'left').merge(\n",
    "    meyer_mode, left_on = 'studyid', right_on = 'studyid', how = 'left')\n",
    "\n",
    "meyer_m.shape # (1507, 250)\n",
    "\n",
    "meyer_m.isna().sum()\n",
    "\n",
    "meyer = meyer_m.copy(deep = True)\n",
    "\n",
    "del meyer_m\n",
    "\n",
    "# meyer.to_csv('2024-06-05_si-imputation-done_remerged.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 3 in full:\n",
    "2024-06-05_finishing-imputation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wednesday, June 5, 2024\n",
    "# Imputation Regrouping\n",
    "\n",
    "# Most of my imputation is now done!  \n",
    "\n",
    "# my spreadsheet got messed up, so I'm going back around on some of these\n",
    "\n",
    "# The easy-er ones\n",
    "meyer[['w1q01_ei']] = meyer[['w1q01']].fillna(7)\n",
    "meyer[['w1q02_ei']] = meyer[['w1q02']].fillna(8)\n",
    "meyer[['w1q33_ei']] = meyer[['w1q33']].fillna(0)\n",
    "meyer[['w1q123a_ei']] = meyer[['w1q123a']].fillna(5)\n",
    "meyer[['w1q123b_ei']] = meyer[['w1q123b']].fillna(5)\n",
    "meyer[['w1q114_ei']] = meyer[['w1q114_ei']].fillna(0) # note that this guy already existed\n",
    "meyer[['w1q34_ei']] = meyer[['w1q34']].fillna(0) \n",
    "# I was originally going to impute this ^ from other info, but a 0 is fine and faster\n",
    "meyer[['w1q121_ei']] = meyer[['w1q121']].fillna(0) # nssh age\n",
    "meyer[['w1q122_ei']] = meyer[['w1q122']].fillna(0) # nssh age\n",
    "meyer.shape # (1507, 258)\n",
    "\n",
    "drop_cols = ['w1q01', 'w1q02', 'w1q33', 'w1q123a', \n",
    "  'w1q123b', 'w1q34', 'w1q121', 'w1q122']\n",
    "\n",
    "meyer.drop(columns = drop_cols, inplace = True)\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ooh boy here we go\n",
    "\n",
    "# impute 2 if sum(139)>0, else mode\n",
    "s = ''\n",
    "for i in list(range(1, 11)):\n",
    "  s += f'meyer[\"w1q139_{i}_ei\"] + '\n",
    "\n",
    "meyer['sum_w1q139'] = meyer[\"w1q139_1_ei\"] + meyer[\n",
    "  \"w1q139_2_ei\"] + meyer[\"w1q139_3_ei\"] + meyer[\n",
    "  \"w1q139_4_ei\"] + meyer[\"w1q139_5_ei\"] + meyer[\n",
    "  \"w1q139_6_ei\"] + meyer[\"w1q139_7_ei\"] + meyer[\n",
    "  \"w1q139_8_ei\"] + meyer[\"w1q139_9_ei\"] + meyer[\"w1q139_10_ei\"]\n",
    "\n",
    "meyer['sum_w1q139'].value_counts(dropna = False)\n",
    "\n",
    "meyer[['w1q137_ei']] = meyer[['w1q137']]\n",
    "meyer[['w1q138_ei']] = meyer[['w1q138']]\n",
    "\n",
    "cond1 = meyer['w1q137'].isna()\n",
    "cond2 = meyer['w1q138'].isna()\n",
    "\n",
    "meyer.loc[cond1, 'w1q137_ei'] = np.where(meyer.loc[cond1, 'sum_w1q139']>0, 2, 1)\n",
    "meyer.loc[cond2, 'w1q138_ei'] = np.where(meyer.loc[cond2, 'sum_w1q139']>0, 2, 1)\n",
    "\n",
    "meyer['w1q137'].value_counts(dropna = False)\n",
    "meyer['w1q137_ei'].value_counts(dropna = False)\n",
    "meyer['w1q138'].value_counts(dropna = False)\n",
    "meyer['w1q138_ei'].value_counts(dropna = False)\n",
    "\n",
    "# nice\n",
    "meyer.drop(columns = ['w1q137', 'w1q138', 'sum_w1q139'], inplace = True)\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute 2 if sum(141)>0, else mode\n",
    "meyer['sum_w1q141'] = meyer[\"w1q141_1_ei\"] + meyer[\n",
    "  \"w1q141_2_ei\"] + meyer[\"w1q141_3_ei\"] + meyer[\n",
    "  \"w1q141_4_ei\"] + meyer[\"w1q141_5_ei\"] + meyer[\n",
    "  \"w1q141_6_ei\"] + meyer[\"w1q141_7_ei\"] + meyer[\n",
    "  \"w1q141_8_ei\"] + meyer[\"w1q141_9_ei\"] + meyer[\"w1q141_10_ei\"]\n",
    "\n",
    "meyer['sum_w1q141'].value_counts(dropna = False)\n",
    "\n",
    "meyer[['w1q140_ei']] = meyer[['w1q140']]\n",
    "\n",
    "cond1 = meyer['w1q140'].isna()\n",
    "\n",
    "meyer.loc[cond1, 'w1q140_ei'] = np.where(meyer.loc[cond1, 'sum_w1q141']>0, 2, 1)\n",
    "\n",
    "meyer['w1q140'].value_counts(dropna = False)\n",
    "meyer['w1q140_ei'].value_counts(dropna = False)\n",
    "\n",
    "# nice\n",
    "meyer.drop(columns = ['w1q140', 'sum_w1q141'], inplace = True)\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset investigation; there were exactly 48 do not knows on q2\n",
    "\n",
    "cond1 = meyer['w1q03'].isna()\n",
    "\n",
    "test = meyer.loc[cond1, ['w1q02_ei', 'w1q03']]\n",
    "test['w1q02_ei'].value_counts(dropna = False, normalize = True) # why so many 8s?\n",
    "meyer['w1q02_ei'].value_counts(dropna = False, normalize = True)  # close enough for now\n",
    "\n",
    "# I conducted that test to see if the 48 NAs in Q3 were the same\n",
    "# people as the 48 \"don't knows\" in  Q2, or any other pattern.\n",
    "# I did not find one, and therefore imputed the mode, which is 2.\n",
    "\n",
    "meyer[['w1q03_ei']] = meyer[['w1q03']].fillna(2)\n",
    "\n",
    "meyer.shape\n",
    "meyer.drop(columns = 'w1q03', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute from 142b or .c=True and 142e=False; and/or w1q171_x\n",
    "meyer['w1q142b_ei'].value_counts(dropna = False)\n",
    "meyer['w1q142c_ei'].value_counts(dropna = False)\n",
    "meyer['w1q146d'].value_counts(dropna = False)\n",
    "\n",
    "jobless_142s = ((meyer['w1q142b_ei']==1) | (meyer['w1q142c_ei']==1))\n",
    "cond1 = meyer['w1q146d'].isna()\n",
    "\n",
    "meyer[['w1q146d_ei']] = meyer[['w1q146d']]\n",
    "\n",
    "# meyer.loc[cond1, 'w1q146d_ei'] = np.where(meyer.loc[cond1, 'sum_w1q141']>0, 2, 1)\n",
    "\n",
    "# trying to do the np.where is making my head hurt, so\n",
    "for i in list(range(1507)):\n",
    "  if pd.notna(meyer.loc[i, 'w1q146d_ei'])==True:\n",
    "    continue\n",
    "  elif meyer.loc[i, 'w1q142c_ei']==1:\n",
    "    meyer.loc[i, 'w1q146d_ei']=1\n",
    "  elif meyer.loc[i, 'w1q142b_ei']==1:\n",
    "    meyer.loc[i, 'w1q146d_ei']=1\n",
    "  else:\n",
    "    meyer.loc[i, 'w1q146d_ei']=0\n",
    "\n",
    "meyer['w1q142b_ei'].value_counts(dropna = False)\n",
    "meyer['w1q142c_ei'].value_counts(dropna = False)\n",
    "meyer['w1q146d'].value_counts(dropna = False)\n",
    "meyer['w1q146d_ei'].value_counts(dropna = False)\n",
    "\n",
    "meyer.drop(columns = 'w1q146d', inplace = True)\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute from poverty\n",
    "meyer[['w1q146b_ei']] = meyer[['w1q146b']]\n",
    "\n",
    "money_cols = ['w1q146b', 'w1poverty_i', # 'w1povertycat_i', \n",
    "  'w1q142h_ei', 'w1age', 'geducation', 'w1q175_ei']\n",
    "\n",
    "[c for c in list(meyer.columns) if c.split('_')[-1] not in ['ei', 'i']]\n",
    "\n",
    "# 142h: During the last year have you experienced a major financial crisis; 1=y, 2=n\n",
    "# 175: under water w/ debt; 1=n, 2=y  --- student loans?  maybe but that's getting too bespoke\n",
    "\n",
    "meyer['w1q175_ei'].value_counts()\n",
    "meyer['w1q146b'].value_counts()\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "meyer.loc[(meyer['w1q146b'].isna()), money_cols]\n",
    "\n",
    "a = sorted(list(meyer.columns))\n",
    "\n",
    "# I checked several columns that also have to do with money.\n",
    "# The census questions can address most of these NAs. \n",
    "# I imputed a bunch of values for w1q175, so I'm hesitant\n",
    "# to use it to impute others.  w1q142h_ei (major financial\n",
    "# crisis) is a 2 (no) for all of the remaining NAs.\n",
    "# For the rest I'm going to impute 0.  It's a close tie\n",
    "# between 0 (not true that they don't have enough money\n",
    "# to make ends meet) and 1+2 (somewhat or very true), and\n",
    "# I'm tempted to impute a 1 (somewhat true that they don't\n",
    "# have enough money to make ends meet), but it feels less\n",
    "# presumptuous to impute a 0.\n",
    "\n",
    "# trying to do the np.where is making my head hurt, so\n",
    "for i in list(range(1507)):\n",
    "  if pd.notna(meyer.loc[i, 'w1q146b_ei'])==True:\n",
    "    continue\n",
    "  elif meyer.loc[i, 'w1poverty_i']==1:  # census poverty yes\n",
    "    meyer.loc[i, 'w1q146b_ei']=1\n",
    "  else:\n",
    "    meyer.loc[i, 'w1q146b_ei']=0\n",
    "\n",
    "meyer['w1q146b'].value_counts(dropna = False)\n",
    "meyer['w1q146b_ei'].value_counts(dropna = False)\n",
    "\n",
    "meyer.drop(columns = 'w1q146b', inplace = True)\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset investigation\n",
    "# these are the Qs about US nativity\n",
    "\n",
    "meyer[['w1q166_ei']] = meyer[['w1q166']]\n",
    "meyer[['w1q167_ei']] = meyer[['w1q167']]\n",
    "meyer[['w1q168_ei']] = meyer[['w1q168']]\n",
    "\n",
    "test = meyer[['w1q166', 'w1q167', 'w1q168']]\n",
    "test1 = test[test['w1q166'].isna()] # u born here?\n",
    "test2 = test[test['w1q167'].isna()] # u live here 6-13?\n",
    "test3 = test[test['w1q168'].isna()] # parents NOT born here?\n",
    "\n",
    "# if they left all 3 blank, I'm imputing the mode\n",
    "a = meyer['w1q166'].isna()\n",
    "b = meyer['w1q167'].isna()\n",
    "c = meyer['w1q168'].isna()\n",
    "\n",
    "meyer.loc[(a & b & c), 'w1q166_ei'] = 1\n",
    "meyer.loc[(a & b & c), 'w1q167_ei'] = 1\n",
    "meyer.loc[(a & b & c), 'w1q168_ei'] = 3\n",
    "\n",
    "# look again\n",
    "test = meyer[['w1q166_ei', 'w1q167_ei', 'w1q168_ei']]\n",
    "test1 = test[test['w1q166_ei'].isna()] # u born here?\n",
    "test2 = test[test['w1q167_ei'].isna()] # u live here 6-13?\n",
    "test3 = test[test['w1q168_ei'].isna()] # parents NOT born here?\n",
    "\n",
    "# ok now the rest\n",
    "# looking at the pattern of associated 167s and 168s, I think \n",
    "# it's safe to impute the remaining (n=5) NAs in 166 as 1 \n",
    "\n",
    "meyer[['w1q166_ei']] = meyer[['w1q166_ei']].fillna(1)\n",
    "\n",
    "# looking at the pattern of associated 166s and 168s, I think \n",
    "# it's safe to impute the remaining (n=5) NAs in 167 thusly:\n",
    "a = meyer['w1q166']==2 # I wasn't born here\n",
    "b = meyer['w1q167'].isna()\n",
    "c = ((meyer['w1q168'].notna()) & (meyer['w1q168']<3)) # 1+ parent not born here\n",
    "\n",
    "meyer.loc[(a & b & c), 'w1q167_ei'] = 2 # under those conditions, impute NO\n",
    "meyer[['w1q167_ei']] = meyer[['w1q167_ei']].fillna(1) # otherwise yes\n",
    "\n",
    "# looking at the pattern of associated 166s and 167s, I think \n",
    "# it's safe to impute the remaining (n=5) NAs in 168 as 1 \n",
    "\n",
    "meyer[['w1q168_ei']] = meyer[['w1q168_ei']].fillna(1)\n",
    "\n",
    "meyer.drop(columns = ['w1q166', 'w1q167', 'w1q168'], inplace = True)\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's back this guy up.  I'm getting nervous.\n",
    "# meyer.to_csv('2024-06-05_most-imputation-done_midway-through-nativity.csv', index = False)\n",
    "\n",
    "# let's back this guy up.  I'm getting nervous.\n",
    "# meyer.to_csv('2024-06-05_all-imputation-except-poverty-done.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset investigation\n",
    "meyer[['w1poverty_i_ei']] = meyer[['w1poverty_i']].fillna(0)\n",
    "meyer[['w1povertycat_i_ei']] = meyer[['w1povertycat_i']].fillna(4)\n",
    "\n",
    "meyer.drop(columns = ['w1poverty_i', 'w1povertycat_i'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "meyer.isna().sum().sum() # 0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meyer.to_csv('2024-06-05_all-imputation-done.csv', index = False)\n",
    "\n",
    "# Reorder the columns\n",
    "ordered_cols = sorted(list(meyer.columns))\n",
    "ordered_cols.remove('studyid')\n",
    "ordered_cols = ['studyid'] + ordered_cols\n",
    "len(ordered_cols) # 250\n",
    "\n",
    "meyer = meyer[ordered_cols]\n",
    "meyer.shape\n",
    "\n",
    "meyer.to_csv('2024-06-05_all-imputation-done_reordered.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 4 in full:\n",
    "2024-06-05_making-a-list-of-potential-7s-98s-etc-to-check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wednesday, June 5, 2024\n",
    "# Wrapping up loose values\n",
    "\n",
    "# All of my imputation is done, hooray!!\n",
    "# Now I'm just going back through and looking for all of the\n",
    "# \"non-NA NAs\" (e.g., 97, 98, 99) that I need to replace with \n",
    "# real imputations, lest they through the entire column off.\n",
    "\n",
    "for i in list(meyer.columns):\n",
    "  print(i)\n",
    "  \n",
    "problem_children = [('w1q01_ei', [98]), ('w1q02_ei', [98]), \n",
    "  ('w1q33_ei', [97]), ('w1q34_ei', [97]), ('w1q35_ei', [97]), \n",
    "  ('w1q36_ei', [97]), ('w1q45_ei', [98, 99]), ('w1q46_ei', [98, 99]), \n",
    "  ('w1q47_ei', [98, 99]), ('w1q48_ei', [98, 99]), ('w1q49_ei', [98, 99]), \n",
    "  ('w1q50_ei', [98, 99]), ('w1q51_ei', [98, 99]), ('w1q89_ei', [7]), \n",
    "  ('w1q102_ei', [97]), ('w1q103_ei', [97]), ('w1q104_ei', [97]), \n",
    "  ('w1q106_ei', [97]), ('w1q107_ei', [97]), ('w1q108_ei', [97]),\n",
    "  ('w1q110_ei', [97]), ('w1q111_ei', [0, 97]), ('w1q112_ei', [97]),\n",
    "  ('w1q114_ei', [6, 7, 8, 9, 97]), ('w1q115_ei', [97]), ('w1q116_ei', [97]),\n",
    "  ('w1q117_ei', [97]), ('w1q120_ei', [97]), ('w1q121_ei', [97]),\n",
    "  ('w1q122_ei', [97]), ('w1q123a_ei', [5]), ('w1q123b_ei', [5]), \n",
    "  ('w1q123c_ei', [5]), ('w1q123d_ei', [5]), ('w1q134_ei', [97]), \n",
    "  ('w1q168_ei', [97]), ('w1q136_10_ei', [7, 97]), ('w1q136_1_ei', [7, 97]), \n",
    "  ('w1q136_2_ei', [7, 97]), ('w1q136_3_ei', [7, 97]), \n",
    "  ('w1q136_4_ei', [7, 97]), ('w1q136_5_ei', [7, 97]), \n",
    "  ('w1q136_6_ei', [7, 97]), ('w1q136_7_ei', [7, 97]), \n",
    "  ('w1q136_8_ei', [7, 97]), ('w1q136_9_ei', [7, 97]), \n",
    "  ('w1q139_10_ei', [7, 97]), ('w1q139_1_ei', [7, 97]), \n",
    "  ('w1q139_2_ei', [7, 97]), ('w1q139_3_ei', [7, 97]), \n",
    "  ('w1q139_4_ei', [7, 97]), ('w1q139_5_ei', [7, 97]), \n",
    "  ('w1q139_6_ei', [7, 97]), ('w1q139_7_ei', [7, 97]), \n",
    "  ('w1q139_8_ei', [7, 97]), ('w1q139_9_ei', [7, 97]), \n",
    "  ('w1q141_10_ei', [7, 97]), ('w1q141_1_ei', [7, 97]), \n",
    "  ('w1q141_2_ei', [7, 97]), ('w1q141_3_ei', [7, 97]), \n",
    "  ('w1q141_4_ei', [7, 97]), ('w1q141_5_ei', [7, 97]), \n",
    "  ('w1q141_6_ei', [7, 97]), ('w1q141_7_ei', [7, 97]), \n",
    "  ('w1q141_8_ei', [7, 97]), ('w1q141_9_ei', [7, 97]), \n",
    "  ('w1q143_10_ei', [7, 97]), ('w1q143_1_ei', [7, 97]), \n",
    "  ('w1q143_2_ei', [7, 97]), ('w1q143_3_ei', [7, 97]), \n",
    "  ('w1q143_4_ei', [7, 97]), ('w1q143_5_ei', [7, 97]), \n",
    "  ('w1q143_6_ei', [7, 97]), ('w1q143_7_ei', [7, 97]), \n",
    "  ('w1q143_8_ei', [7, 97]), ('w1q143_9_ei', [7, 97]), \n",
    "  ('w1q145_10_ei', [7, 97]), ('w1q145_1_ei', [7, 97]), \n",
    "  ('w1q145_2_ei', [7, 97]), ('w1q145_3_ei', [7, 97]), \n",
    "  ('w1q145_4_ei', [7, 97]), ('w1q145_5_ei', [7, 97]), \n",
    "  ('w1q145_6_ei', [7, 97]), ('w1q145_7_ei', [7, 97]), \n",
    "  ('w1q145_8_ei', [7, 97]), ('w1q145_9_ei', [7, 97]), \n",
    "  ('w1q163_10_ei', [7, 97]), ('w1q163_1_ei', [7, 97]), \n",
    "  ('w1q163_2_ei', [7, 97]), ('w1q163_3_ei', [7, 97]), \n",
    "  ('w1q163_4_ei', [7, 97]), ('w1q163_5_ei', [7, 97]), \n",
    "  ('w1q163_6_ei', [7, 97]), ('w1q163_7_ei', [7, 97]), \n",
    "  ('w1q163_8_ei', [7, 97]), ('w1q163_9_ei', [7, 97])]\n",
    "\n",
    "\n",
    "s = [\"w1q136_10_ei\", \"w1q136_1_ei\", \"w1q136_2_ei\", \"w1q136_3_ei\", \n",
    "  \"w1q136_4_ei\", \"w1q136_5_ei\", \"w1q136_6_ei\", \"w1q136_7_ei\", \n",
    "  \"w1q136_8_ei\", \"w1q136_9_ei\", \"w1q139_10_ei\", \"w1q139_1_ei\", \n",
    "  \"w1q139_2_ei\", \"w1q139_3_ei\", \"w1q139_4_ei\", \"w1q139_5_ei\", \n",
    "  \"w1q139_6_ei\", \"w1q139_7_ei\", \"w1q139_8_ei\", \"w1q139_9_ei\", \n",
    "  \"w1q141_10_ei\", \"w1q141_1_ei\", \"w1q141_2_ei\", \"w1q141_3_ei\", \n",
    "  \"w1q141_4_ei\", \"w1q141_5_ei\", \"w1q141_6_ei\", \"w1q141_7_ei\", \n",
    "  \"w1q141_8_ei\", \"w1q141_9_ei\", \"w1q143_10_ei\", \"w1q143_1_ei\", \n",
    "  \"w1q143_2_ei\", \"w1q143_3_ei\", \"w1q143_4_ei\", \"w1q143_5_ei\", \n",
    "  \"w1q143_6_ei\", \"w1q143_7_ei\", \"w1q143_8_ei\", \"w1q143_9_ei\", \n",
    "  \"w1q145_10_ei\", \"w1q145_1_ei\", \"w1q145_2_ei\", \"w1q145_3_ei\", \n",
    "  \"w1q145_4_ei\", \"w1q145_5_ei\", \"w1q145_6_ei\", \"w1q145_7_ei\", \n",
    "  \"w1q145_8_ei\", \"w1q145_9_ei\", \"w1q163_10_ei\", \"w1q163_1_ei\", \n",
    "  \"w1q163_2_ei\", \"w1q163_3_ei\", \"w1q163_4_ei\", \"w1q163_5_ei\", \n",
    "  \"w1q163_6_ei\", \"w1q163_7_ei\", \"w1q163_8_ei\", \"w1q163_9_ei\"]\n",
    "\n",
    "g = ''\n",
    "for i in s:\n",
    "  g += f\"('{i}', [7, 97]), \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSVs are in the main project folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thursday, June 6, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes:\n",
    "Today I finished tracking down and handling all the non-NA NAs (7, 97, 98, 99), dropped some more columns, and did all the recoding and transformations I could think of in preparation to combine the columns via `feat_eng_dict`.  I even created a function today!\n",
    "\n",
    "I created a new set-up file today, in which I imported the most up to date version of the CSV that I saved yesterday.  I plan to do the same tomorrow.\n",
    "\n",
    "Today I finally chose a y variable - **kessler6**.  This is a general mental health measure.  I dropped a few rows because they had more than 1 missing value in the features that comprise this scale.  The original authors imputed values for everyone, but I didn't want to have y values with more than 1 component part that had to be imputed.  Interestingly, in doing so, I think (although I'm not sure) that I may have dropped the rows from which many of the other missing values came, too.  \n",
    "\n",
    "Because I was already well underway with my cleaning, I had to go about dropping these rows in a rather roundabout way.  (This would probably be best to whitewash in the final report.)  I reimported a clean copy of the dataset, located all the rows with more than one NA in that set of columns,and then extracted the corresponding `studyid`.  I then dropped the rows with that `studyid` in my working copy of the dataframe.  At the end of the day, my dataset is **(1494, 228)**.\n",
    "\n",
    "**I discovered that the kessler values *are not normally distributed*.**  I tried a few different things, and taking the square root of them showed the most improvement, although the qqplot still looks not-great (not as bad as project 4 though!).  **I created a column for the square root of these values, but *did not delete the original values*.  It will be important to not include them, or the `studyid` in my X matrix!**\n",
    "\n",
    "All of the non-NA NAs were handled a little differently, but (I think) I sufficiently commented the code to explain what I did and why.  Where possible/necessary/I remembered, I tried to mirror what I did with the actual NAs when dealing with these values.\n",
    "\n",
    "I then went through all the columns indicated in my `feat_eng_dict`, and recoded or transformed them as necessary.  I also updated the names of the columns in the dictionary, because all but literally 1 of them have had `'_ei'` and sometimes `'_r'` tacked onto them by now.  What I did with each batch of columns is marked in comments in the code.  This is also the script where I defined and used a function.  Also in this script, I OHE'd the columns about what religion someone was raised in or practices now.  I first collapsed these categories into fewer bins, then OHE'd them.  I chose to drop the resulting column that corresponded to, more or less, \"not religious,\" because it seemed most intuitive.  The other 2 categories are for Christian-ish religions, and other.  Despite it being somewhat disrespectful, I chose to collapse all other religious practices into \"other\" because these categories were all so small on their own.  Even combined, they are dwarfed by the other two options.\n",
    "\n",
    "Finally, when all that was done, I updated my dictionary and saved a fresh copy of it to use tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.py` files created: \n",
    "1. 2024-06-06_set-up.py\n",
    "2. 2024-06-06_non-NA-NAs.py\n",
    "3. 2024-06-06_y-variable.py\n",
    "4. 2024-06-06_column_recoding_for_feat_eng_dict_combinations.py\n",
    "5. 2024-06-06_updated_feat_eng_dict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 2 in full:\n",
    "2024-06-06_non-NA-NAs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thursday, June 6, 2024\n",
    "# \"Imputation\" of non-NA NAs (97s, etc.)\n",
    "\n",
    "# Wait, why did I make problem_children a list rather than a dictionary?\n",
    "\n",
    "s = \"\"\"\n",
    "('w1q01_ei', [98]), ('w1q02_ei', [98]), \n",
    "  ('w1q33_ei', [97]), ('w1q34_ei', [97]), ('w1q35_ei', [97]), \n",
    "  ('w1q36_ei', [97]), ('w1q45_ei', [98, 99]), ('w1q46_ei', [98, 99]), \n",
    "  ('w1q47_ei', [98, 99]), ('w1q48_ei', [98, 99]), ('w1q49_ei', [98, 99]), \n",
    "  ('w1q50_ei', [98, 99]), ('w1q51_ei', [98, 99]), ('w1q89_ei', [7]), \n",
    "  ('w1q102_ei', [97]), ('w1q103_ei', [97]), ('w1q104_ei', [97]), \n",
    "  ('w1q106_ei', [97]), ('w1q107_ei', [97]), ('w1q108_ei', [97]),\n",
    "  ('w1q110_ei', [97]), ('w1q111_ei', [0, 97]), ('w1q112_ei', [97]),\n",
    "  ('w1q114_ei', [6, 7, 8, 9, 97]), ('w1q115_ei', [97]), ('w1q116_ei', [97]),\n",
    "  ('w1q117_ei', [97]), ('w1q120_ei', [97]), ('w1q121_ei', [97]),\n",
    "  ('w1q122_ei', [97]), ('w1q123a_ei', [5]), ('w1q123b_ei', [5]), \n",
    "  ('w1q123c_ei', [5]), ('w1q123d_ei', [5]), ('w1q134_ei', [97]), \n",
    "  ('w1q168_ei', [97]), ('w1q136_10_ei', [7, 97]), ('w1q136_1_ei', [7, 97]), \n",
    "  ('w1q136_2_ei', [7, 97]), ('w1q136_3_ei', [7, 97]), \n",
    "  ('w1q136_4_ei', [7, 97]), ('w1q136_5_ei', [7, 97]), \n",
    "  ('w1q136_6_ei', [7, 97]), ('w1q136_7_ei', [7, 97]), \n",
    "  ('w1q136_8_ei', [7, 97]), ('w1q136_9_ei', [7, 97]), \n",
    "  ('w1q139_10_ei', [7, 97]), ('w1q139_1_ei', [7, 97]), \n",
    "  ('w1q139_2_ei', [7, 97]), ('w1q139_3_ei', [7, 97]), \n",
    "  ('w1q139_4_ei', [7, 97]), ('w1q139_5_ei', [7, 97]), \n",
    "  ('w1q139_6_ei', [7, 97]), ('w1q139_7_ei', [7, 97]), \n",
    "  ('w1q139_8_ei', [7, 97]), ('w1q139_9_ei', [7, 97]), \n",
    "  ('w1q141_10_ei', [7, 97]), ('w1q141_1_ei', [7, 97]), \n",
    "  ('w1q141_2_ei', [7, 97]), ('w1q141_3_ei', [7, 97]), \n",
    "  ('w1q141_4_ei', [7, 97]), ('w1q141_5_ei', [7, 97]), \n",
    "  ('w1q141_6_ei', [7, 97]), ('w1q141_7_ei', [7, 97]), \n",
    "  ('w1q141_8_ei', [7, 97]), ('w1q141_9_ei', [7, 97]), \n",
    "  ('w1q143_10_ei', [7, 97]), ('w1q143_1_ei', [7, 97]), \n",
    "  ('w1q143_2_ei', [7, 97]), ('w1q143_3_ei', [7, 97]), \n",
    "  ('w1q143_4_ei', [7, 97]), ('w1q143_5_ei', [7, 97]), \n",
    "  ('w1q143_6_ei', [7, 97]), ('w1q143_7_ei', [7, 97]), \n",
    "  ('w1q143_8_ei', [7, 97]), ('w1q143_9_ei', [7, 97]), \n",
    "  ('w1q145_10_ei', [7, 97]), ('w1q145_1_ei', [7, 97]), \n",
    "  ('w1q145_2_ei', [7, 97]), ('w1q145_3_ei', [7, 97]), \n",
    "  ('w1q145_4_ei', [7, 97]), ('w1q145_5_ei', [7, 97]), \n",
    "  ('w1q145_6_ei', [7, 97]), ('w1q145_7_ei', [7, 97]), \n",
    "  ('w1q145_8_ei', [7, 97]), ('w1q145_9_ei', [7, 97]), \n",
    "  ('w1q163_10_ei', [7, 97]), ('w1q163_1_ei', [7, 97]), \n",
    "  ('w1q163_2_ei', [7, 97]), ('w1q163_3_ei', [7, 97]), \n",
    "  ('w1q163_4_ei', [7, 97]), ('w1q163_5_ei', [7, 97]), \n",
    "  ('w1q163_6_ei', [7, 97]), ('w1q163_7_ei', [7, 97]), \n",
    "  ('w1q163_8_ei', [7, 97]), ('w1q163_9_ei', [7, 97])\"\"\"\n",
    "  \n",
    "d = s.replace(\"(\", \"\").replace(\")\", \"\").replace(\"', [\", \"': [\").replace(' \\n', '').replace('\\n', '')\n",
    "d\n",
    "\n",
    "# w1q120 is the only one without an _ei\n",
    "problem_children = {'w1q01_ei': [98], 'w1q02_ei': [98],  'w1q33_ei': [97], \n",
    "  'w1q34_ei': [97], 'w1q35_ei': [97],  'w1q36_ei': [97], 'w1q45_ei': [98, 99], \n",
    "  'w1q46_ei': [98, 99],  'w1q47_ei': [98, 99], 'w1q48_ei': [98, 99], \n",
    "  'w1q49_ei': [98, 99],  'w1q50_ei': [98, 99], 'w1q51_ei': [98, 99], \n",
    "  'w1q89_ei': [7],  'w1q102_ei': [97], 'w1q103_ei': [97], 'w1q104_ei': [97],  \n",
    "  'w1q106_ei': [97], 'w1q107_ei': [97], 'w1q108_ei': [97],  'w1q110_ei': [97], \n",
    "  'w1q111_ei': [0, 97], 'w1q112_ei': [97],  'w1q114_ei': [6, 7, 8, 9, 97], \n",
    "  'w1q115_ei': [97], 'w1q116_ei': [97],  'w1q117_ei': [97], 'w1q120': [97], \n",
    "  'w1q121_ei': [97],  'w1q122_ei': [97], 'w1q123a_ei': [5], 'w1q123b_ei': [5],  \n",
    "  'w1q123c_ei': [5], 'w1q123d_ei': [5], 'w1q134_ei': [97],  'w1q168_ei': [97], \n",
    "  'w1q136_10_ei': [7, 97], 'w1q136_1_ei': [7, 97],  'w1q136_2_ei': [7, 97], \n",
    "  'w1q136_3_ei': [7, 97],  'w1q136_4_ei': [7, 97], 'w1q136_5_ei': [7, 97],  \n",
    "  'w1q136_6_ei': [7, 97], 'w1q136_7_ei': [7, 97],  'w1q136_8_ei': [7, 97], \n",
    "  'w1q136_9_ei': [7, 97],  'w1q139_10_ei': [7, 97], 'w1q139_1_ei': [7, 97],  \n",
    "  'w1q139_2_ei': [7, 97], 'w1q139_3_ei': [7, 97],  'w1q139_4_ei': [7, 97], \n",
    "  'w1q139_5_ei': [7, 97],  'w1q139_6_ei': [7, 97], 'w1q139_7_ei': [7, 97],  \n",
    "  'w1q139_8_ei': [7, 97], 'w1q139_9_ei': [7, 97],  'w1q141_10_ei': [7, 97], \n",
    "  'w1q141_1_ei': [7, 97],  'w1q141_2_ei': [7, 97], 'w1q141_3_ei': [7, 97],  \n",
    "  'w1q141_4_ei': [7, 97], 'w1q141_5_ei': [7, 97],  'w1q141_6_ei': [7, 97], \n",
    "  'w1q141_7_ei': [7, 97],  'w1q141_8_ei': [7, 97], 'w1q141_9_ei': [7, 97],  \n",
    "  'w1q143_10_ei': [7, 97], 'w1q143_1_ei': [7, 97],  'w1q143_2_ei': [7, 97], \n",
    "  'w1q143_3_ei': [7, 97],  'w1q143_4_ei': [7, 97], 'w1q143_5_ei': [7, 97],  \n",
    "  'w1q143_6_ei': [7, 97], 'w1q143_7_ei': [7, 97],  'w1q143_8_ei': [7, 97], \n",
    "  'w1q143_9_ei': [7, 97],  'w1q145_10_ei': [7, 97], 'w1q145_1_ei': [7, 97],  \n",
    "  'w1q145_2_ei': [7, 97], 'w1q145_3_ei': [7, 97],  'w1q145_4_ei': [7, 97], \n",
    "  'w1q145_5_ei': [7, 97],  'w1q145_6_ei': [7, 97], 'w1q145_7_ei': [7, 97],  \n",
    "  'w1q145_8_ei': [7, 97], 'w1q145_9_ei': [7, 97],  'w1q163_10_ei': [7, 97], \n",
    "  'w1q163_1_ei': [7, 97],  'w1q163_2_ei': [7, 97], 'w1q163_3_ei': [7, 97],  \n",
    "  'w1q163_4_ei': [7, 97], 'w1q163_5_ei': [7, 97],  'w1q163_6_ei': [7, 97], \n",
    "  'w1q163_7_ei': [7, 97],  'w1q163_8_ei': [7, 97], 'w1q163_9_ei': [7, 97]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_jail = []\n",
    "for c, v in problem_children.items():\n",
    "  a = list(meyer[c].unique())\n",
    "  for i in v:\n",
    "    if i in a:\n",
    "      column_jail.append((c, v, i))\n",
    "      break\n",
    "\n",
    "for i in column_jail:\n",
    "  if i[0]=='w1q02_ei':\n",
    "    continue\n",
    "  meyer[i[0]].value_counts(dropna = False)\n",
    "  meyer[i[0]].value_counts(dropna = False, normalize = True)\n",
    "  print('')\n",
    "  print('='*20)\n",
    "  print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the remaining values, and what I intend to do about them.\n",
    "\n",
    "# [('w1q01_ei', [98], 98), 98.0      2      0.001327\n",
    "# For only 2 values, imputing the median seems fine.\n",
    "\n",
    "# Because I *already* imputed values, RE-fitting the si may well end up with a DIFFERENT \n",
    "# median.  If I have time, I should just go back and skip si, and instead use .fillna() \n",
    "# to put in the POPULATED median, which is provided in the documentation (and I could \n",
    "# easily verify myself).  I'm not going to take the time NOW to go back and change all \n",
    "# that, but I am going to do it for these 2 values.\n",
    "cond = meyer['w1q01_ei']==98.0\n",
    "meyer.loc[cond, 'w1q01_ei']=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('w1q02_ei', [98], 98), 98.0     48    0.031851\n",
    "# I'm just going to drop this column. I've done SO MUCH imputation and it's really making\n",
    "#  me nervous.  I'm not planning to really hone in deep on optimism for the future or \n",
    "# whatever (THIS TIME), so I'm just going to chuck it for now.\n",
    "meyer.drop(columns = 'w1q02_ei', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('w1q33_ei', [97], 97), \n",
    "# The Q is how long someone's been with their partner, and the 97s are people who said\n",
    "# they don't have partners.  The logical imputation is 0.  There are no natural 0s.\n",
    "cond = meyer['w1q33_ei']==97.0\n",
    "meyer.loc[cond, 'w1q33_ei']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('w1q89_ei', [7], 7), \n",
    "# This is the question about how often people smoke cigarettes, and the 7s are people\n",
    "# who said on the previous question that they do not smoke (or failed to answer the \n",
    "# question, whom I (think I) imputed as no). Therefore, I'll impute \"not at all\" here.\n",
    "cond = meyer['w1q89_ei']==7.0\n",
    "meyer.loc[cond, 'w1q89_ei']=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('w1q111_ei', [0, 97], 0), \n",
    "# Ok, truly F these age-when columns.  They are way too complicated to get into in the \n",
    "# time I have.  They're colinear(ish) with the current-age column, or at least not\n",
    "# independent.  It is not possible for a 19 year old person to have made their first\n",
    "# suicide attempt at 25.  Plus, I highly doubt the relationship between age of attempt(s)\n",
    "# and overall mental health is linear.  I'd be way more worried about someone who made \n",
    "# their first suicide attempt at 11 OR at 35 than I would be about someone who made their\n",
    "# first attempt at 19.  A lot of people have suicidal ideation in their late teens/early\n",
    "# 20s, because they're old enough to feel the weight of the world but not old enough for \n",
    "# their brains to be done cooking.  I'd have to do a ton of research and reading to be sure\n",
    "# (and I don't have time for that either), but my intuition says it's probably a parabolic\n",
    "# relationship between the age of attempt and the redness of the flag.  PLUS, imputating\n",
    "# these guys has been the bane of my existence for over a week.  I think what I did already\n",
    "# was impute the missings and 97s as 0s, but now I'm thinking that was a bad idea!  A true\n",
    "# value of 0 (if it were even psychologically possible, which it isn't - I HIGHLY doubt a\n",
    "# baby can understand the concept of suicide well enough to have suicidal ideation, but\n",
    "# even if they did, there's no way that person would remember it as an adult), would be \n",
    "# EXTREMELY alarming, in a way that a linear model is just not equipped to deal with.  \n",
    "# Furthermore, getting back to the point about the curvilinear nature of this, I don't think\n",
    "# there's really ANY good value to impute here!  If one wanted to look at age effects, one \n",
    "# would really have to limit the sample to people who have made attempts.  \n",
    "\n",
    "# I also considered a scheme wherein I would subtract the min age from the max age to get a \n",
    "# range of time that they were suicidal, and if the min and max were both populated but\n",
    "# neither was 0 (i.e., they made multiple attempts at the same age) I would sub in a 1 (for\n",
    "# that 1 year of suicidality), and if one or the other (but not both!) was 0, or they only \n",
    "# answered the \"how old were you?\" question (i.e., they indicated that they'd only made one \n",
    "# attempt), I would also sub in 1, for that 1 year where they were feeling that way, but if\n",
    "# neither was populated or they were both 0 or whatever (i.e., for people who made no \n",
    "# attempts ever) I would keep a 0 - so it'd be 0 for people who never attempted, and a \n",
    "# minimum of 1 for people who did ever attempt - but in addition to being way too complicated\n",
    "# and messy for the amount of time I have left, that runs the risk of dramatically \n",
    "# misrepresenting people's actual relationship with suicidality.  Someone who made one attempt\n",
    "# at 17 because of teen stuff and another, more or less completely unrelated, attempt at 47 \n",
    "# because they were going through a messy divorce would be indicated to have been suicidal \n",
    "# for THIRTY YEARS, which isn't true, and would make it look like they were 6x more impacted \n",
    "# by sucidality than someone who made their first attempt 18 at their last attempt at 23, \n",
    "# even if the latter person made 20 attempts in that time, and they were all due to the same\n",
    "# ongoing issues.  I know I'm probably overthinking this, but to do a range seems so reductive\n",
    "# as to be practically meaningless - and definitely not worth the time, effort, and \n",
    "# dimensionality it would take to accomplish it.  I am going to just drop these age columns.\n",
    "age_cols = ['w1q102_ei', 'w1q103_ei', 'w1q104_ei',  # these are the suicide/nssh age Qs\n",
    "  'w1q106_ei', 'w1q107_ei', 'w1q108_ei', 'w1q110_ei', 'w1q111_ei', 'w1q112_ei', \n",
    "  'w1q115_ei', 'w1q116_ei', 'w1q117_ei', 'w1q120', 'w1q121_ei', 'w1q122_ei',\n",
    "  'w1q134_ei']  # this how old were you when you got conversion therapy \n",
    "len(age_cols)\n",
    "meyer.drop(columns = age_cols, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('w1q114_ei', [6, 7, 8, 9, 97], 6), \n",
    "# This is the one asking how many suicide attempts people made, and they used a messy scale.\n",
    "# I get why they did it, I probably would have done the same, but it still makes it ordinal\n",
    "# rather than truly linear.  I could leave it as is, or change these values to the minimum\n",
    "# in their range (or something like that).  I could also make it categorical, like\n",
    "# 0 attempts, 1 attempt, multiple attempts.\n",
    "np.linspace(6, 21, 4) # oh it's exactly what they have.\n",
    "\n",
    "# I'm going to reassign those values to the minimum value in their range so that it's \n",
    "# sort of linear, but then that's enough.  That's fine.\n",
    "cond7 = meyer['w1q114_ei']==7.0\n",
    "meyer.loc[cond7, 'w1q114_ei']=11\n",
    "\n",
    "cond8 = meyer['w1q114_ei']==8.0\n",
    "meyer.loc[cond8, 'w1q114_ei']=16\n",
    "\n",
    "cond9 = meyer['w1q114_ei']==9.0\n",
    "meyer.loc[cond9, 'w1q114_ei']=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the 123 questions are about outness, but they're coded weird. It's are you out to...\n",
    "# 1 = All, 2 = Most, 3 = Some, 4 = None, 5 = don't know/does not apply/[missing value]\n",
    "# So firstly, it's counter-intuitive.  Higher numbers = less out.  Sort of.  Secondly, 5 is\n",
    "# in a weird place on the scale  I think I'm going to recode it like this:\n",
    "# 4 -> 0 = I can confidently say I am out to \"None\" of these people. (LOWEST OUTNESS)\n",
    "# 5 -> 1 = I'm being wishy-washy about how out I am to these people.\n",
    "# 3 -> 2 = Some\n",
    "# 2 -> 3 = Most\n",
    "# 1 -> 4 = All (HIGHEST OUTNESS)\n",
    "# ('w1q123a_ei', [5], 5), ('w1q123b_ei', [5], 5), ('w1q123c_ei', [5], 5), ('w1q123d_ei', [5], 5)]\n",
    "\n",
    "meyer[['w1q123a_ei_r', 'w1q123b_ei_r', 'w1q123c_ei_r', 'w1q123d_ei_r']] = meyer[[\n",
    "  'w1q123a_ei', 'w1q123b_ei', 'w1q123c_ei', 'w1q123d_ei']]\n",
    "\n",
    "recode_123s = {4: 0, 5: 1, 3: 2, 2: 3, 1: 4}\n",
    "\n",
    "old_cols = ['w1q123a_ei', 'w1q123b_ei', 'w1q123c_ei', 'w1q123d_ei']\n",
    "new_cols = ['w1q123a_ei_r', 'w1q123b_ei_r', 'w1q123c_ei_r', 'w1q123d_ei_r']\n",
    "\n",
    "for old, new in list(zip(old_cols, new_cols)):\n",
    "  meyer[new] = meyer[old].map(recode_123s)\n",
    "\n",
    "check = meyer[['w1q123a_ei', 'w1q123a_ei_r', 'w1q123b_ei', 'w1q123b_ei_r', \n",
    "  'w1q123c_ei', 'w1q123c_ei_r', 'w1q123d_ei', 'w1q123d_ei_r']]\n",
    "# Yeah checks out!\n",
    "\n",
    "meyer.drop(columns = old_cols, inplace = True)\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't remember if I actually *executed* all that code, so let me test it again. I reran \n",
    "# the code to define problem_children just to be safe, and now I'm going to modify the loop.\n",
    "column_jail = []\n",
    "columns_done = []\n",
    "z = 0 # I'm a little hazy on `continue` so this is just a check\n",
    "for c, v in problem_children.items():\n",
    "\n",
    "  # if that column name isn't in the list, find out why\n",
    "  if c not in list(meyer.columns):\n",
    "    if c=='w1q02_ei':\n",
    "      columns_done.append(c)\n",
    "      z+=1\n",
    "      continue\n",
    "  \n",
    "    elif c in age_cols:\n",
    "      columns_done.append(c)\n",
    "      z+=1\n",
    "      continue\n",
    "    \n",
    "    # if neither of those work, try renaming it\n",
    "    c_r = ''.join([c, '_r'])\n",
    "    if c_r not in list(meyer.columns):\n",
    "      print(c)\n",
    "      columns_done.append(c)\n",
    "      z+=1\n",
    "      continue\n",
    "    elif c_r in list(meyer.columns): \n",
    "      c = c_r # rename it and continue thru the loop\n",
    "\n",
    "  # if that column name is in the list, or if c_r is, do this\n",
    "  a = list(meyer[c].unique())\n",
    "  for i in v:\n",
    "    if i in a:\n",
    "      column_jail.append((c, v, i))\n",
    "      break\n",
    "  columns_done.append(c)\n",
    "  z+=1\n",
    "z==len(problem_children.keys())\n",
    "\n",
    "column_jail\n",
    "# oh it's just [('w1q114_ei', [6, 7, 8, 9, 97], 6)]\n",
    "meyer['w1q114_ei'].value_counts(dropna = False)\n",
    "# w1q114_ei\n",
    "# 0.0     1146\n",
    "# 1.0      251\n",
    "# 3.0       32\n",
    "# 2.0       30\n",
    "# 6.0       17\n",
    "# 4.0       14\n",
    "# 5.0        9\n",
    "# 11.0       6\n",
    "# 16.0       1\n",
    "# 21.0       1\n",
    "# Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's totally fine, I fixed the thing I wanted to fix with 6+\n",
    "# I should probably indicate that I changed stuff though.\n",
    "\n",
    "meyer.shape # (1507, 233)\n",
    "meyer[['w1q114_ei_r']] = meyer[['w1q114_ei']]\n",
    "meyer.shape\n",
    "meyer.drop(columns = ['w1q114_ei'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "\n",
    "# ok!  Woohoo!  Let's save another copy.\n",
    "meyer.to_csv('2024-06-06_non-NA-NAs-fixed.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 3 in full:\n",
    "2024-06-06_y-variable.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thursday, June 6, 2024\n",
    "\n",
    "# I should probably also choose a y variable.  [several hours latere]  Let's go with\n",
    "# Kessler6!  It's a general mental health scale.  According to the documentation, there were\n",
    "# 1491 complete cases of this, and honestly I'd be fine just dropping the rest.  Let me see\n",
    "# how many missing values there were per row in the imputed versions.  If it's only 1, I \n",
    "# might let it slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oh no is it skewed\n",
    "# Plot a histogram of it\n",
    "plt.figure(figsize = (16, 9));\n",
    "plt.hist(meyer['w1kessler6_i'], bins = 'auto', color = 'purple');\n",
    "plt.suptitle(f'Distribution of {Y}', size = 24)\n",
    "# plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "plt.xlabel('w1kessler6_i', size = 20);\n",
    "plt.ylabel('Frequency', size = 20);\n",
    "plt.xticks(size = 16, rotation = 60);\n",
    "plt.yticks(size = 16)\n",
    "#plt.tight_layout()\n",
    "# plt.savefig(f'./{a}/{i}_histogram.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot a histogram of it\n",
    "plt.figure(figsize = (16, 9));\n",
    "plt.hist(meyer['w1kessler6_i']**(1/2), bins = 'auto', color = 'purple');\n",
    "plt.suptitle(f'Distribution of {Y}', size = 24)\n",
    "# plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "plt.xlabel('w1kessler6_i', size = 20);\n",
    "plt.ylabel('Frequency', size = 20);\n",
    "plt.xticks(size = 16, rotation = 60);\n",
    "plt.yticks(size = 16)\n",
    "#plt.tight_layout()\n",
    "# plt.savefig(f'./{a}/{i}_histogram.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot a histogram of it\n",
    "plt.figure(figsize = (16, 9));\n",
    "plt.hist(meyer['w1kessler6_i']**(1/3), bins = 'auto', color = 'purple');\n",
    "plt.suptitle(f'Distribution of {Y}', size = 24)\n",
    "# plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "plt.xlabel('w1kessler6_i', size = 20);\n",
    "plt.ylabel('Frequency', size = 20);\n",
    "plt.xticks(size = 16, rotation = 60);\n",
    "plt.yticks(size = 16)\n",
    "#plt.tight_layout()\n",
    "# plt.savefig(f'./{a}/{i}_histogram.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot a histogram of it\n",
    "\n",
    "# log transformation gets a div by 0 warning\n",
    "# plt.figure(figsize = (16, 9));\n",
    "# plt.hist(np.log(meyer['w1kessler6_i']), bins = 'auto', color = 'purple');\n",
    "# plt.suptitle(f'Distribution of {Y}', size = 24)\n",
    "# # plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "# plt.xlabel('w1kessler6_i', size = 20);\n",
    "# plt.ylabel('Frequency', size = 20);\n",
    "# plt.xticks(size = 16, rotation = 60);\n",
    "# plt.yticks(size = 16)\n",
    "# #plt.tight_layout()\n",
    "# # plt.savefig(f'./{a}/{i}_histogram.png')\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# QQplot of raw kessler\n",
    "sm.qqplot(meyer['w1kessler6_i'], line='45');\n",
    "plt.suptitle(f'QQ-Plot of kessler6_i', size = 20);\n",
    "plt.xticks(size = 14, rotation = 60);\n",
    "plt.yticks(size = 14);\n",
    "plt.tight_layout();\n",
    "# plt.savefig('./03_images/output/dropout_rate_qqplot.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# QQplot of sqrt kessler\n",
    "sm.qqplot((meyer['w1kessler6_i']**0.5), line='45');\n",
    "plt.suptitle(f'QQ-Plot of kessler6_i', size = 20);\n",
    "plt.xticks(size = 14, rotation = 60);\n",
    "plt.yticks(size = 14);\n",
    "plt.tight_layout();\n",
    "# plt.savefig('./03_images/output/dropout_rate_qqplot.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# QQplot of cbrt kessler\n",
    "sm.qqplot((meyer['w1kessler6_i']**(1/3)), line='45');\n",
    "plt.suptitle(f'QQ-Plot of kessler6_i', size = 20);\n",
    "plt.xticks(size = 14, rotation = 60);\n",
    "plt.yticks(size = 14);\n",
    "plt.tight_layout();\n",
    "# plt.savefig('./03_images/output/dropout_rate_qqplot.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Ok, I'll square root it.\n",
    "\n",
    "meyer['kessler6_sqrt'] = (meyer['w1kessler6_i']**0.5)\n",
    "meyer.shape\n",
    "\n",
    "# let's get a copy of that too\n",
    "meyer.to_csv('2024-06-06_sqrt-kessler.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 4 in full:\n",
    "2024-06-06_column_recoding_for_feat_eng_dict_combinations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite Scale Calculations\n",
    "\n",
    "# In this script, I need to go back to `feat_eng_dict`, that list I made in \n",
    "# `2024-05-23_thru_2024-05-30_exploring_Meyer_2023_dataset.py`, and use it to \n",
    "# combine some of these columns into composite columns and drop the rest.\n",
    "\n",
    "unaltered_col_names = [\n",
    "  c.replace('_r', '').replace('_ei', '') for c in list(meyer.columns)]\n",
    "  \n",
    "for k, v in feat_eng_dict.items():\n",
    "  a = v[1:]\n",
    "  for j in a:\n",
    "    if ((j not in unaltered_col_names) & (j not in list(meyer.columns))):\n",
    "      print(k, j)\n",
    "\n",
    "# redefine it here without those\n",
    "feat_eng_dict = {'pers_well_being': ['sum', 'w1q01'],\n",
    "  'neighb_welcoming': ['mean', 'w1q19a', 'w1q19b', 'w1q19c', 'w1q19d'],\n",
    "  'age_awakening': ['min','w1q45', 'w1q46', 'w1q47', 'w1q48'],\n",
    "  'age_out': ['min', 'w1q49', 'w1q50', 'w1q51'],\n",
    "  'health_insurance': ['binarize', 'w1q64_1', 'w1q64_2', 'w1q64_3', 'w1q64_4', \n",
    "    'w1q64_5', 'w1q64_6', 'w1q64_7', 'w1q64_8', 'w1q64_9', 'w1q64_10', \n",
    "    'w1q64_11', 'w1q64_12', 'w1q64_13', 'w1q64_t_num'], \n",
    "  'serious_health_cond': ['binarize', 'w1q74_5', 'w1q74_6', 'w1q74_10', \n",
    "    'w1q74_11', 'w1q74_14', 'w1q74_17', 'w1q74_18', 'w1q74_20'], \n",
    "  'disabled': ['binarize', 'w1q75', 'w1q76'],\n",
    "  'suicidal_ideation': ['sum', 'w1q101', 'w1q105', 'w1q109'], \n",
    "  'suicide_attempts': ['recode', 'w1q113', 'w1q114'],\n",
    "  'outness': ['sum', 'w1q123a', 'w1q123b', 'w1q123c', 'w1q123d', 'w1q124'],\n",
    "  'abusive_treatment': ['sum', 'w1q135a', 'w1q135b', \n",
    "    'w1q135c', 'w1q135d', 'w1q135e', 'w1q135f'],\n",
    "  'work_neg_outcomes': ['recode', 'w1q137', 'w1q138'], # account for age\n",
    "  'abus_treat_non_queer': ['binarize', 'w1q136_1', 'w1q136_5', 'w1q136_6', \n",
    "    'w1q136_8', 'w1q136_9', 'w1q136_10'],\n",
    "  'stress_past_year_gen': ['recode', 'w1q142a', 'w1q142h', 'w1q142i'],\n",
    "  'stress_past_year_work': ['recode', 'w1q142b', 'w1q142c', 'w1q142e'],\n",
    "  'stress_past_year_interpersonal': ['recode', 'w1q142d', 'w1q142f', 'w1q142g'],\n",
    "  'stress_past_year_crime': ['recode', 'w1q142j', 'w1q142k'],\n",
    "  'work_disc_non_queer': ['binarize', 'w1q139_1', 'w1q139_5', 'w1q139_6', \n",
    "    'w1q139_8', 'w1q139_9', 'w1q139_10'],\n",
    "  'housing_disc_non_queer': ['binarize', 'w1q141_1', 'w1q141_5', 'w1q141_6', \n",
    "    'w1q141_8', 'w1q141_9', 'w1q141_10'],\n",
    "  'stress_past_year_non_queer': ['binarize', 'w1q143_1', 'w1q143_5', 'w1q143_6', \n",
    "    'w1q143_8', 'w1q143_9', 'w1q143_10'],\n",
    "  'daily_discr_non_queer': ['binarize', 'w1q145_1', 'w1q145_5', 'w1q145_6'],\n",
    "  'childhd_bullying_non_queer': ['binarize', 'w1q163_1', 'w1q163_5', 'w1q163_6', \n",
    "    'w1q163_8', 'w1q163_9', 'w1q163_10'],\n",
    "  'abus_treat_sex_gender': ['binarize', 'w1q136_2', 'w1q136_3', 'w1q136_4'],\n",
    "  'work_disc_sex_gender': ['binarize', 'w1q139_2', 'w1q139_3', 'w1q139_4'],\n",
    "  'housing_disc_sex_gender': ['binarize', 'w1q141_2', 'w1q141_3', 'w1q141_4'],\n",
    "  'stress_past_year_sex_gender': ['binarize', 'w1q143_2', 'w1q143_3', 'w1q143_4'],\n",
    "  'daily_discr_sex_gender': ['binarize', 'w1q145_2', 'w1q145_3', 'w1q145_4', \n",
    "    'w1q145_8', 'w1q145_9', 'w1q145_10'],\n",
    "  'childhd_bullying_sex_gender': ['binarize', 'w1q163_2', 'w1q163_3', 'w1q163_4'],\n",
    "  'religiosity': ['recode', 'w1q179', 'w1q180', 'w1q181'], \n",
    "  'chronic_strain': ['sum', 'w1q146a', 'w1q146b', 'w1q146c', 'w1q146d', 'w1q146e', \n",
    "    'w1q146f', 'w1q146g', 'w1q146h', 'w1q146i', 'w1q146j', 'w1q146k', 'w1q146l']}\n",
    "\n",
    "# then go back up and check again\n",
    "# got them all!\n",
    "\n",
    "# Now I'm going to go through and find which ones have NOT been altered\n",
    "for k, v in feat_eng_dict.items():\n",
    "  a = v[1:]\n",
    "  for j in a:\n",
    "    if j in list(meyer.columns):\n",
    "      print(k, j)\n",
    "\n",
    "# oh dear just one\n",
    "for k, v in feat_eng_dict.items():\n",
    "  a = v[:1]\n",
    "  b = ['_'.join([x, 'ei']) for x in v[1:]]\n",
    "  c = a + b\n",
    "  feat_eng_dict[k] = c\n",
    "  \n",
    "# manually fix this one\n",
    "feat_eng_dict['health_insurance'] = ['binarize', 'w1q64_1_ei', \n",
    "  'w1q64_2_ei', 'w1q64_3_ei', 'w1q64_4_ei', 'w1q64_5_ei', 'w1q64_6_ei', \n",
    "  'w1q64_7_ei', 'w1q64_8_ei', 'w1q64_9_ei', 'w1q64_10_ei', \n",
    "  'w1q64_11_ei', 'w1q64_12_ei', 'w1q64_13_ei', 'w1q64_t_num']\n",
    "  \n",
    "# and these\n",
    "feat_eng_dict['suicide_attempts'] = ['recode', 'w1q113_ei', 'w1q114_ei_r']\n",
    "feat_eng_dict['outness'] = ['sum', 'w1q123a_ei_r', \n",
    "  'w1q123b_ei_r', 'w1q123c_ei_r', 'w1q123d_ei_r', 'w1q124_ei']\n",
    "  \n",
    "# check again\n",
    "for k, v in feat_eng_dict.items():\n",
    "  a = v[1:]\n",
    "  for j in a:\n",
    "    if j not in list(meyer.columns):\n",
    "      print(k, j)\n",
    "      \n",
    "      \n",
    "for k, v in feat_eng_dict.items():\n",
    "  print(k, v)\n",
    "  print('')\n",
    "\n",
    "feat_eng_dict['health_insurance'] = ['binarize', 'w1q64_2_ei', \n",
    "  'w1q64_3_ei', 'w1q64_4_ei', 'w1q64_5_ei', 'w1q64_6_ei', \n",
    "  'w1q64_7_ei', 'w1q64_8_ei', 'w1q64_9_ei', 'w1q64_10_ei', \n",
    "  'w1q64_11_ei', 'w1q64_12_ei', 'w1q64_13_ei', 'w1q64_t_num']\n",
    "\n",
    "# cut\n",
    "drop_items = ['pers_well_being', 'age_awakening', 'age_out', 'neighb_welcoming']\n",
    "drop_cols = ['w1q45_ei', 'w1q46_ei', 'w1q47_ei', 'w1q48_ei', 'w1q49_ei', 'w1q50_ei', \n",
    "  'w1q51_ei']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mess with the columns before computing\n",
    "\n",
    "def recode(dictry, name, cols):\n",
    "  '''cols is a list of columns that need to be recoded.  \n",
    "  name is the name I want to give the composite column based on cols\n",
    "  dict is the STRING name of the dictionary (I need that for the func to modify it)\n",
    "    this function will spit out the appropriate syntax, but only for THIS df.'''\n",
    "  for i in cols[1:]:\n",
    "    print(f\"meyer[['{i}_r']] = meyer[['{i}']]\")\n",
    "  print('')\n",
    "  for i in cols[1:]:\n",
    "    print(f\"meyer['{i}'].value_counts(dropna = False).sort_index()\")\n",
    "    print(f\"meyer['{i}_r'].value_counts(dropna = False).sort_index()\")\n",
    "  print('')\n",
    "  print(f\"meyer.drop(columns = {cols[1:]}).shape\")\n",
    "  print(f\"meyer.drop(columns = {cols[1:]}, inplace = True)\")\n",
    "  print('')\n",
    "  cols_r = cols[:1] + [''.join([x, '_r']) for x in cols[1:]]\n",
    "  print(f\"{dictry}['{name}'] = {cols_r}\")\n",
    "# I then ran this a bunch of times to generate the code seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse code these so that 1=bad neighborhood and 0=fine\n",
    "meyer[['w1q19a_ei_r']] = meyer[['w1q19a_ei']]-1\n",
    "meyer[['w1q19b_ei_r']] = meyer[['w1q19b_ei']]-1\n",
    "meyer[['w1q19c_ei_r']] = meyer[['w1q19c_ei']]-1\n",
    "meyer[['w1q19d_ei_r']] = meyer[['w1q19d_ei']]-1\n",
    "meyer.drop(columns = ['w1q19a_ei', 'w1q19b_ei', 'w1q19c_ei', 'w1q19d_ei']).shape\n",
    "meyer.drop(columns = ['w1q19a_ei', 'w1q19b_ei', 'w1q19c_ei', 'w1q19d_ei'], inplace = True)\n",
    "feat_eng_dict['bad_neighbhd'] = ['sum', 'w1q19a_ei_r', 'w1q19b_ei_r', 'w1q19c_ei_r', 'w1q19d_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse code these so that 1=disabled, 0=non-disabled\n",
    "# currently it's 1=disabled, 2=non-disabled\n",
    "# (1-2)*(-1)==1, (2-2)*(-1)==0\n",
    "meyer[['w1q75_ei_r']] = abs(meyer[['w1q75_ei']]-2)\n",
    "meyer[['w1q76_ei_r']] = abs(meyer[['w1q76_ei']]-2)\n",
    "\n",
    "meyer['w1q75_ei'].value_counts(dropna = False, sort = True, ascending = True)\n",
    "meyer['w1q75_ei_r'].value_counts(dropna = False, sort = True, ascending = True)\n",
    "meyer['w1q76_ei'].value_counts(dropna = False, sort = True, ascending = True)\n",
    "meyer['w1q76_ei_r'].value_counts(dropna = False, sort = True, ascending = True)\n",
    "\n",
    "meyer.drop(columns = ['w1q75_ei', 'w1q76_ei']).shape\n",
    "meyer.drop(columns = ['w1q75_ei', 'w1q76_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['disabled'] = ['binarize', 'w1q75_ei_r', 'w1q76_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode these to eliminate the 0 (created during imputation), then 0-base\n",
    "meyer[['w1q101_ei_r']] = meyer[['w1q101_ei']]\n",
    "meyer[['w1q105_ei_r']] = meyer[['w1q105_ei']]\n",
    "meyer[['w1q109_ei_r']] = meyer[['w1q109_ei']]\n",
    "\n",
    "for i in ['sum', 'w1q101_ei', 'w1q105_ei', 'w1q109_ei']:\n",
    "  print(f\"cond = meyer['{i}']==0\")\n",
    "cond1 = meyer['w1q101_ei']==0\n",
    "cond2 = meyer['w1q105_ei']==0\n",
    "cond3 = meyer['w1q109_ei']==0\n",
    "\n",
    "meyer.loc[cond1, 'w1q101_ei_r'] = 1\n",
    "meyer.loc[cond2, 'w1q105_ei_r'] = 1\n",
    "meyer.loc[cond3, 'w1q109_ei_r'] = 1\n",
    "\n",
    "meyer['w1q101_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q101_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q105_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q105_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q109_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q109_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q101_ei', 'w1q105_ei', 'w1q109_ei']).shape\n",
    "meyer.drop(columns = ['w1q101_ei', 'w1q105_ei', 'w1q109_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['suicidal_ideation'] = ['sum', 'w1q101_ei_r', 'w1q105_ei_r', 'w1q109_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ack I forgot to 0-base\n",
    "# recode these to eliminate the 0 (created during imputation), then 0-base\n",
    "meyer[['w1q101_ei_r']] = meyer[['w1q101_ei_r']]-1\n",
    "meyer[['w1q105_ei_r']] = meyer[['w1q105_ei_r']]-1\n",
    "meyer[['w1q109_ei_r']] = meyer[['w1q109_ei_r']]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for these\n",
    "# actually, do I need both of these?\n",
    "\n",
    "cond1 = meyer['w1q113_ei']>1\n",
    "cond2 = meyer['w1q114_ei_r']==0\n",
    "\n",
    "check = meyer.loc[(cond1 & cond2), ['w1q113_ei', 'w1q114_ei_r']]\n",
    "# both times it's 3, 0\n",
    "# I'm guessing they didn't want to say how many\n",
    "# I'm going to impute 2 for them.\n",
    "meyer.loc[(cond1 & cond2), 'w1q114_ei_r'] = 2\n",
    "\n",
    "# And with that done, the first column is redundant!\n",
    "# This entry in the dictionary now only has one column in its\n",
    "# composite, and I actually think it would be better to combine\n",
    "# it with the previous one, for an overall 'suicidality' column\n",
    "\n",
    "meyer.shape\n",
    "meyer[['w1q113_ei_r']] = meyer[['w1q113_ei']]\n",
    "# meyer[['w1q114_ei_r_r']] = meyer[['w1q114_ei_r']]\n",
    "\n",
    "meyer['w1q113_ei'].value_counts(dropna = False).sort_index()\n",
    "# w1q113_ei\n",
    "# 0.0       9\n",
    "# 1.0    1122\n",
    "# 2.0     251\n",
    "# 3.0     112\n",
    "\n",
    "# meyer['w1q113_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q114_ei_r'].value_counts(dropna = False).sort_index()\n",
    "# w1q114_ei_r\n",
    "# 0.0     1131\n",
    "# 1.0      251\n",
    "# [etc.]   112   # I manually added them up\n",
    "\n",
    "# meyer['w1q114_ei_r_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q113_ei']).shape\n",
    "meyer.drop(columns = ['w1q113_ei'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_eng_dict['suicide_attempts'] = ['recode', 'w1q113_ei_r', 'w1q114_ei_r_r']\n",
    "drop_items.append('suicidal_ideation')\n",
    "drop_items.append('suicide_attempts')\n",
    "feat_eng_dict['suicidality'] = ['sum', 'w1q101_ei_r', 'w1q105_ei_r', 'w1q109_ei_r', 'w1q114_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I already got the 123s, but I need to reverse code 124\n",
    "# here's what it is now -> what I want it to be\n",
    "# MOST VISIBLE\n",
    "# 1 -> \n",
    "# 2 -> \n",
    "# 3 -> \n",
    "# 4 -> 1\n",
    "# 5 -> 0\n",
    "# LEAST VISIBLE\n",
    "# oh it's just 5 minus thing\n",
    "\n",
    "meyer[['w1q124_ei_r']] = 5-meyer[['w1q124_ei']]\n",
    "\n",
    "meyer['w1q123a_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q123b_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q123c_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q123d_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q124_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q124_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q124_ei']).shape\n",
    "meyer.drop(columns = ['w1q124_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['outness'] = ['mean', 'w1q123a_ei_r', 'w1q123b_ei_r', \n",
    "  'w1q123c_ei_r', 'w1q123d_ei_r', 'w1q124_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these guys I just need to 0-base\n",
    "meyer[['w1q135a_ei_r']] = meyer[['w1q135a_ei']]-1\n",
    "meyer[['w1q135b_ei_r']] = meyer[['w1q135b_ei']]-1\n",
    "meyer[['w1q135c_ei_r']] = meyer[['w1q135c_ei']]-1\n",
    "meyer[['w1q135d_ei_r']] = meyer[['w1q135d_ei']]-1\n",
    "meyer[['w1q135e_ei_r']] = meyer[['w1q135e_ei']]-1\n",
    "meyer[['w1q135f_ei_r']] = meyer[['w1q135f_ei']]-1\n",
    "\n",
    "meyer['w1q135a_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135a_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135b_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135b_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135c_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135c_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135d_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135d_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135e_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135e_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135f_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q135f_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q135a_ei', 'w1q135b_ei', 'w1q135c_ei', 'w1q135d_ei', 'w1q135e_ei', 'w1q135f_ei']).shape\n",
    "meyer.drop(columns = ['w1q135a_ei', 'w1q135b_ei', 'w1q135c_ei', 'w1q135d_ei', 'w1q135e_ei', 'w1q135f_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['abusive_treatment'] = ['sum', 'w1q135a_ei_r', 'w1q135b_ei_r', 'w1q135c_ei_r', 'w1q135d_ei_r', 'w1q135e_ei_r', 'w1q135f_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same \n",
    "meyer[['w1q137_ei_r']] = meyer[['w1q137_ei']]-1\n",
    "meyer[['w1q138_ei_r']] = meyer[['w1q138_ei']]-1\n",
    "\n",
    "meyer['w1q137_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q137_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q138_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q138_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q137_ei', 'w1q138_ei']).shape\n",
    "meyer.drop(columns = ['w1q137_ei', 'w1q138_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['work_neg_outcomes'] = ['sum', 'w1q137_ei_r', 'w1q138_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode code these guys so 1=stress and 0=not\n",
    "meyer[['w1q142a_ei_r']] = abs(meyer[['w1q142a_ei']]-2)\n",
    "meyer[['w1q142h_ei_r']] = abs(meyer[['w1q142h_ei']]-2)\n",
    "meyer[['w1q142i_ei_r']] = abs(meyer[['w1q142i_ei']]-2)\n",
    "\n",
    "meyer['w1q142a_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142a_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142h_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142h_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142i_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142i_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q142a_ei', 'w1q142h_ei', 'w1q142i_ei']).shape\n",
    "meyer.drop(columns = ['w1q142a_ei', 'w1q142h_ei', 'w1q142i_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['stress_past_year_gen'] = ['sum', 'w1q142a_ei_r', 'w1q142h_ei_r', 'w1q142i_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these too\n",
    "meyer[['w1q142b_ei_r']] = abs(meyer[['w1q142b_ei']]-2)\n",
    "meyer[['w1q142c_ei_r']] = abs(meyer[['w1q142c_ei']]-2)\n",
    "meyer[['w1q142e_ei_r']] = abs(meyer[['w1q142e_ei']]-2)\n",
    "\n",
    "meyer['w1q142b_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142b_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142c_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142c_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142e_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142e_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q142b_ei', 'w1q142c_ei', 'w1q142e_ei']).shape\n",
    "meyer.drop(columns = ['w1q142b_ei', 'w1q142c_ei', 'w1q142e_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['stress_past_year_work'] = ['sum', 'w1q142b_ei_r', 'w1q142c_ei_r', 'w1q142e_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and these\n",
    "meyer[['w1q142d_ei_r']] = abs(meyer[['w1q142d_ei']]-2)\n",
    "meyer[['w1q142f_ei_r']] = abs(meyer[['w1q142f_ei']]-2)\n",
    "meyer[['w1q142g_ei_r']] = abs(meyer[['w1q142g_ei']]-2)\n",
    "\n",
    "meyer['w1q142d_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142d_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142f_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142f_ei_r'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142g_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q142g_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q142d_ei', 'w1q142f_ei', 'w1q142g_ei']).shape\n",
    "meyer.drop(columns = ['w1q142d_ei', 'w1q142f_ei', 'w1q142g_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['stress_past_year_interpersonal'] = ['sum', 'w1q142d_ei_r', 'w1q142f_ei_r', 'w1q142g_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and these\n",
    "meyer[['w1q142j_ei_r']] = abs(meyer[['w1q142j_ei']]-2)\n",
    "meyer[['w1q142k_ei_r']] = abs(meyer[['w1q142k_ei']]-2)\n",
    "\n",
    "print(meyer['w1q142j_ei'].value_counts(dropna = False).sort_index())\n",
    "print(meyer['w1q142j_ei_r'].value_counts(dropna = False).sort_index())\n",
    "print(meyer['w1q142k_ei'].value_counts(dropna = False).sort_index())\n",
    "print(meyer['w1q142k_ei_r'].value_counts(dropna = False).sort_index())\n",
    "\n",
    "meyer.drop(columns = ['w1q142j_ei', 'w1q142k_ei']).shape\n",
    "meyer.drop(columns = ['w1q142j_ei', 'w1q142k_ei'], inplace = True)\n",
    "\n",
    "feat_eng_dict['stress_past_year_crime'] = ['sum', 'w1q142j_ei_r', 'w1q142k_ei_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# religiosity \n",
    "\n",
    "# here's what 179 is now; 1 and 2 are massive in 180\n",
    "# 1 Protestant (for example, Baptist, Methodist) 295 19.4 %\n",
    "# 2 Roman Catholic 133 8.8 %\n",
    "# 3 Mormon (Church of Jesus Christ of Latter-day Saints or LDS) 10 0.7 %\n",
    "# 4 Orthodox (Greek, Russian, or another Orthodox church) 6 0.4 %\n",
    "\n",
    "# 5 Jewish 38 2.5 %\n",
    "# 6 Muslim 3 0.2 %\n",
    "# 7 Buddhist 30 2.0 %\n",
    "# 8 Hindu 1 0.1 %\n",
    "# 11 Spiritual 262 17.3 %\n",
    "# 12 Something else 95 6.3 %\n",
    "\n",
    "# 9 Atheist (do not believe in God) 192 12.6 %\n",
    "# 10 Agnostic (not sure if there is a God) 156 10.3 %\n",
    "# 13 Nothing in particular 273 18.0 %\n",
    "\n",
    "# collapse 3-8 into \"other organized\"\n",
    "# .... actually collapse more, bcz this needs to be OHE'd\n",
    "\n",
    "# 1-4 christian-influenced religious     1\n",
    "# 5-8, 11-12  non-christian religious    5\n",
    "# 9-10, 13 not religious                 make this 0, because it's literally none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode the religion columns\n",
    "relig_recode = {1: 1, 2: 1, 3: 1, 4: 1, # vaguely christian\n",
    "  5: 5, 6: 5, 7: 5, 8: 5, 11: 5, 12: 5, # religious but not christian\n",
    "  9: 0, 10: 0, 13: 0} # not religious\n",
    "\n",
    "# 179 and 180 are the \"are you religious / were you raised religious\" Qs\n",
    "meyer['w1q179_ei_r'] = meyer['w1q179_ei'].map(relig_recode)\n",
    "meyer['w1q180_ei_r'] = meyer['w1q180_ei'].map(relig_recode)\n",
    "\n",
    "print(meyer['w1q179_ei'].value_counts(dropna = False).sort_index())\n",
    "print(meyer['w1q179_ei_r'].value_counts(dropna = False).sort_index())\n",
    "print(meyer['w1q180_ei'].value_counts(dropna = False).sort_index())\n",
    "print(meyer['w1q180_ei_r'].value_counts(dropna = False).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "meyer.drop(columns = ['w1q179_ei', 'w1q180_ei']).shape\n",
    "meyer.drop(columns = ['w1q179_ei', 'w1q180_ei'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "# I actually cannot think of a meaningful way to combine these.I think the \n",
    "# thing to do is just leave them all in alone, then see what pops as meaningful.\n",
    "drop_items.append('religiosity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 181 is about how often you attend religious services\n",
    "meyer[['w1q181_ei_r']] = 6-(meyer[['w1q181_ei']])\n",
    "\n",
    "meyer['w1q181_ei'].value_counts(dropna = False).sort_index()\n",
    "meyer['w1q181_ei_r'].value_counts(dropna = False).sort_index()\n",
    "\n",
    "meyer.drop(columns = ['w1q181_ei']).shape\n",
    "meyer.drop(columns = ['w1q181_ei'], inplace = True)\n",
    "meyer.shape\n",
    "\n",
    "# woohoo!  save it to a csv\n",
    "meyer.to_csv('2024-06-06_recodes-for-feat-eng-dict-done_not-ohe-yet.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just ohe them now while I'm thinking about it\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(drop = None, # I want to manually drop a specific one\n",
    "  handle_unknown = 'ignore', sparse_output = False) \n",
    "\n",
    "ctx = ColumnTransformer(transformers=[('one_hot', ohe, ['w1q179_ei_r', 'w1q180_ei_r'])],\n",
    "    remainder = 'passthrough', verbose_feature_names_out=False)\n",
    "\n",
    "meyer_ohe = pd.DataFrame(data = ctx.fit_transform(meyer), \n",
    "  columns = ctx.get_feature_names_out())\n",
    "meyer_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the non-religious ones\n",
    "meyer_ohe.drop(columns = ['w1q179_ei_r_0', 'w1q180_ei_r_0']).shape\n",
    "meyer_ohe.drop(columns = ['w1q179_ei_r_0', 'w1q180_ei_r_0'], inplace = True)\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the new columns more semantic names\n",
    "relig_rename = {'w1q179_ei_r_1': 'w1q179_ei_r_relig_christ', \n",
    "  'w1q179_ei_r_5': 'w1q179_ei_r_relig_other', \n",
    "  'w1q180_ei_r_1': 'w1q180_ei_r_relig_christ', \n",
    "  'w1q180_ei_r_5': 'w1q180_ei_r_relig_other'}\n",
    "\n",
    "meyer_ohe.rename(columns = relig_rename, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it back in the right name\n",
    "meyer = meyer_ohe.copy(deep = True)\n",
    "\n",
    "# drop the columns I set aside before\n",
    "meyer.drop(columns = drop_cols, inplace = True)\n",
    "\n",
    "# save to csv\n",
    "meyer.to_csv('2024-06-06_recodes-for-feat-eng-dict-done_religious-ohe-done.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finish updating the feat_eng_list\n",
    "\n",
    "# safety first\n",
    "feat_eng_backup = feat_eng_dict.copy()\n",
    "\n",
    "for i in drop_items:\n",
    "  del feat_eng_dict[i]\n",
    "# thanks to stack overflow for the knowledge that that's how to del dict entries\n",
    "# https://stackoverflow.com/questions/5844672/delete-an-element-from-a-dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oh one more thing\n",
    "# Reorder the columns\n",
    "ordered_cols = sorted(list(meyer.columns))\n",
    "ordered_cols.remove('studyid')\n",
    "ordered_cols = ['studyid'] + ordered_cols\n",
    "len(ordered_cols) # 228\n",
    "len(list(meyer.columns)) # 228\n",
    "meyer.shape # (1494, 228)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meyer = meyer[ordered_cols]\n",
    "meyer.shape\n",
    "\n",
    "# save to csv\n",
    "meyer.to_csv('2024-06-06_recodes-for-feat-eng-dict-done_religious-ohe-done_reordered.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 5 in full:\n",
    "2024-06-06_updated_feat_eng_dict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thursday, June 6, 2024 - End of day\n",
    "# updated version of my feat_eng_dict\n",
    "\n",
    "feat_eng_dict = {\n",
    "  'health_insurance': [\n",
    "    'binarize', 'w1q64_2_ei', 'w1q64_3_ei', 'w1q64_4_ei', 'w1q64_5_ei', \n",
    "    'w1q64_6_ei', 'w1q64_7_ei', 'w1q64_8_ei', 'w1q64_9_ei', 'w1q64_10_ei', \n",
    "    'w1q64_11_ei', 'w1q64_12_ei', 'w1q64_13_ei', 'w1q64_t_num'], \n",
    "  'serious_health_cond': [\n",
    "    'binarize', 'w1q74_5_ei', 'w1q74_6_ei', 'w1q74_10_ei', 'w1q74_11_ei', \n",
    "    'w1q74_14_ei', 'w1q74_17_ei', 'w1q74_18_ei', 'w1q74_20_ei'], \n",
    "  'disabled': [\n",
    "    'binarize', 'w1q75_ei_r', 'w1q76_ei_r'], \n",
    "  'outness': [\n",
    "    'mean', 'w1q123a_ei_r', 'w1q123b_ei_r', 'w1q123c_ei_r', 'w1q123d_ei_r',\n",
    "    'w1q124_ei_r'], \n",
    "  'abusive_treatment': [\n",
    "    'sum', 'w1q135a_ei_r', 'w1q135b_ei_r', 'w1q135c_ei_r', 'w1q135d_ei_r', \n",
    "    'w1q135e_ei_r', 'w1q135f_ei_r'], \n",
    "  'work_neg_outcomes': [\n",
    "    'sum', 'w1q137_ei_r', 'w1q138_ei_r'], \n",
    "  'abus_treat_non_queer': [\n",
    "    'binarize', 'w1q136_1_ei', 'w1q136_5_ei', 'w1q136_6_ei', 'w1q136_8_ei', \n",
    "    'w1q136_9_ei', 'w1q136_10_ei'], \n",
    "  'stress_past_year_gen': [\n",
    "    'sum', 'w1q142a_ei_r', 'w1q142h_ei_r', 'w1q142i_ei_r'], \n",
    "  'stress_past_year_work': [\n",
    "    'sum', 'w1q142b_ei_r', 'w1q142c_ei_r', 'w1q142e_ei_r'], \n",
    "  'stress_past_year_interpersonal': [\n",
    "    'sum', 'w1q142d_ei_r', 'w1q142f_ei_r', 'w1q142g_ei_r'], \n",
    "  'stress_past_year_crime': [\n",
    "    'sum', 'w1q142j_ei_r', 'w1q142k_ei_r'], \n",
    "  'work_disc_non_queer': [\n",
    "    'binarize', 'w1q139_1_ei', 'w1q139_5_ei', 'w1q139_6_ei', 'w1q139_8_ei', \n",
    "    'w1q139_9_ei', 'w1q139_10_ei'], \n",
    "  'housing_disc_non_queer': [\n",
    "    'binarize', 'w1q141_1_ei', 'w1q141_5_ei', 'w1q141_6_ei', 'w1q141_8_ei', \n",
    "    'w1q141_9_ei', 'w1q141_10_ei'], \n",
    "  'stress_past_year_non_queer': [\n",
    "    'binarize', 'w1q143_1_ei', 'w1q143_5_ei', 'w1q143_6_ei', 'w1q143_8_ei', \n",
    "    'w1q143_9_ei', 'w1q143_10_ei'], \n",
    "  'daily_discr_non_queer': [\n",
    "    'binarize', 'w1q145_1_ei', 'w1q145_5_ei', 'w1q145_6_ei'], \n",
    "  'childhd_bullying_non_queer': [\n",
    "    'binarize', 'w1q163_1_ei', 'w1q163_5_ei', 'w1q163_6_ei', 'w1q163_8_ei', \n",
    "    'w1q163_9_ei', 'w1q163_10_ei'], \n",
    "  'abus_treat_sex_gender': [\n",
    "    'binarize', 'w1q136_2_ei', 'w1q136_3_ei', 'w1q136_4_ei'], \n",
    "  'work_disc_sex_gender': [\n",
    "    'binarize', 'w1q139_2_ei', 'w1q139_3_ei', 'w1q139_4_ei'], \n",
    "  'housing_disc_sex_gender': [\n",
    "    'binarize', 'w1q141_2_ei', 'w1q141_3_ei', 'w1q141_4_ei'], \n",
    "  'stress_past_year_sex_gender': [\n",
    "    'binarize', 'w1q143_2_ei', 'w1q143_3_ei', 'w1q143_4_ei'], \n",
    "  'daily_discr_sex_gender': [\n",
    "    'binarize', 'w1q145_2_ei', 'w1q145_3_ei', 'w1q145_4_ei', 'w1q145_8_ei', \n",
    "    'w1q145_9_ei', 'w1q145_10_ei'], \n",
    "  'childhd_bullying_sex_gender': [\n",
    "    'binarize', 'w1q163_2_ei', 'w1q163_3_ei', 'w1q163_4_ei'], \n",
    "  'chronic_strain': [\n",
    "    'sum', 'w1q146a_ei', 'w1q146b_ei', 'w1q146c_ei', 'w1q146d_ei', 'w1q146e_ei', \n",
    "    'w1q146f_ei', 'w1q146g_ei', 'w1q146h_ei', 'w1q146i_ei', 'w1q146j_ei', \n",
    "    'w1q146k_ei', 'w1q146l_ei'], \n",
    "  'bad_neighbhd': [\n",
    "    'sum', 'w1q19a_ei_r', 'w1q19b_ei_r', 'w1q19c_ei_r', 'w1q19d_ei_r'], \n",
    "  'suicidality': ['sum', 'w1q101_ei_r', 'w1q105_ei_r', 'w1q109_ei_r', 'w1q114_ei_r']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Friday, June 7, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today I finally got to run my `feat_eng_dict`!  I made some changes to it, and just summed up a lot of the things I was originally going to binarize.  I wanted to get a good look at them to see if there were differences based on the extent to which whatever the thing is has happened to someone, and in a handful of cases there were, but for the most part I think I would have been better off binarizing them.  I will try to do that in the future.  **Throughout the course of this evening, I have been thinking that I have to treat all of these columns the same way.  At this moment, I no longer think that's true.**  I think it's probably fine to say, \"I looked at these guys, and some of them had magnitude effects and some of them didn't.  I binarized the ones that didn't, and kept the others.\"\n",
    "\n",
    "I edited my autoplots function (`autoplots_v2.py`) to include qqplots.  This makes it take even longer to run and the output folder even more comically large, but it is handy.\n",
    "\n",
    "I then conducted some exploratory data analysis, mostly through the production of graphs.  These graphs can be found in the images folder.  In addition to the frequency distributions of each variable and their relationship (shown through scatterplots) with the target variable, I also created qqplots for each variable, and for 3 transformed versions of each variable.  The purpose of this was to assess each variable's normality, and determine whether any of them would benefit from a linear transformation.  I ultimately decided that this was not likely to useful for any variable other than the target, which is the square root of the Kessler scores.\n",
    "\n",
    "Look at these plots also revealed a number of variables that still need to be one-hot encoded, 0-based, or otherwise modified.  I would also like to rename the columns to be more intuitive, so that I don't have to contiually refer to the documentation.  I opted not to do all that before running my first model, but it would be good to do it before including any of those variables in the model.  I started several lists to do these cleaning tasks, but did not finish any of them.  These lists can be found in `2024-06-07-eda.py`, `2024-06-07_NEW_feat_eng_dict.py`, and `2024-06-07_columns-i-want-to-screw-with-some-more-and-how.py`.  When I have time to address these cleaning issues, I intend to rename and modify these scripts.\n",
    "\n",
    "Finally, I created my first model!  I flipped through the scatterplots and just eyeballed some predictors that seemed reasonable, and put them all into a linear regression.  I conducted a train-test split for a little bit of a check on the model, to get a sense of how overfit it might be, but I kept the test size small because this model is mostly for inference.  I am not concerned about its applicability to new data in the future, because newer data is likely to be legitimately different than this data from 2016.  I do, however, want to keep an eye on overfitting, especially as I add more predictors to my model.  Dimensionality has been a concern throughout this entire process, and I am wary of allowing my model to coil itself too tightly around the data.  Although I am not concerned about this model generalizing perfectly to future data, I also do not want it to be so bespoke to the current data that it becomes unsuitable for interpretation.  In other words, if it becomes so overfit that it cannot even generalize to testing data from the same dataset, then its coefficients are unlikely to be meaningful for any level of analysis.\n",
    "\n",
    "I did some cross validation because it seemed like a good idea to demonstrate that I know how to do that, although without gridsearching over anything, I'm not sure it's really necessary.\n",
    "\n",
    "I used several metrics to evaluate my model, and I think it's a decent model for the area.  $R^2$ was around 0.6 for both training and testing, and all of the loss functions were <1.  (This makes sense, because the y-variable itself is a square root.  Squaring a float <1 will make it smaller, not larger.)  This implies that my model is usually within a range of +/-1 with its predictions of the square root of Kessler scores (range: 0-4.9), and within +/-0.5 for the re-squared values (range: 0-24).\n",
    "\n",
    "I honestly can't believe how good this thing is for being so quickly slapped together.\n",
    "\n",
    "The most up to date CSV is **2024-06-07_h21-m19-s52_end-of-day.csv**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other files created:\n",
    "- 2 folders worth of plots in `dsb318-capstone/03_images/02_all_graphs/`\n",
    "\n",
    "`.py` files created: \n",
    "1. 2024-06-07_set-up.py\n",
    "2. 2024-06-07_updated_feat_eng_dict.py\n",
    "3. 2024-06-07_column-combination-at-last.py\n",
    "4. 2024-06-07-eda.py\n",
    "5. 2024-06-07_NEW_feat_eng_dict.py\n",
    "6. 2024-06-07_columns-i-want-to-screw-with-some-more-and-how.py\n",
    "7. 2024-06-07_modeling-part-1.py\n",
    "8. autoplots_v2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 1 in full:\n",
    "1. 2024-06-07_set-up.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capstone Setup\n",
    "\n",
    "# Module Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, date, time\n",
    "from string import capwords\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working Directory\n",
    "os.getcwd()\n",
    "#os.chdir('C:/Users/emily/Git_Stuff/General_Assembly/04_Projects/project-capstone')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set these right off the bat\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "\n",
    "# THIS IS THE ORIGINAL\n",
    "# meyer = pd.read_csv(\n",
    "#   './potential_datasets/2024_05_23_download_ICPSR_Meyer_2023_generations_data_attempt_2/ICPSR_37166/DS0007/37166-0007-Data.tsv', \n",
    "#   sep = '\\t', low_memory=False, na_values = ' ') # Thanks to ibrahim rupawala for highlighting the na_values argument\n",
    "#   # https://stackoverflow.com/questions/13445241/replacing-blank-values-white-space-with-nan-in-pandas/47105408#47105408\n",
    "\n",
    "# THIS IS THE MOST UP TO DATE VERSION FOR WORKING ON\n",
    "meyer = pd.read_csv('2024-06-06_recodes-for-feat-eng-dict-done_religious-ohe-done_reordered.csv')\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 2 in full:\n",
    "2. 2024-06-07_updated_feat_eng_dict.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thursday, June 6, 2024 - End of day\n",
    "# updated version of my feat_eng_dict\n",
    "\n",
    "feat_eng_dict = {\n",
    "  'health_insurance': [\n",
    "    'binarize', 'w1q64_2_ei', 'w1q64_3_ei', 'w1q64_4_ei', 'w1q64_5_ei', \n",
    "    'w1q64_6_ei', 'w1q64_7_ei', 'w1q64_8_ei', 'w1q64_9_ei', 'w1q64_10_ei', \n",
    "    'w1q64_11_ei', 'w1q64_12_ei', 'w1q64_13_ei', 'w1q64_t_num'], \n",
    "  'serious_health_cond': [\n",
    "    'binarize', 'w1q74_5_ei', 'w1q74_6_ei', 'w1q74_10_ei', 'w1q74_11_ei', \n",
    "    'w1q74_14_ei', 'w1q74_17_ei', 'w1q74_18_ei', 'w1q74_20_ei'], \n",
    "  'disabled': [\n",
    "    'binarize', 'w1q75_ei_r', 'w1q76_ei_r'], \n",
    "  'outness': [\n",
    "    'mean', 'w1q123a_ei_r', 'w1q123b_ei_r', 'w1q123c_ei_r', 'w1q123d_ei_r',\n",
    "    'w1q124_ei_r'], \n",
    "  'abusive_treatment': [\n",
    "    'sum', 'w1q135a_ei_r', 'w1q135b_ei_r', 'w1q135c_ei_r', 'w1q135d_ei_r', \n",
    "    'w1q135e_ei_r', 'w1q135f_ei_r'], \n",
    "  'work_neg_outcomes': [\n",
    "    'sum', 'w1q137_ei_r', 'w1q138_ei_r'], \n",
    "  'abus_treat_non_queer': [\n",
    "    'sum', 'w1q136_1_ei', 'w1q136_5_ei', 'w1q136_6_ei', 'w1q136_8_ei', \n",
    "    'w1q136_9_ei', 'w1q136_10_ei'], \n",
    "  'stress_past_year_gen': [\n",
    "    'sum', 'w1q142a_ei_r', 'w1q142h_ei_r', 'w1q142i_ei_r'], \n",
    "  'stress_past_year_work': [\n",
    "    'sum', 'w1q142b_ei_r', 'w1q142c_ei_r', 'w1q142e_ei_r'], \n",
    "  'stress_past_year_interpersonal': [\n",
    "    'sum', 'w1q142d_ei_r', 'w1q142f_ei_r', 'w1q142g_ei_r'], \n",
    "  'stress_past_year_crime': [\n",
    "    'sum', 'w1q142j_ei_r', 'w1q142k_ei_r'], \n",
    "  'work_disc_non_queer': [\n",
    "    'sum', 'w1q139_1_ei', 'w1q139_5_ei', 'w1q139_6_ei', 'w1q139_8_ei', \n",
    "    'w1q139_9_ei', 'w1q139_10_ei'], \n",
    "  'housing_disc_non_queer': [\n",
    "    'sum', 'w1q141_1_ei', 'w1q141_5_ei', 'w1q141_6_ei', 'w1q141_8_ei', \n",
    "    'w1q141_9_ei', 'w1q141_10_ei'], \n",
    "  'stress_past_year_non_queer': [\n",
    "    'sum', 'w1q143_1_ei', 'w1q143_5_ei', 'w1q143_6_ei', 'w1q143_8_ei', \n",
    "    'w1q143_9_ei', 'w1q143_10_ei'], \n",
    "  'daily_discr_non_queer': [\n",
    "    'sum', 'w1q145_1_ei', 'w1q145_5_ei', 'w1q145_6_ei'], \n",
    "  'childhd_bullying_non_queer': [\n",
    "    'sum', 'w1q163_1_ei', 'w1q163_5_ei', 'w1q163_6_ei', 'w1q163_8_ei', \n",
    "    'w1q163_9_ei', 'w1q163_10_ei'], \n",
    "  'abus_treat_sex_gender': [\n",
    "    'sum', 'w1q136_2_ei', 'w1q136_3_ei', 'w1q136_4_ei'], \n",
    "  'work_disc_sex_gender': [\n",
    "    'sum', 'w1q139_2_ei', 'w1q139_3_ei', 'w1q139_4_ei'], \n",
    "  'housing_disc_sex_gender': [\n",
    "    'sum', 'w1q141_2_ei', 'w1q141_3_ei', 'w1q141_4_ei'], \n",
    "  'stress_past_year_sex_gender': [\n",
    "    'sum', 'w1q143_2_ei', 'w1q143_3_ei', 'w1q143_4_ei'], \n",
    "  'daily_discr_sex_gender': [\n",
    "    'sum', 'w1q145_2_ei', 'w1q145_3_ei', 'w1q145_4_ei', 'w1q145_8_ei', \n",
    "    'w1q145_9_ei', 'w1q145_10_ei'], \n",
    "  'childhd_bullying_sex_gender': [\n",
    "    'sum', 'w1q163_2_ei', 'w1q163_3_ei', 'w1q163_4_ei'], \n",
    "  'chronic_strain': [\n",
    "    'sum', 'w1q146a_ei', 'w1q146b_ei', 'w1q146c_ei', 'w1q146d_ei', 'w1q146e_ei', \n",
    "    'w1q146f_ei', 'w1q146g_ei', 'w1q146h_ei', 'w1q146i_ei', 'w1q146j_ei', \n",
    "    'w1q146k_ei', 'w1q146l_ei'], \n",
    "  'bad_neighbhd': [\n",
    "    'sum', 'w1q19a_ei_r', 'w1q19b_ei_r', 'w1q19c_ei_r', 'w1q19d_ei_r'], \n",
    "  'suicidality': ['sum', 'w1q101_ei_r', 'w1q105_ei_r', 'w1q109_ei_r', 'w1q114_ei_r']}\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 3 in full:\n",
    "3. 2024-06-07_column-combination-at-last.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friday, June 7, 2024\n",
    "# Feature Engineering!  Whoo!\n",
    "\n",
    "# In this script, I will finally execute my feat_eng_dict to create\n",
    "# composite score columns and drop the individual parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to look at the values in each variable and triple check \n",
    "# that they're ready to go and that the method is appropriate.\n",
    "for k, v in feat_eng_dict.items():\n",
    "  print('='*20)\n",
    "  print(f'{k}: {v[0]}')\n",
    "  print('-'*20)\n",
    "  for i in v[1:]:\n",
    "    if i not in list(meyer.columns):\n",
    "      print(f'{i} not in columns')\n",
    "    else:\n",
    "      # print(f'Dtype: {meyer[[i]].dytpe}')\n",
    "      meyer[i].value_counts(dropna = False).sort_index()\n",
    "    print('-'*20)\n",
    "  print('='*20, '\\n'*2)\n",
    "# All good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I want to see a list of all the methods of combination.\n",
    "methods = []\n",
    "for v in feat_eng_dict.values():\n",
    "  if v[0] not in methods:\n",
    "    methods.append(v[0])\n",
    "methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok let's run this bad boy!!\n",
    "# (I'm so freaking excited I'm so proud of myself for \n",
    "# coming up with this and I'm so clever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meyer.shape # (1494, 228)\n",
    "cols_added = 0\n",
    "cols_done = []\n",
    "for k, v in feat_eng_dict.items():\n",
    "  # print('')\n",
    "  # print(k)\n",
    "  meyer[k] = meyer[v[1]]\n",
    "  cols_done.append(v[1])\n",
    "  cols_added += 1\n",
    "  for i in v[2:]:\n",
    "    # print('doing the sum')\n",
    "    meyer[k] += meyer[i]\n",
    "    cols_done.append(i)\n",
    "  if v[0]=='mean':\n",
    "    # print('doing the mean')\n",
    "    meyer[k] = meyer[k]/len(v[1:])\n",
    "  elif v[0]=='binarize': \n",
    "    # print('binarizing')\n",
    "    meyer[k] = np.where(meyer[k]>1, 1, meyer[k])\n",
    "  \n",
    "cols_added + 228 # 253\n",
    "meyer.shape # (1494, 253)\n",
    "len(cols_done) # 121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks\n",
    "\n",
    "# Binarization\n",
    "meyer['check_disabled'] = (meyer['w1q75_ei_r'] + meyer['w1q76_ei_r'])\n",
    "meyer.loc[(meyer['check_disabled']==2), 'check_disabled'] = 1 \n",
    "print(sum((meyer['check_disabled']-meyer['disabled'])!=0))\n",
    "print(sum(meyer['check_disabled']!=meyer['disabled']))\n",
    "\n",
    "# Sum\n",
    "meyer['check_suicidality'] = (meyer['w1q101_ei_r'] + meyer['w1q105_ei_r'] + meyer['w1q109_ei_r'] + meyer['w1q114_ei_r'])\n",
    "print(sum(meyer['check_suicidality']==meyer['suicidality']))\n",
    "print(sum(meyer['check_suicidality']-meyer['suicidality'])==0))\n",
    "\n",
    "# Mean\n",
    "meyer['check_outness'] = (meyer['w1q123a_ei_r'] + meyer['w1q123b_ei_r'] + meyer[\n",
    "  'w1q123c_ei_r'] + meyer['w1q123d_ei_r'] + meyer['w1q124_ei_r'])/len([\n",
    "  'mean', 'w1q123a_ei_r', 'w1q123b_ei_r', 'w1q123c_ei_r', 'w1q123d_ei_r', 'w1q124_ei_r'][1:])\n",
    "print(sum(meyer['check_outness']==meyer['outness']))\n",
    "print(sum(meyer['check_outness']-meyer['outness'])==0)\n",
    "\n",
    "# save a backup\n",
    "meyer.to_csv(f'{my_date()}_column-combination-done_no-drops_not-reordered_FIXED.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the component columns\n",
    "cols_done.append('check_outness')\n",
    "cols_done.append('check_suicidality')\n",
    "cols_done.append('check_disabled')\n",
    "228-121+25\n",
    "meyer.drop(columns = cols_done).shape # goal is 132\n",
    "meyer.drop(columns = cols_done, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the columns\n",
    "ordered_cols = sorted(list(meyer.columns))\n",
    "ordered_cols.remove('studyid')\n",
    "ordered_cols.remove('kessler6_sqrt')\n",
    "ordered_cols.remove('w1kessler6_i')\n",
    "ordered_cols = ['studyid', 'w1kessler6_i', 'kessler6_sqrt'] + ordered_cols\n",
    "ordered_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ordered_cols) # 132\n",
    "meyer.shape # (1494, 132)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meyer = meyer[ordered_cols]\n",
    "meyer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save again\n",
    "meyer.to_csv(f'{my_date()}_column-combination-done_drops-done_reordered_FIXED.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 4 in full:\n",
    "4. 2024-06-07-eda.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friday, June 7, 2024\n",
    "# EDA\n",
    "\n",
    "# As usual, we're starting with my beloved autoplots - now with new arguments!\n",
    "\n",
    "autoplots(meyer, 'w1kessler6_i', qqplots=True, transform=True, line=False, verbose=False)\n",
    "autoplots(meyer, 'kessler6_sqrt', qqplots=True, transform=True, line=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a few more that I need to OHE, 0-base, or otherwise tinker with, but I'm going\n",
    "# to try to resist the urge to do that until I have SOME kind of model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meyer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meyer_corr = meyer.corr()\n",
    "\n",
    "meyer.shape\n",
    "meyer.to_csv(f'{my_date()}_end-of-day.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 5 in full:\n",
    "5. 2024-06-07_NEW_feat_eng_dict.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thursday, June 7, 2024\n",
    "# NEW feat_eng_dict\n",
    "\n",
    "# I thought I was so smart before, just getting a sum from these guys.\n",
    "# Fool.  It dilutes the power of the \"yes\"s! There aren't enough \"yes\"s \n",
    "# in any one level of frequency to compete with the \"no\"s.\n",
    "\n",
    "# I'm going to create this list with the first one I found and\n",
    "# then append to it in the console.\n",
    "columns_to_binarize = ['abus_treat_non_queer']\n",
    "# make a shorter name, dummy\n",
    "col_bi = columns_to_binarize.copy()\n",
    "\n",
    "# these are some visually very promising ones\n",
    "good_ones = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 6 in full:\n",
    "6. 2024-06-07_columns-i-want-to-screw-with-some-more-and-how.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 7 in full:\n",
    "7. 2024-06-07_modeling-part-1.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friday, June 7, 2024\n",
    "# Modeling!\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm just going to eyeball some features from the scatterplots\n",
    "good_ones = ['chronic_strain', 'suicidality', 'w1age', 'w1auditc_i', \n",
    "    'w1connectedness_i', 'w1conversion', 'w1dudit_i', 'w1everyday_i', \n",
    "  'w1feltstigma_i', 'w1idcentral_i', 'w1internalized_i', 'w1lifesat_i', \n",
    "  'w1meim_i', 'w1pinc_i', 'w1poverty_i_ei', 'w1q03_ei', 'w1q33_ei', \n",
    "  'w1q52_ei', 'w1q72_ei', 'w1q74_22_ei', 'w1q74_21_ei', 'w1q140_ei', \n",
    "  'w1q166_ei', 'w1q167_ei', 'w1q171_8_ei', 'w1q181_ei_r', 'w1socialwb_i', \n",
    "  'w1socsupport_i']\n",
    "\n",
    "# Make X and y\n",
    "X = meyer[good_ones]\n",
    "y = meyer['kessler6_sqrt']\n",
    "\n",
    "# Very small testing set because this is an inferential model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "  test_size = 0.1, random_state = 6)\n",
    "  \n",
    "for i in [X_train, X_test, y_train, y_test]:\n",
    "  print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Cross validation just for funsies\n",
    "cross_val_score(lr, X_train, y_train)\n",
    "\n",
    "# Fit the model\n",
    "model_1 = lr.fit(X_train, y_train)\n",
    "\n",
    "# Make some predictions\n",
    "model_1_train_preds = model_1.predict(X_train)\n",
    "model_1_test_preds = model_1.predict(X_test)\n",
    "\n",
    "# Score the model\n",
    "print('Training Set')\n",
    "model_1.score(X_train, y_train)\n",
    "mean_squared_error(y_train, model_1_train_preds)\n",
    "mean_squared_error(y_train, model_1_train_preds, squared = False)\n",
    "mean_absolute_error(y_train, model_1_train_preds)\n",
    "print('='*20)\n",
    "print('Testing Set')\n",
    "model_1.score(X_test, y_test)\n",
    "mean_squared_error(y_test, model_1_test_preds)\n",
    "mean_squared_error(y_test, model_1_test_preds, squared = False)\n",
    "mean_absolute_error(y_test, model_1_test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script 8 in full:\n",
    "8. autoplots_v2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, date, time\n",
    "from string import capwords\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Define my_date()\n",
    "def my_date():\n",
    "  return datetime.now().strftime('%Y-%m-%d_h%H-m%M-s%S')\n",
    "my_date()\n",
    "\n",
    "# Define autoplots()\n",
    "def autoplots(d, y, qqplots = True, transform = False, line = False, verbose = True):\n",
    "  '''a function to make a ton of graphs.\n",
    "  Each plot is based on a subset of d where all variables in the\n",
    "  plot have no null values.  The size of this subset (n) is \n",
    "  displayed in the subtitle of the plot, and can be used \n",
    "  similarly to d.isnull().sum(), if desired.\n",
    "  \n",
    "  args:\n",
    "    d: dataframe, the dataframe of the information.\n",
    "    \n",
    "    y: string, the name of the column within the dataframe that is the target.\n",
    "    \n",
    "    qqplots: bool, whether to generate qqplots for each variable. Default = True.\n",
    "    \n",
    "    transform: bool, whether to generate qqplots of transformed versions \n",
    "      of each variable. Only used when `qqplots = True`, otherwise ignored. \n",
    "      Default = False. Included transformations are (x+z)**(1/2), (x+z)**(1/3), \n",
    "      and log(x+z+1), where z=abs(x.min()) if x.min()<0, else z=0.\n",
    "    \n",
    "    line: bool, whether to plot line graphs. Default = False.\n",
    "    \n",
    "    verbose: bool, whether to print updates while running. Default = True.\n",
    "  \n",
    "  return:\n",
    "    a ton of plots in a FOLDER\n",
    "  \n",
    "  raise:\n",
    "    pls no'''\n",
    "  \n",
    "  # Need these\n",
    "  import os\n",
    "  from string import capwords\n",
    "  import matplotlib.pyplot as plt\n",
    "  import seaborn as sns\n",
    "  import statsmodels.api as sm\n",
    "  import numpy as np\n",
    "  import pandas as pd\n",
    "  \n",
    "  # Make a folder \n",
    "  try: \n",
    "    os.mkdir('images')\n",
    "  except:\n",
    "    pass\n",
    "  a = f'images/plots_{my_date()}'\n",
    "  os.mkdir(a)\n",
    "  \n",
    "  # Define these once\n",
    "  n_grand = len(d[y])\n",
    "  if verbose==True: print(n_grand)\n",
    "  # in future versions, I'd like to raise a warning\n",
    "  # if len(d[y])!=len(d[d[y].notna()])    \n",
    "  # (i.e., if there are nulls in the target)\n",
    "  n_features = d.shape[1]\n",
    "  \n",
    "  # Give y a good(ish) name\n",
    "  ty = capwords(y.replace('_', ' '))\n",
    "  if verbose==True: print(ty)\n",
    "  \n",
    "  # Plot the distributions of all variables\n",
    "  for i in d.columns:\n",
    "    if verbose==True: print(i)\n",
    "    # Give it a good(ish) name\n",
    "    t = capwords(i.replace('_', ' '))\n",
    "    if verbose==True: print(t)\n",
    "    \n",
    "    # Extract the subset dataframe, drop NAs, get n\n",
    "    df = d[i]\n",
    "    if verbose==True: print(df.shape)\n",
    "    df.dropna(inplace = True)\n",
    "    n = len(df)\n",
    "    \n",
    "    # Plot a histogram of it\n",
    "    plt.figure(figsize = (16, 9));\n",
    "    plt.hist(df, bins = 'auto', color = 'purple');\n",
    "    plt.suptitle(f'Distribution of {t}', size = 24)\n",
    "    plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "    plt.xlabel(f'{t}', size = 20);\n",
    "    plt.ylabel('Frequency', size = 20);\n",
    "    plt.xticks(size = 16, rotation = 60);\n",
    "    plt.yticks(size = 16)\n",
    "    # plt.tight_layout()\n",
    "    plt.savefig(f'./{a}/{i}_histogram.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot a boxplot of it\n",
    "    plt.figure(figsize = (16, 9))\n",
    "    sns.boxplot(data = df, color = 'purple', orient = 'h')\n",
    "    plt.suptitle(f'Distribution of {t}', size = 24)\n",
    "    plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "    plt.xlabel(f'{t}', size = 20);\n",
    "    plt.xticks(size = 16, rotation = 60)\n",
    "    # plt.tight_layout()\n",
    "    plt.savefig(f'./{a}/{i}_boxplot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot (a) qqplot(s) of it, if indicated\n",
    "    if qqplots==True:\n",
    "      \n",
    "      # Raw data\n",
    "      plt.figure(figsize = (9, 9))\n",
    "      sm.qqplot(data = df, line='45', markerfacecolor = 'purple', \n",
    "        markeredgecolor = 'purple', alpha = 0.5)\n",
    "      plt.suptitle(f'QQ-Plot of {t}', size = 24)\n",
    "      plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "      plt.xticks(size = 16, rotation = 60);\n",
    "      plt.yticks(size = 16);\n",
    "      # plt.tight_layout();\n",
    "      plt.savefig(f'./{a}/{i}_qqplot.png')\n",
    "      plt.close()\n",
    "      \n",
    "      # Plot transformations, if indicated\n",
    "      if transform == True:\n",
    "        \n",
    "        # Calculate z\n",
    "        if df.min()<0:\n",
    "          z=abs(df.min())\n",
    "        else: \n",
    "          z=0\n",
    "        \n",
    "        # Square root\n",
    "        plt.figure(figsize = (9, 9))\n",
    "        sm.qqplot(data = ((df+z)**(1/2)), line='45', alpha = 0.5, \n",
    "          markerfacecolor = 'purple', markeredgecolor = 'purple')\n",
    "        plt.suptitle(f'QQ-Plot of Square Root of {t}-Plus-{z}', size = 24)\n",
    "        plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "        plt.xticks(size = 16, rotation = 60);\n",
    "        plt.yticks(size = 16);\n",
    "        # plt.tight_layout();\n",
    "        plt.savefig(f'./{a}/{i}_qqplot_2nd_root.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Cube root\n",
    "        plt.figure(figsize = (9, 9))\n",
    "        sm.qqplot(data = ((df+z)**(1/3)), line='45', alpha = 0.5, \n",
    "          markerfacecolor = 'purple', markeredgecolor = 'purple')\n",
    "        plt.suptitle(f'QQ-Plot of Cube Root of {t}-Plus-{z}', size = 24)\n",
    "        plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "        plt.xticks(size = 16, rotation = 60);\n",
    "        plt.yticks(size = 16);\n",
    "        # plt.tight_layout();\n",
    "        plt.savefig(f'./{a}/{i}_qqplot_3rd_root.png')\n",
    "        plt.close()        \n",
    "        \n",
    "        # Log... ish\n",
    "        plt.figure(figsize = (9, 9))\n",
    "        sm.qqplot(data = np.log(df+z+1), line='45', alpha = 0.5, \n",
    "          markerfacecolor = 'purple', markeredgecolor = 'purple')\n",
    "        plt.suptitle(f'QQ-Plot of Log of {t}-Plus-{z+1}', size = 24)\n",
    "        plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "        plt.xticks(size = 16, rotation = 60);\n",
    "        plt.yticks(size = 16);\n",
    "        # plt.tight_layout();\n",
    "        plt.savefig(f'./{a}/{i}_qqplot_log.png')\n",
    "        plt.close()       \n",
    "    \n",
    "  # Drop y from the list\n",
    "  X = [col for col in list(d.drop(columns = [y]).columns)]\n",
    "  \n",
    "  # Make plots of each x against y\n",
    "  for i in X:\n",
    "    # Give it a good(ish) name\n",
    "    t = capwords(i.replace('_', ' '))\n",
    "    if verbose==True: print(t)\n",
    "    \n",
    "    # Extract the subset dataframe, drop NAs, get n\n",
    "    df = d[[i, y]]\n",
    "    df.dropna(inplace = True)\n",
    "    n = len(df[y])\n",
    "    \n",
    "    # Plot a scatterplot of it against y\n",
    "    plt.figure(figsize = (16, 9))\n",
    "    plt.scatter(df[i], df[y], alpha = 0.5, color = 'purple')\n",
    "    plt.suptitle(f'Relationship between {t} and {ty}', size = 24)\n",
    "    plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "    plt.xlabel(f'{t}', size = 20);\n",
    "    plt.ylabel(f'{ty}', size = 20);\n",
    "    plt.xticks(size = 16, rotation = 60)\n",
    "    plt.yticks(size = 16)\n",
    "    # plt.tight_layout()\n",
    "    plt.savefig(f'./{a}/{i}-by-{y}_scatterplot.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot a line plot of it against y\n",
    "    if line==True:\n",
    "      plt.figure(figsize = (16, 9))\n",
    "      plt.plot(i, y, data = df, color = 'purple')\n",
    "      plt.suptitle(f'Relationship between {t} and {ty}', size = 24)\n",
    "      plt.title(f'Based on {n} Observations out of {n_grand}', size = 18)\n",
    "      plt.xlabel(f'{t}', size = 20);\n",
    "      plt.ylabel(f'{ty}', size = 20);\n",
    "      plt.xticks(size = 16, rotation = 60)\n",
    "      plt.yticks(size = 16)\n",
    "      # plt.tight_layout()\n",
    "      plt.savefig(f'./{a}/{i}-by-{y}_lineplot.png')\n",
    "      plt.close()\n",
    "    \n",
    "  # All together now\n",
    "  n = len(d[y])\n",
    "  \n",
    "  # Plot a line plot of everything against y\n",
    "  if line==True:\n",
    "    plt.figure(figsize = (16, 9))\n",
    "    for i in X:\n",
    "      if verbose==True: print(i)\n",
    "      plt.plot(i, y, data = d)\n",
    "    plt.suptitle(f'Relationship between Predictors and {ty}', size = 24)\n",
    "    plt.title(f'Based on {n_grand} Observations out of {n_grand}', size = 18)\n",
    "    plt.xlabel(f'{t}', size = 20);\n",
    "    plt.ylabel(f'{ty}', size = 20);\n",
    "    plt.xticks(size = 16, rotation = 60)\n",
    "    plt.yticks(size = 16)\n",
    "    plt.legend();\n",
    "    # plt.tight_layout()\n",
    "    plt.savefig(f'./{a}/all-by-{y}_lineplot.png')\n",
    "    plt.close()\n",
    "  \n",
    "  # Get some correlations\n",
    "  corr = round(d.corr(numeric_only = True), 2)\n",
    "  \n",
    "  # Plot a heatmap\n",
    "  mask = np.zeros_like(corr)\n",
    "  mask[np.triu_indices_from(mask)] = True\n",
    "  quarter_features = np.round((n_features/4), 0)\n",
    "  plt.figure(figsize = (quarter_features, quarter_features))\n",
    "  sns.heatmap(corr, square = True, \n",
    "    annot = True, cmap = 'coolwarm', mask = mask);\n",
    "  plt.suptitle(f'Relationships Between Variables', size = 24)\n",
    "  plt.title(f'Based on {n_grand} Observations out of {n_grand}', size = 18)\n",
    "  # plt.tight_layout()\n",
    "  plt.savefig(f'./{a}/all_heatmap.png')\n",
    "  plt.close()\n",
    "  \n",
    "  # Plot a heatmap column on y\n",
    "  if y in corr:\n",
    "    plt.figure(figsize = (16, 9))\n",
    "    sns.heatmap(np.asarray([corr[y].sort_values(ascending = False)]).T, \n",
    "      vmin = 0, vmax = 1, annot = True, cmap = 'coolwarm')\n",
    "    plt.suptitle(f'Relationship between Predictors and {ty}', size = 24)\n",
    "    plt.title(f'Based on {n_grand} Observations out of {n_grand}', size = 18)\n",
    "    plt.xlabel(f'{ty}', size = 20)\n",
    "    plt.yticklabels = True\n",
    "    # plt.tight_layout()\n",
    "    plt.savefig(f'./{a}/all-by-{y}_heatmap.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfinished Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I hate spending the time on this, but I need the column names to have meaning.\n",
    "for i in list(meyer.columns):\n",
    "  print(f\"'{i}': '{i}', \")\n",
    "\n",
    "# I'm trying to both at once and it's not working\n",
    "# This list is for whether to screw with them some more.\n",
    "for i in list(meyer.columns):\n",
    "  print(f\"'{i}': _, \")\n",
    "\n",
    "meyer['w1kessler6_i'].value_counts(dropna = False).sort_index()\n",
    "col_bi.append('') # abus_treat_non_queer?\n",
    "\n",
    "col_names = {\n",
    "  'studyid': 'studyid', \n",
    "  'w1kessler6_i': 'kessler6_orig', \n",
    "  'kessler6_sqrt': 'kessler6_sqrt', \n",
    "  \n",
    "  'abus_treat_non_queer': 'abus_treat_non_queer', \n",
    "  'abus_treat_sex_gender': 'abus_treat_sex_gender', \n",
    "  'abusive_treatment': 'abusive_treatment', \n",
    "  'bad_neighbhd': 'bad_neighbhd', \n",
    "  'childhd_bullying_non_queer': 'childhd_bullying_non_queer', \n",
    "  'childhd_bullying_sex_gender': 'childhd_bullying_sex_gender', \n",
    "  'chronic_strain': 'chronic_strain', \n",
    "  'cohort': 'cohort', \n",
    "  'daily_discr_non_queer': 'daily_discr_non_queer', \n",
    "  'daily_discr_sex_gender': 'daily_discr_sex_gender', \n",
    "  'disabled': 'disabled', \n",
    "  'gcendiv': 'gcendiv', \n",
    "  'gcenreg': 'gcenreg', \n",
    "  'geduc1': 'geduc1', \n",
    "  'geduc2': 'geduc2', \n",
    "  'geducation': 'geducation', \n",
    "  'gmilesaway2_ei_r': 'gmilesaway2_ei_r', \n",
    "  'gurban_i': 'gurban_i', \n",
    "  'health_insurance': 'health_insurance', \n",
    "  'housing_disc_non_queer': 'housing_disc_non_queer', \n",
    "  'housing_disc_sex_gender': 'housing_disc_sex_gender', \n",
    "  'outness': 'outness', \n",
    "  'screen_race': 'screen_race', \n",
    "  'serious_health_cond': 'serious_health_cond', \n",
    "  'stress_past_year_crime': 'stress_past_year_crime', \n",
    "  'stress_past_year_gen': 'stress_past_year_gen', \n",
    "  'stress_past_year_interpersonal': 'stress_past_year_interpersonal', \n",
    "  'stress_past_year_non_queer': 'stress_past_year_non_queer', \n",
    "  'stress_past_year_sex_gender': 'stress_past_year_sex_gender', \n",
    "  'stress_past_year_work': 'stress_past_year_work', \n",
    "  'suicidality': 'suicidality', \n",
    "  'w1ace_emo_i': 'w1ace_emo_i', \n",
    "  'w1ace_i': 'w1ace_i', \n",
    "  'w1ace_inc_i': 'w1ace_inc_i', \n",
    "  'w1ace_ipv_i': 'w1ace_ipv_i', \n",
    "  'w1ace_men_i': 'w1ace_men_i', \n",
    "  'w1ace_phy_i': 'w1ace_phy_i', \n",
    "  'w1ace_sep_i': 'w1ace_sep_i', \n",
    "  'w1ace_sex_i': 'w1ace_sex_i', \n",
    "  'w1ace_sub_i': 'w1ace_sub_i', \n",
    "  'w1age': 'w1age', \n",
    "  'w1auditc_i': 'w1auditc_i', \n",
    "  'w1childgnc_i': 'w1childgnc_i', \n",
    "  'w1connectedness_i': 'w1connectedness_i', \n",
    "  'w1conversion': 'w1conversion', \n",
    "  'w1conversionhc': 'w1conversionhc', \n",
    "  'w1conversionrel': 'w1conversionrel', \n",
    "  'w1dudit_i': 'w1dudit_i', \n",
    "  'w1everyday_i': 'w1everyday_i', \n",
    "  'w1feltstigma_i': 'w1feltstigma_i', \n",
    "  'w1gender': 'w1gender', \n",
    "  'w1hcthreat_i': 'w1hcthreat_i', \n",
    "  'w1hinc_i': 'w1hinc_i', \n",
    "  'w1idcentral_i': 'w1idcentral_i', \n",
    "  'w1internalized_i': 'w1internalized_i', \n",
    "  'w1lifesat_i': 'w1lifesat_i', \n",
    "  'w1meim_i': 'w1meim_i', \n",
    "  'w1pinc_i': 'w1pinc_i', \n",
    "  'w1poverty_i_ei': 'w1poverty_i_ei', \n",
    "  'w1povertycat_i_ei': 'w1povertycat_i_ei', \n",
    "  'w1q01_ei': 'w1q01_ei', \n",
    "  'w1q03_ei': 'w1q03_ei', \n",
    "  'w1q119_ei': 'w1q119_ei', \n",
    "  'w1q136_7_ei': 'w1q136_7_ei', \n",
    "  'w1q139_7_ei': 'w1q139_7_ei', \n",
    "  'w1q140_ei': 'w1q140_ei', \n",
    "  'w1q141_7_ei': 'w1q141_7_ei', \n",
    "  'w1q143_7_ei': 'w1q143_7_ei', \n",
    "  'w1q145_7_ei': 'w1q145_7_ei', \n",
    "  'w1q162_ei': 'w1q162_ei', \n",
    "  'w1q163_7_ei': 'w1q163_7_ei', \n",
    "  'w1q166_ei': 'w1q166_ei', \n",
    "  'w1q167_ei': 'w1q167_ei', \n",
    "  'w1q168_ei': 'w1q168_ei', \n",
    "  'w1q169_ei': 'w1q169_ei', \n",
    "  'w1q171_1_ei': 'w1q171_1_ei', \n",
    "  'w1q171_2_ei': 'w1q171_2_ei', \n",
    "  'w1q171_3_ei': 'w1q171_3_ei', \n",
    "  'w1q171_4_ei': 'w1q171_4_ei', \n",
    "  'w1q171_5_ei': 'w1q171_5_ei', \n",
    "  'w1q171_6_ei': 'w1q171_6_ei', \n",
    "  'w1q171_7_ei': 'w1q171_7_ei', \n",
    "  'w1q171_8_ei': 'w1q171_8_ei', \n",
    "  'w1q171_9_ei': 'w1q171_9_ei', \n",
    "  'w1q175_ei': 'w1q175_ei', \n",
    "  'w1q179_ei_r_relig_christ': 'w1q179_ei_r_relig_christ', \n",
    "  'w1q179_ei_r_relig_other': 'w1q179_ei_r_relig_other', \n",
    "  'w1q180_ei_r_relig_christ': 'w1q180_ei_r_relig_christ', \n",
    "  'w1q180_ei_r_relig_other': 'w1q180_ei_r_relig_other', \n",
    "  'w1q181_ei_r': 'w1q181_ei_r', \n",
    "  'w1q30_1_ei': 'w1q30_1_ei', \n",
    "  'w1q30_2_ei': 'w1q30_2_ei', \n",
    "  'w1q30_3_ei': 'w1q30_3_ei', \n",
    "  'w1q30_4_ei': 'w1q30_4_ei', \n",
    "  'w1q30_5_ei': 'w1q30_5_ei', \n",
    "  'w1q32_ei': 'w1q32_ei', \n",
    "  'w1q33_ei': 'w1q33_ei', \n",
    "  'w1q34_ei': 'w1q34_ei', \n",
    "  'w1q35_ei': 'w1q35_ei', \n",
    "  'w1q36_ei': 'w1q36_ei', \n",
    "  'w1q37_ei': 'w1q37_ei', \n",
    "  'w1q38_ei': 'w1q38_ei', \n",
    "  'w1q52_ei': 'w1q52_ei', \n",
    "  'w1q64_1_ei': 'w1q64_1_ei', \n",
    "  'w1q65_ei': 'w1q65_ei', \n",
    "  'w1q69_ei': 'w1q69_ei', \n",
    "  'w1q72_ei': 'w1q72_ei', \n",
    "  'w1q74_21_ei': 'w1q74_21_ei', \n",
    "  'w1q74_22_ei': 'w1q74_22_ei', \n",
    "  'w1q74_23_ei': 'w1q74_23_ei', \n",
    "  'w1q78_ei': 'w1q78_ei', \n",
    "  'w1q79_ei': 'w1q79_ei', \n",
    "  'w1q89_ei': 'w1q89_ei', \n",
    "  'w1race': 'w1race', \n",
    "  'w1sex': 'w1sex', \n",
    "  'w1sex_gender': 'w1sex_gender', \n",
    "  'w1sexminid': 'w1sexminid', \n",
    "  'w1sexualid': 'w1sexualid', \n",
    "  'w1socialwb_i': 'w1socialwb_i', \n",
    "  'w1socsupport_fam_i': 'w1socsupport_fam_i', \n",
    "  'w1socsupport_fr_i': 'w1socsupport_fr_i', \n",
    "  'w1socsupport_i': 'w1socsupport_i', \n",
    "  'w1socsupport_so_i': 'w1socsupport_so_i', \n",
    "  'w1survey_yr': 'w1survey_yr', \n",
    "  'w1weight_full': 'w1weight_full', \n",
    "  'waveparticipated': 'waveparticipated', \n",
    "  'work_disc_non_queer': 'work_disc_non_queer', \n",
    "  'work_disc_sex_gender': 'work_disc_sex_gender', \n",
    "  'work_neg_outcomes': 'work_neg_outcomes'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_screw = {\n",
    "  # 'studyid': _, \n",
    "  # 'w1kessler6_i': _, \n",
    "  # 'kessler6_sqrt': _, \n",
    "  \n",
    "  'abus_treat_non_queer': len(feat_eng_dict[k][1:]), \n",
    "  'abus_treat_sex_gender': len(feat_eng_dict[k][1:]), \n",
    "  # 'abusive_treatment': _, \n",
    "  # 'bad_neighbhd': _, \n",
    "  'childhd_bullying_non_queer': len(feat_eng_dict['childhd_bullying_non_queer'][1:]), \n",
    "  'childhd_bullying_sex_gender': len(feat_eng_dict['childhd_bullying_sex_gender'][1:]), \n",
    "  # 'chronic_strain': _, \n",
    "  'cohort': 'ohe', \n",
    "  'daily_discr_non_queer': len(feat_eng_dict[k][1:]), \n",
    "  'daily_discr_sex_gender': len(feat_eng_dict[k][1:]), \n",
    "  # 'disabled': _, \n",
    "  'gcendiv': d, \n",
    "  'gcenreg': o, \n",
    "  # 'geduc1': _, \n",
    "  'geduc2': d, \n",
    "  # 'geducation': _, \n",
    "  # 'gmilesaway2_ei_r': _, \n",
    "  # 'gurban_i': _, \n",
    "  'health_insurance': 'combine with q64', \n",
    "  'housing_disc_non_queer': len(feat_eng_dict[k][1:]), \n",
    "  'housing_disc_sex_gender': len(feat_eng_dict[k][1:]), \n",
    "  # 'outness': _, \n",
    "  'screen_race': 'check with other race stuff', \n",
    "  'serious_health_cond': _, \n",
    "  'stress_past_year_crime': _, \n",
    "  'stress_past_year_gen': _, \n",
    "  'stress_past_year_interpersonal': _, \n",
    "  'stress_past_year_non_queer': _, \n",
    "  'stress_past_year_sex_gender': _, \n",
    "  'stress_past_year_work': _, \n",
    "  'suicidality': _, \n",
    "  'w1ace_emo_i': _, \n",
    "  'w1ace_i': _, \n",
    "  'w1ace_inc_i': _, \n",
    "  'w1ace_ipv_i': _, \n",
    "  'w1ace_men_i': _, \n",
    "  'w1ace_phy_i': _, \n",
    "  'w1ace_sep_i': _, \n",
    "  'w1ace_sex_i': _, \n",
    "  'w1ace_sub_i': _, \n",
    "  'w1age': _, \n",
    "  'w1auditc_i': _, \n",
    "  'w1childgnc_i': _, \n",
    "  'w1connectedness_i': _, \n",
    "  'w1conversion': _, \n",
    "  'w1conversionhc': _, \n",
    "  'w1conversionrel': _, \n",
    "  'w1dudit_i': _, \n",
    "  'w1everyday_i': _, \n",
    "  'w1feltstigma_i': _, \n",
    "  'w1gender': _, \n",
    "  'w1hcthreat_i': _, \n",
    "  'w1hinc_i': _, \n",
    "  'w1idcentral_i': _, \n",
    "  'w1internalized_i': _, \n",
    "  'w1lifesat_i': _, \n",
    "  'w1meim_i': _, \n",
    "  'w1pinc_i': _, \n",
    "  'w1poverty_i_ei': _, \n",
    "  'w1povertycat_i_ei': _, \n",
    "  'w1q01_ei': _, \n",
    "  'w1q03_ei': _, \n",
    "  'w1q119_ei': _, \n",
    "  'w1q136_7_ei': _, \n",
    "  'w1q139_7_ei': _, \n",
    "  'w1q140_ei': _, \n",
    "  'w1q141_7_ei': _, \n",
    "  'w1q143_7_ei': _, \n",
    "  'w1q145_7_ei': _, \n",
    "  'w1q162_ei': _, \n",
    "  'w1q163_7_ei': _, \n",
    "  'w1q166_ei': _, \n",
    "  'w1q167_ei': _, \n",
    "  'w1q168_ei': _, \n",
    "  'w1q169_ei': _, \n",
    "  'w1q171_1_ei': _, \n",
    "  'w1q171_2_ei': _, \n",
    "  'w1q171_3_ei': _, \n",
    "  'w1q171_4_ei': _, \n",
    "  'w1q171_5_ei': _, \n",
    "  'w1q171_6_ei': _, \n",
    "  'w1q171_7_ei': _, \n",
    "  'w1q171_8_ei': _, \n",
    "  'w1q171_9_ei': _, \n",
    "  'w1q175_ei': _, \n",
    "  'w1q179_ei_r_relig_christ': _, \n",
    "  'w1q179_ei_r_relig_other': _, \n",
    "  'w1q180_ei_r_relig_christ': _, \n",
    "  'w1q180_ei_r_relig_other': _, \n",
    "  'w1q181_ei_r': _, \n",
    "  'w1q30_1_ei': _, \n",
    "  'w1q30_2_ei': _, \n",
    "  'w1q30_3_ei': _, \n",
    "  'w1q30_4_ei': _, \n",
    "  'w1q30_5_ei': _, \n",
    "  'w1q32_ei': _, \n",
    "  'w1q33_ei': _, \n",
    "  'w1q34_ei': _, \n",
    "  'w1q35_ei': _, \n",
    "  'w1q36_ei': _, \n",
    "  'w1q37_ei': _, \n",
    "  'w1q38_ei': _, \n",
    "  'w1q52_ei': _, \n",
    "  'w1q64_1_ei': _, \n",
    "  'w1q65_ei': _, \n",
    "  'w1q69_ei': _, \n",
    "  'w1q72_ei': _, \n",
    "  'w1q74_21_ei': _, \n",
    "  'w1q74_22_ei': _, \n",
    "  'w1q74_23_ei': _, \n",
    "  'w1q78_ei': _, \n",
    "  'w1q79_ei': _, \n",
    "  'w1q89_ei': _, \n",
    "  'w1race': _, \n",
    "  'w1sex': _, \n",
    "  'w1sex_gender': _, \n",
    "  'w1sexminid': _, \n",
    "  'w1sexualid': _, \n",
    "  'w1socialwb_i': _, \n",
    "  'w1socsupport_fam_i': _, \n",
    "  'w1socsupport_fr_i': _, \n",
    "  'w1socsupport_i': _, \n",
    "  'w1socsupport_so_i': _, \n",
    "  'w1survey_yr': _, \n",
    "  'w1weight_full': _, \n",
    "  'waveparticipated': _, \n",
    "  'work_disc_non_queer': _, \n",
    "  'work_disc_sex_gender': _, \n",
    "  'work_neg_outcomes': _,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_eng_dict_NEW = {\n",
    "  'health_insurance': [\n",
    "    'binarize', 'w1q64_2_ei', 'w1q64_3_ei', 'w1q64_4_ei', 'w1q64_5_ei', \n",
    "    'w1q64_6_ei', 'w1q64_7_ei', 'w1q64_8_ei', 'w1q64_9_ei', 'w1q64_10_ei', \n",
    "    'w1q64_11_ei', 'w1q64_12_ei', 'w1q64_13_ei', 'w1q64_t_num'], \n",
    "  'serious_health_cond': [\n",
    "    'binarize', 'w1q74_5_ei', 'w1q74_6_ei', 'w1q74_10_ei', 'w1q74_11_ei', \n",
    "    'w1q74_14_ei', 'w1q74_17_ei', 'w1q74_18_ei', 'w1q74_20_ei'], \n",
    "  'disabled': [\n",
    "    'binarize', 'w1q75_ei_r', 'w1q76_ei_r'], \n",
    "  'outness': [\n",
    "    'mean', 'w1q123a_ei_r', 'w1q123b_ei_r', 'w1q123c_ei_r', 'w1q123d_ei_r',\n",
    "    'w1q124_ei_r'], \n",
    "  'abusive_treatment': [\n",
    "    'sum', 'w1q135a_ei_r', 'w1q135b_ei_r', 'w1q135c_ei_r', 'w1q135d_ei_r', \n",
    "    'w1q135e_ei_r', 'w1q135f_ei_r'], \n",
    "  'work_neg_outcomes': [\n",
    "    'sum', 'w1q137_ei_r', 'w1q138_ei_r'], \n",
    "  'abus_treat_non_queer': [\n",
    "    'sum', 'w1q136_1_ei', 'w1q136_5_ei', 'w1q136_6_ei', 'w1q136_8_ei', \n",
    "    'w1q136_9_ei', 'w1q136_10_ei'], \n",
    "  'stress_past_year_gen': [\n",
    "    'sum', 'w1q142a_ei_r', 'w1q142h_ei_r', 'w1q142i_ei_r'], \n",
    "  'stress_past_year_work': [\n",
    "    'sum', 'w1q142b_ei_r', 'w1q142c_ei_r', 'w1q142e_ei_r'], \n",
    "  'stress_past_year_interpersonal': [\n",
    "    'sum', 'w1q142d_ei_r', 'w1q142f_ei_r', 'w1q142g_ei_r'], \n",
    "  'stress_past_year_crime': [\n",
    "    'sum', 'w1q142j_ei_r', 'w1q142k_ei_r'], \n",
    "  'work_disc_non_queer': [\n",
    "    'sum', 'w1q139_1_ei', 'w1q139_5_ei', 'w1q139_6_ei', 'w1q139_8_ei', \n",
    "    'w1q139_9_ei', 'w1q139_10_ei'], \n",
    "  'housing_disc_non_queer': [\n",
    "    'sum', 'w1q141_1_ei', 'w1q141_5_ei', 'w1q141_6_ei', 'w1q141_8_ei', \n",
    "    'w1q141_9_ei', 'w1q141_10_ei'], \n",
    "  'stress_past_year_non_queer': [\n",
    "    'sum', 'w1q143_1_ei', 'w1q143_5_ei', 'w1q143_6_ei', 'w1q143_8_ei', \n",
    "    'w1q143_9_ei', 'w1q143_10_ei'], \n",
    "  'daily_discr_non_queer': [\n",
    "    'sum', 'w1q145_1_ei', 'w1q145_5_ei', 'w1q145_6_ei'], \n",
    "  'childhd_bullying_non_queer': [\n",
    "    'sum', 'w1q163_1_ei', 'w1q163_5_ei', 'w1q163_6_ei', 'w1q163_8_ei', \n",
    "    'w1q163_9_ei', 'w1q163_10_ei'], \n",
    "  'abus_treat_sex_gender': [\n",
    "    'sum', 'w1q136_2_ei', 'w1q136_3_ei', 'w1q136_4_ei'], \n",
    "  'work_disc_sex_gender': [\n",
    "    'sum', 'w1q139_2_ei', 'w1q139_3_ei', 'w1q139_4_ei'], \n",
    "  'housing_disc_sex_gender': [\n",
    "    'sum', 'w1q141_2_ei', 'w1q141_3_ei', 'w1q141_4_ei'], \n",
    "  'stress_past_year_sex_gender': [\n",
    "    'sum', 'w1q143_2_ei', 'w1q143_3_ei', 'w1q143_4_ei'], \n",
    "  'daily_discr_sex_gender': [\n",
    "    'sum', 'w1q145_2_ei', 'w1q145_3_ei', 'w1q145_4_ei', 'w1q145_8_ei', \n",
    "    'w1q145_9_ei', 'w1q145_10_ei'], \n",
    "  'childhd_bullying_sex_gender': [\n",
    "    'sum', 'w1q163_2_ei', 'w1q163_3_ei', 'w1q163_4_ei'], \n",
    "  'chronic_strain': [\n",
    "    'sum', 'w1q146a_ei', 'w1q146b_ei', 'w1q146c_ei', 'w1q146d_ei', 'w1q146e_ei', \n",
    "    'w1q146f_ei', 'w1q146g_ei', 'w1q146h_ei', 'w1q146i_ei', 'w1q146j_ei', \n",
    "    'w1q146k_ei', 'w1q146l_ei'], \n",
    "  'bad_neighbhd': [\n",
    "    'sum', 'w1q19a_ei_r', 'w1q19b_ei_r', 'w1q19c_ei_r', 'w1q19d_ei_r'], \n",
    "  'suicidality': ['sum', 'w1q101_ei_r', 'w1q105_ei_r', 'w1q109_ei_r', 'w1q114_ei_r']}\n",
    " \n",
    "len(feat_eng_dict['_'][1:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
